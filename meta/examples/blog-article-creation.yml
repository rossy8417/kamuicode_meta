# SEOæœ€é©åŒ–ãƒ–ãƒ­ã‚°è¨˜äº‹ä½œæˆ - è¶…è©³ç´°ã‚¿ã‚¹ã‚¯åˆ†è§£
name: "seo-blog-article-creation"
description: "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰SEOæœ€é©åŒ–ã•ã‚ŒãŸãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’ä½œæˆ"
category: "content-creation"
complexity_level: 4
estimated_duration_minutes: 60

# äººé–“ã®ç„¡æ„è­˜æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹
human_process:
  - phase: "information_gathering"
    description: "æƒ…å ±åé›†æ®µéšï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç†è§£ãƒ»ç«¶åˆåˆ†æï¼‰"
  - phase: "strategy_planning" 
    description: "æˆ¦ç•¥ç«‹æ¡ˆæ®µéšï¼ˆSEOæˆ¦ç•¥ãƒ»æ§‹æˆè¨­è¨ˆï¼‰"
  - phase: "content_creation"
    description: "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆæ®µéšï¼ˆåŸ·ç­†ãƒ»æœ€é©åŒ–ï¼‰"
  - phase: "quality_assurance"
    description: "å“è³ªä¿è¨¼æ®µéšï¼ˆæ¤œè¨¼ãƒ»æ”¹å–„ï¼‰"
  - phase: "publication_preparation"
    description: "å…¬é–‹æº–å‚™æ®µéšï¼ˆæœ€çµ‚èª¿æ•´ãƒ»é…ä¿¡æº–å‚™ï¼‰"

# GitHub Actions ãƒãƒ¼ãƒ‰è¨­è¨ˆ
github_actions_config:
  workflow_name: "SEO Blog Article Creation"
  on_triggers: ["workflow_dispatch", "issues"]
  permissions:
    contents: "write"
    issues: "write"
    actions: "read"
  artifacts_retention_days: 30
  max_parallel_jobs: 5

# è¶…è©³ç´°ã‚¿ã‚¹ã‚¯åˆ†è§£
tasks:
  # === äº‹å‰ãƒ†ã‚¹ãƒˆæ®µéš ===
  - id: "mcp-api-connectivity-test"
    name: "MCPãƒ»APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"
    phase: "pre_validation"
    github_job: "connectivity-test"
    type: "validation"
    implementation: "mcp"
    duration_minutes: 2
    dependencies: []
    parallel_group: null
    
    github_steps:
      - name: "Test MCP and API connections"
        shell: "bash"
        script: |
          echo "ğŸ”Œ Testing required external services for blog article creation..."
          
          mkdir -p .logs/connectivity-tests
          
          # Webæ¤œç´¢API ãƒ†ã‚¹ãƒˆï¼ˆç«¶åˆåˆ†æãƒ»æ¤œç´¢æ„å›³åˆ†æã§å¿…è¦ï¼‰
          echo "Testing web search capabilities..."
          if [ -n "${{ secrets.GOOGLE_SEARCH_API_KEY }}" ] && [ -n "${{ secrets.GOOGLE_SEARCH_ENGINE_ID }}" ]; then
            # Google Custom Search API ãƒ†ã‚¹ãƒˆ
            if timeout 30 curl -s "https://www.googleapis.com/customsearch/v1?key=${{ secrets.GOOGLE_SEARCH_API_KEY }}&cx=${{ secrets.GOOGLE_SEARCH_ENGINE_ID }}&q=test" > .logs/connectivity-tests/google-search-test.log 2>&1; then
              WEB_SEARCH_STATUS="success"
              WEB_SEARCH_METHOD="google_api"
              echo "âœ… Google Search API: Available"
            else
              WEB_SEARCH_STATUS="failed"
              WEB_SEARCH_METHOD="none"
              echo "âŒ Google Search API: Failed"
            fi
          else
            WEB_SEARCH_STATUS="fallback"
            WEB_SEARCH_METHOD="claude_built_in"
            echo "â„¹ï¸ External Search API: Not configured - will use Claude built-in capabilities"
          fi
          
          # æ³¨æ„ï¼škamuicodeã¯mcp-kamuicode.jsonå†…ã®MCPã‚µãƒ¼ãƒãƒ¼ç¾¤ã‚’æŒ‡ã™ãŒã€
          # ãƒ–ãƒ­ã‚°è¨˜äº‹ä½œæˆã§ã¯ç”»åƒãƒ»å‹•ç”»ãƒ»éŸ³æ¥½ç”ŸæˆMCPã¯ä½¿ç”¨ã—ãªã„ãŸã‚ã€
          # ã‚«ã‚¹ã‚¿ãƒ æ©Ÿèƒ½ã¯å¿…è¦æ™‚ã®ã¿ãƒ†ã‚¹ãƒˆ
          echo "Testing custom MCP functions (if needed)..."
          KAMUICODE_STATUS="not_required"
          echo "â„¹ï¸ MCP Kamuicode: Not required for blog article creation"
          
          # å¤–éƒ¨SEOãƒ„ãƒ¼ãƒ«API ãƒ†ã‚¹ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
          echo "Testing external SEO APIs..."
          if [ -n "${{ secrets.SEO_API_KEY }}" ]; then
            if timeout 15 curl -s -H "Authorization: Bearer ${{ secrets.SEO_API_KEY }}" https://api.example-seo-tool.com/health > .logs/connectivity-tests/seo-api-test.log 2>&1; then
              SEO_API_STATUS="success"
              echo "âœ… SEO API: Available"
            else
              SEO_API_STATUS="failed"
              echo "âš ï¸ SEO API: Failed - will use built-in analysis"
            fi
          else
            SEO_API_STATUS="not_configured"
            echo "â„¹ï¸ SEO API: Not configured - will use built-in analysis"
          fi
          
          # ãƒ†ã‚¹ãƒˆçµæœã‚’JSONä¿å­˜
          cat > .logs/connectivity-tests/test-results.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "required_for_workflow": {
              "web_search_capability": "$WEB_SEARCH_STATUS",
              "web_search_method": "$WEB_SEARCH_METHOD",
              "kamuicode_mcp": "$KAMUICODE_STATUS",
              "seo_api": "$SEO_API_STATUS"
            },
            "overall_status": "$([ "$WEB_SEARCH_STATUS" != "failed" ] && echo "ready" || echo "degraded")",
            "critical_services_available": $([ "$WEB_SEARCH_STATUS" != "failed" ] && echo "true" || echo "false"),
            "workflow_can_proceed": true,
            "fallback_methods_needed": $([ "$WEB_SEARCH_STATUS" = "fallback" ] && echo "true" || echo "false")
          }
          EOF
          
          # ç’°å¢ƒå¤‰æ•°ã«çµæœã‚’è¨­å®š
          echo "CONNECTIVITY_STATUS=$([ "$WEB_SEARCH_STATUS" != "failed" ] && echo "ready" || echo "degraded")" >> $GITHUB_ENV
          echo "WEB_SEARCH_METHOD=$WEB_SEARCH_METHOD" >> $GITHUB_ENV
          echo "FALLBACK_MODE=$([ "$WEB_SEARCH_STATUS" = "fallback" ] && echo "true" || echo "false")" >> $GITHUB_ENV
          
          echo "ğŸ”Œ Connectivity tests completed"
          
    validation:
      criteria:
        - "å¿…è¦ãªå¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹ã®æ¥ç¶šçŠ¶æ³ã‚’ç¢ºèªæ¸ˆã¿"
        - "Web Search MCPã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹æ³•ãŒåˆ©ç”¨å¯èƒ½"
        - "ãƒ†ã‚¹ãƒˆçµæœãŒJSONå½¢å¼ã§ä¿å­˜ã•ã‚Œã¦ã„ã‚‹"
      validation_script: |
        if [ ! -f ".logs/connectivity-tests/test-results.json" ]; then
          echo "âŒ VALIDATION FAILED: Test results missing"
          exit 1
        fi
        
        WORKFLOW_CAN_PROCEED=$(jq -r '.workflow_can_proceed' .logs/connectivity-tests/test-results.json)
        if [ "$WORKFLOW_CAN_PROCEED" = "true" ]; then
          echo "âœ… Validation passed: Workflow can proceed with available services"
        else
          echo "âŒ VALIDATION FAILED: Critical services unavailable"
          exit 1
        fi
        
    error_handling:
      retry_count: 2
      retry_delay_seconds: 10
      fallback_strategy: "continue_with_limitations"
      fallback_script: |
        echo "âš ï¸ Using fallback: All external services failed, using manual methods"
        cat > .logs/connectivity-tests/test-results.json << EOF
        {
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "required_for_workflow": {
            "web_search_mcp": "failed",
            "kamuicode_mcp": "failed",
            "seo_api": "failed"
          },
          "overall_status": "fallback_mode",
          "critical_services_available": false,
          "workflow_can_proceed": true,
          "fallback_methods_needed": true
        }
        EOF
        echo "CONNECTIVITY_STATUS=fallback_mode" >> $GITHUB_ENV
        echo "FALLBACK_MODE=true" >> $GITHUB_ENV
        
    progress_links:
      log_url: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      test_results: ".logs/connectivity-tests/test-results.json"

  # === æƒ…å ±åé›†æ®µéš ===
  - id: "keyword-input-processing"
    name: "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›ã®æ­£è¦åŒ–"
    phase: "information_gathering"
    github_job: "keyword-processing"
    type: "processing"
    implementation: "script"
    duration_minutes: 1
    dependencies: ["mcp-api-connectivity-test"]
    parallel_group: null
    
    github_steps:
      - name: "Extract and normalize keywords"
        shell: "bash"
        script: |
          echo "ğŸ” Processing input keywords..."
          KEYWORDS="${{ github.event.inputs.keywords || github.event.issue.title }}"
          echo "Raw input: $KEYWORDS"
          
          # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ­£è¦åŒ–
          NORMALIZED=$(echo "$KEYWORDS" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-zA-Z0-9 ]//g' | tr -s ' ')
          echo "Normalized: $NORMALIZED"
          
          # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
          mkdir -p .logs/keyword-processing
          
          # å‡¦ç†çµæœã‚’ãƒ­ã‚°ä¿å­˜
          cat > .logs/keyword-processing/result.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "raw_input": "$KEYWORDS",
            "normalized_keywords": "$NORMALIZED",
            "keyword_count": $(echo "$NORMALIZED" | wc -w),
            "processing_status": "success"
          }
          EOF
          
          # GitHubç’°å¢ƒå¤‰æ•°ã«å‡ºåŠ›
          echo "NORMALIZED_KEYWORDS=$NORMALIZED" >> $GITHUB_ENV
          echo "âœ… Keywords processed successfully"
          
    validation:
      criteria:
        - "æ­£è¦åŒ–ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå­˜åœ¨ã™ã‚‹"
        - "ç‰¹æ®Šæ–‡å­—ãŒé©åˆ‡ã«é™¤å»ã•ã‚Œã¦ã„ã‚‹"
        - "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹"
      validation_script: |
        if [ -z "$NORMALIZED_KEYWORDS" ]; then
          echo "âŒ VALIDATION FAILED: No normalized keywords"
          exit 1
        fi
        if [ ! -f ".logs/keyword-processing/result.json" ]; then
          echo "âŒ VALIDATION FAILED: Log file missing"
          exit 1
        fi
        echo "âœ… Validation passed: Keywords processed correctly"
        
    error_handling:
      retry_count: 2
      retry_delay_seconds: 5
      fallback_strategy: "use_default_keywords"
      fallback_script: |
        echo "âš ï¸ Using fallback: default keywords"
        echo "NORMALIZED_KEYWORDS=default blog topic" >> $GITHUB_ENV
        
    progress_links:
      log_url: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
      artifact_path: ".logs/keyword-processing/"

  - id: "search-intent-analysis"
    name: "æ¤œç´¢æ„å›³ã®åˆ†æ"
    phase: "information_gathering"
    github_job: "search-intent-analysis"
    type: "analysis"
    implementation: "mcp"
    tool: "web-search"
    duration_minutes: 3
    dependencies: ["keyword-input-processing"]
    parallel_group: null
    
    github_steps:
      - name: "Analyze search intent"
        shell: "bash"
        script: |
          echo "ğŸ” Analyzing search intent for: $NORMALIZED_KEYWORDS"
          
          mkdir -p .logs/search-intent
          
          # Webæ¤œç´¢ã‚’ä½¿ç”¨ã—ã¦æ¤œç´¢çµæœåˆ†æ
          if [ "$WEB_SEARCH_METHOD" = "google_api" ]; then
            # Google Custom Search APIä½¿ç”¨
            curl -s "https://www.googleapis.com/customsearch/v1?key=${{ secrets.GOOGLE_SEARCH_API_KEY }}&cx=${{ secrets.GOOGLE_SEARCH_ENGINE_ID }}&q=${NORMALIZED_KEYWORDS}&num=10" > .logs/search-intent/search-results.json
          else
            # Claudeå†…è”µæ©Ÿèƒ½ä½¿ç”¨ï¼ˆæ¤œç´¢çµæœã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
            echo '{"items": [], "note": "Using Claude built-in knowledge for search intent analysis"}' > .logs/search-intent/search-results.json
          fi
          
          # Claude Code ã§æ¤œç´¢æ„å›³ã‚’åˆ†æ
          claude-code --prompt "ä»¥ä¸‹ã®æ¤œç´¢çµæœã‹ã‚‰æ¤œç´¢æ„å›³ã‚’åˆ†æã—ã¦ãã ã•ã„ï¼š
          
          ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: $NORMALIZED_KEYWORDS
          æ¤œç´¢çµæœ: $(cat .logs/search-intent/search-results.json)
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"primary_intent\": \"æƒ…å ±åé›†|è³¼å…¥æ¤œè¨|æ¯”è¼ƒæ¤œè¨|å•é¡Œè§£æ±º\",
            \"user_motivation\": \"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å‹•æ©Ÿèª¬æ˜\",
            \"content_type_preference\": \"è¨˜äº‹ã‚¿ã‚¤ãƒ—ã®æ¨å¥¨\",
            \"target_audience\": \"æƒ³å®šèª­è€…å±¤\",
            \"search_volume_estimate\": \"é«˜|ä¸­|ä½\",
            \"competition_level\": \"é«˜|ä¸­|ä½\"
          }" > .logs/search-intent/analysis.json
          
          # åˆ†æçµæœã‚’ãƒ­ã‚°ã«ä¿å­˜
          INTENT_RESULT=$(cat .logs/search-intent/analysis.json)
          echo "SEARCH_INTENT=$INTENT_RESULT" >> $GITHUB_ENV
          
          echo "âœ… Search intent analysis completed"
          
    validation:
      criteria:
        - "æ¤œç´¢æ„å›³åˆ†æçµæœãŒJSONå½¢å¼ã§å­˜åœ¨ã™ã‚‹"
        - "å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒå…¨ã¦å«ã¾ã‚Œã¦ã„ã‚‹"
        - "æ¤œç´¢çµæœãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¦ã„ã‚‹"
      validation_script: |
        if [ ! -f ".logs/search-intent/analysis.json" ]; then
          echo "âŒ VALIDATION FAILED: Analysis file missing"
          exit 1
        fi
        if ! jq -e '.primary_intent' .logs/search-intent/analysis.json > /dev/null; then
          echo "âŒ VALIDATION FAILED: Invalid JSON structure"
          exit 1
        fi
        echo "âœ… Validation passed: Search intent analysis complete"
        
    error_handling:
      retry_count: 3
      retry_delay_seconds: 10
      fallback_strategy: "manual_intent_estimation"
      fallback_script: |
        echo "âš ï¸ Using fallback: manual intent estimation"
        cat > .logs/search-intent/analysis.json << EOF
        {
          "primary_intent": "æƒ…å ±åé›†",
          "user_motivation": "ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦åŸºæœ¬çš„ãªæƒ…å ±ã‚’æ±‚ã‚ã¦ã„ã‚‹",
          "content_type_preference": "èª¬æ˜è¨˜äº‹",
          "target_audience": "ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼",
          "search_volume_estimate": "ä¸­",
          "competition_level": "ä¸­"
        }
        EOF

  - id: "competitor-content-research"
    name: "ç«¶åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®èª¿æŸ»"
    phase: "information_gathering"
    github_job: "competitor-research"
    type: "research"
    implementation: "mcp"
    tool: "web-search"
    duration_minutes: 4
    dependencies: ["search-intent-analysis"]
    parallel_group: "research_parallel"
    
    github_steps:
      - name: "Research competitor content"
        shell: "bash"
        script: |
          echo "ğŸ” Researching competitor content for: $NORMALIZED_KEYWORDS"
          
          mkdir -p .logs/competitor-research
          
          # ä¸Šä½ã‚µã‚¤ãƒˆã®åˆ†æ
          if [ "$WEB_SEARCH_METHOD" = "google_api" ]; then
            # Google Custom Search APIä½¿ç”¨
            curl -s "https://www.googleapis.com/customsearch/v1?key=${{ secrets.GOOGLE_SEARCH_API_KEY }}&cx=${{ secrets.GOOGLE_SEARCH_ENGINE_ID }}&q=${NORMALIZED_KEYWORDS}&num=10" > .logs/competitor-research/top-results.json
          else
            # Claudeå†…è”µçŸ¥è­˜ä½¿ç”¨
            echo '{"items": [], "note": "Using Claude built-in knowledge for competitor analysis"}' > .logs/competitor-research/top-results.json
          fi
          
          # ç«¶åˆåˆ†æã‚’Claude Codeã§å®Ÿè¡Œ
          claude-code --prompt "ä»¥ä¸‹ã®æ¤œç´¢ä¸Šä½ã‚µã‚¤ãƒˆã‚’åˆ†æã—ã¦ã€ç«¶åˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç‰¹å¾´ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
          
          æ¤œç´¢çµæœ: $(cat .logs/competitor-research/top-results.json)
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"common_topics\": [\"å…±é€šã—ã¦æ‰±ã‚ã‚Œã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯\"],
            \"content_gaps\": [\"ç«¶åˆãŒæ‰±ã£ã¦ã„ãªã„å†…å®¹\"],
            \"average_word_count\": \"æ¨å®šæ–‡å­—æ•°\",
            \"common_structure\": [\"ã‚ˆãä½¿ã‚ã‚Œã‚‹æ§‹æˆ\"],
            \"unique_angles\": [\"å·®åˆ¥åŒ–ã®ã‚¢ã‚¤ãƒ‡ã‚¢\"],
            \"seo_patterns\": [\"SEOçš„ãªç‰¹å¾´\"]
          }" > .logs/competitor-research/analysis.json
          
          COMPETITOR_ANALYSIS=$(cat .logs/competitor-research/analysis.json)
          echo "COMPETITOR_ANALYSIS=$COMPETITOR_ANALYSIS" >> $GITHUB_ENV
          
          echo "âœ… Competitor research completed"
          
    validation:
      criteria:
        - "ç«¶åˆåˆ†æçµæœãŒJSONå½¢å¼ã§å­˜åœ¨ã™ã‚‹"
        - "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—ãŒç‰¹å®šã•ã‚Œã¦ã„ã‚‹"
        - "å·®åˆ¥åŒ–ãƒã‚¤ãƒ³ãƒˆãŒæ˜ç¢ºã«ãªã£ã¦ã„ã‚‹"
      validation_script: |
        if [ ! -f ".logs/competitor-research/analysis.json" ]; then
          echo "âŒ VALIDATION FAILED: Competitor analysis missing"
          exit 1
        fi
        if ! jq -e '.content_gaps' .logs/competitor-research/analysis.json > /dev/null; then
          echo "âŒ VALIDATION FAILED: Content gaps not identified"
          exit 1
        fi
        echo "âœ… Validation passed: Competitor research complete"

  - id: "user-questions-extraction"
    name: "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç–‘å•ãƒ»è³ªå•ã®æŠ½å‡º"
    phase: "information_gathering"
    github_job: "user-questions"
    type: "analysis"
    implementation: "mcp"
    tool: "web-search"
    duration_minutes: 3
    dependencies: ["search-intent-analysis"]
    parallel_group: "research_parallel"
    
    github_steps:
      - name: "Extract user questions"
        shell: "bash"
        script: |
          echo "â“ Extracting user questions for: $NORMALIZED_KEYWORDS"
          
          mkdir -p .logs/user-questions
          
          # é–¢é€£ã™ã‚‹è³ªå•ã‚µã‚¤ãƒˆã‚’æ¤œç´¢
          claude-code --mcp-config=~/.claude/mcp-kamuicode.json --mcp web-search --query "site:yahoo.co.jp $NORMALIZED_KEYWORDS è³ªå•" --limit 5 > .logs/user-questions/yahoo-questions.json
          claude-code --mcp-config=~/.claude/mcp-kamuicode.json --mcp web-search --query "$NORMALIZED_KEYWORDS ã‚ˆãã‚ã‚‹è³ªå• FAQ" --limit 5 > .logs/user-questions/faq-results.json
          
          # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åˆ†æãƒ»æŠ½å‡º
          claude-code --prompt "ä»¥ä¸‹ã®æ¤œç´¢çµæœã‹ã‚‰ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚ˆãæŒã¤ç–‘å•ã‚„è³ªå•ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š
          
          YahooçŸ¥æµè¢‹ç­‰ã®çµæœ: $(cat .logs/user-questions/yahoo-questions.json)
          FAQé–¢é€£ã®çµæœ: $(cat .logs/user-questions/faq-results.json)
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"frequent_questions\": [\"ã‚ˆãã‚ã‚‹è³ªå•ã®ãƒªã‚¹ãƒˆ\"],
            \"beginner_questions\": [\"åˆå¿ƒè€…å‘ã‘ã®è³ªå•\"],
            \"advanced_questions\": [\"ä¸Šç´šè€…å‘ã‘ã®è³ªå•\"],
            \"practical_questions\": [\"å®Ÿè·µçš„ãªè³ªå•\"],
            \"concern_points\": [\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¸å®‰ãƒ»æ‡¸å¿µç‚¹\"]
          }" > .logs/user-questions/extracted.json
          
          USER_QUESTIONS=$(cat .logs/user-questions/extracted.json)
          echo "USER_QUESTIONS=$USER_QUESTIONS" >> $GITHUB_ENV
          
          echo "âœ… User questions extraction completed"

  # === æˆ¦ç•¥ç«‹æ¡ˆæ®µéš ===
  - id: "content-gap-identification"
    name: "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—ã®ç‰¹å®š"
    phase: "strategy_planning"
    github_job: "content-gap-analysis"
    type: "analysis"
    implementation: "ai"
    duration_minutes: 2
    dependencies: ["competitor-content-research", "user-questions-extraction"]
    parallel_group: null
    
    github_steps:
      - name: "Identify content gaps"
        shell: "bash"
        script: |
          echo "ğŸ¯ Identifying content gaps..."
          
          mkdir -p .logs/content-gaps
          
          # ç«¶åˆåˆ†æã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•ã‚’çµ±åˆã—ã¦ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®š
          claude-code --prompt "ç«¶åˆåˆ†æçµæœã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’åŸºã«ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ï¼š
          
          ç«¶åˆåˆ†æ: $COMPETITOR_ANALYSIS
          ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: $USER_QUESTIONS
          æ¤œç´¢æ„å›³: $SEARCH_INTENT
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"critical_gaps\": [\"é‡è¦ã ãŒç«¶åˆãŒæ‰±ã£ã¦ã„ãªã„å†…å®¹\"],
            \"opportunity_areas\": [\"å·®åˆ¥åŒ–ã§ãã‚‹é ˜åŸŸ\"],
            \"user_pain_points\": [\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è§£æ±ºã•ã‚Œã¦ã„ãªã„èª²é¡Œ\"],
            \"content_depth_opportunities\": [\"ã‚ˆã‚Šæ·±ãæ˜ã‚Šä¸‹ã’ã‚‹ã¹ãå†…å®¹\"],
            \"unique_value_proposition\": \"ã“ã®è¨˜äº‹ç‹¬è‡ªã®ä¾¡å€¤ææ¡ˆ\"
          }" > .logs/content-gaps/gaps.json
          
          CONTENT_GAPS=$(cat .logs/content-gaps/gaps.json)
          echo "CONTENT_GAPS=$CONTENT_GAPS" >> $GITHUB_ENV
          
          echo "âœ… Content gap identification completed"

  - id: "seo-strategy-design"
    name: "SEOæˆ¦ç•¥ãƒ»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é…ç½®è¨ˆç”»"
    phase: "strategy_planning"
    github_job: "seo-strategy"
    type: "planning"
    implementation: "ai"
    duration_minutes: 4
    dependencies: ["content-gap-identification"]
    parallel_group: null
    
    github_steps:
      - name: "Design SEO strategy"
        shell: "bash"
        script: |
          echo "ğŸ“Š Designing SEO strategy..."
          
          mkdir -p .logs/seo-strategy
          
          # SEOæˆ¦ç•¥ã®è¨­è¨ˆ
          claude-code --prompt "ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€åŒ…æ‹¬çš„ãªSEOæˆ¦ç•¥ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ï¼š
          
          ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: $NORMALIZED_KEYWORDS
          æ¤œç´¢æ„å›³: $SEARCH_INTENT
          ç«¶åˆåˆ†æ: $COMPETITOR_ANALYSIS
          ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—: $CONTENT_GAPS
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"primary_keyword\": \"ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\",
            \"secondary_keywords\": [\"é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ\"],
            \"long_tail_keywords\": [\"ãƒ­ãƒ³ã‚°ãƒ†ãƒ¼ãƒ«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\"],
            \"keyword_density_targets\": {
              \"primary\": \"1-3%\",
              \"secondary\": \"0.5-1%\"
            },
            \"heading_structure\": {
              \"h1\": \"è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«æ¡ˆ\",
              \"h2_sections\": [\"å¤§è¦‹å‡ºã—æ¡ˆã®ãƒªã‚¹ãƒˆ\"],
              \"h3_subsections\": [\"å°è¦‹å‡ºã—æ¡ˆã®ãƒªã‚¹ãƒˆ\"]
            },
            \"meta_description\": \"ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æ¡ˆ\",
            \"internal_linking_strategy\": [\"å†…éƒ¨ãƒªãƒ³ã‚¯æˆ¦ç•¥\"],
            \"target_word_count\": \"æ¨å¥¨æ–‡å­—æ•°\"
          }" > .logs/seo-strategy/strategy.json
          
          SEO_STRATEGY=$(cat .logs/seo-strategy/strategy.json)
          echo "SEO_STRATEGY=$SEO_STRATEGY" >> $GITHUB_ENV
          
          echo "âœ… SEO strategy design completed"

  - id: "article-outline-generation"
    name: "è¨˜äº‹ã®ç« ç«‹ã¦ãƒ»è¦‹å‡ºã—æ§‹æˆ"
    phase: "strategy_planning"
    github_job: "article-outline"
    type: "generation"
    implementation: "ai"
    duration_minutes: 3
    dependencies: ["seo-strategy-design"]
    parallel_group: null
    
    github_steps:
      - name: "Generate article outline"
        shell: "bash"
        script: |
          echo "ğŸ“ Generating article outline..."
          
          mkdir -p .logs/article-outline
          
          # è¨˜äº‹ã®è©³ç´°æ§‹æˆã‚’ç”Ÿæˆ
          claude-code --prompt "ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€è©³ç´°ãªè¨˜äº‹æ§‹æˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
          
          SEOæˆ¦ç•¥: $SEO_STRATEGY
          ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚®ãƒ£ãƒƒãƒ—: $CONTENT_GAPS
          ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: $USER_QUESTIONS
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"article_title\": \"SEOæœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«\",
            \"introduction\": {
              \"hook\": \"èª­è€…ã‚’å¼•ãã¤ã‘ã‚‹å°å…¥\",
              \"problem_statement\": \"èª­è€…ã®èª²é¡Œæç¤º\",
              \"article_promise\": \"è¨˜äº‹ã§å¾—ã‚‰ã‚Œã‚‹ä¾¡å€¤\"
            },
            \"main_sections\": [
              {
                \"section_number\": 1,
                \"h2_title\": \"å¤§è¦‹å‡ºã—\",
                \"content_points\": [\"å«ã‚ã‚‹ã¹ãå†…å®¹ãƒã‚¤ãƒ³ãƒˆ\"],
                \"h3_subsections\": [\"å°è¦‹å‡ºã—ãƒªã‚¹ãƒˆ\"],
                \"estimated_words\": \"æ¨å®šæ–‡å­—æ•°\",
                \"keywords_to_include\": [\"å«ã‚ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰\"]
              }
            ],
            \"conclusion\": {
              \"summary_points\": [\"ã¾ã¨ã‚ã®ãƒã‚¤ãƒ³ãƒˆ\"],
              \"call_to_action\": \"èª­è€…ã¸ã®è¡Œå‹•å‘¼ã³ã‹ã‘\"
            },
            \"estimated_total_words\": \"ç·æ¨å®šæ–‡å­—æ•°\"
          }" > .logs/article-outline/outline.json
          
          ARTICLE_OUTLINE=$(cat .logs/article-outline/outline.json)
          echo "ARTICLE_OUTLINE=$ARTICLE_OUTLINE" >> $GITHUB_ENV
          
          echo "âœ… Article outline generation completed"

  # === ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆæ®µéš ===
  - id: "introduction-writing"
    name: "å°å…¥æ–‡ã®åŸ·ç­†"
    phase: "content_creation"
    github_job: "write-introduction"
    type: "generation"
    implementation: "ai"
    duration_minutes: 3
    dependencies: ["article-outline-generation"]
    parallel_group: null
    
    github_steps:
      - name: "Write introduction"
        shell: "bash"
        script: |
          echo "âœï¸ Writing article introduction..."
          
          mkdir -p .logs/content-creation
          
          # å°å…¥æ–‡ã®åŸ·ç­†
          claude-code --prompt "ä»¥ä¸‹ã®æ§‹æˆã«åŸºã¥ã„ã¦ã€é­…åŠ›çš„ãªå°å…¥æ–‡ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ï¼š
          
          è¨˜äº‹æ§‹æˆ: $ARTICLE_OUTLINE
          æ¤œç´¢æ„å›³: $SEARCH_INTENT
          
          è¦ä»¶:
          - èª­è€…ã®èˆˆå‘³ã‚’å¼•ãæœ€åˆã®ä¸€æ–‡
          - èª­è€…ã®èª²é¡Œãƒ»æ‚©ã¿ã«å…±æ„Ÿ
          - è¨˜äº‹ã§å¾—ã‚‰ã‚Œã‚‹ä¾¡å€¤ã‚’æ˜ç¢ºã«æç¤º
          - è‡ªç„¶ã«ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹
          - 300-500æ–‡å­—ç¨‹åº¦
          
          ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚" > .logs/content-creation/introduction.md
          
          echo "âœ… Introduction writing completed"
          
    validation:
      criteria:
        - "å°å…¥æ–‡ãŒãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ä½œæˆã•ã‚Œã¦ã„ã‚‹"
        - "æ–‡å­—æ•°ãŒé©åˆ‡ãªç¯„å›²å†…"
        - "ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹"
      validation_script: |
        if [ ! -f ".logs/content-creation/introduction.md" ]; then
          echo "âŒ VALIDATION FAILED: Introduction file missing"
          exit 1
        fi
        WORD_COUNT=$(wc -c < .logs/content-creation/introduction.md)
        if [ $WORD_COUNT -lt 200 ] || [ $WORD_COUNT -gt 800 ]; then
          echo "âŒ VALIDATION FAILED: Word count out of range ($WORD_COUNT)"
          exit 1
        fi
        echo "âœ… Validation passed: Introduction written successfully"

  - id: "main-content-section-1"
    name: "ç¬¬1ç« ã®è©³ç´°åŸ·ç­†"
    phase: "content_creation"
    github_job: "write-section-1"
    type: "generation"
    implementation: "ai"
    duration_minutes: 5
    dependencies: ["introduction-writing"]
    parallel_group: "main_content_parallel"
    
    github_steps:
      - name: "Write main section 1"
        shell: "bash"
        script: |
          echo "âœï¸ Writing main section 1..."
          
          mkdir -p .logs/content-creation
          
          # ç¬¬1ç« ã®åŸ·ç­†
          SECTION_1=$(echo "$ARTICLE_OUTLINE" | jq -r '.main_sections[0]')
          
          claude-code --prompt "ä»¥ä¸‹ã®æ§‹æˆã«åŸºã¥ã„ã¦ã€ç¬¬1ç« ã‚’è©³ç´°ã«åŸ·ç­†ã—ã¦ãã ã•ã„ï¼š
          
          ç« ã®æƒ…å ±: $SECTION_1
          å…¨ä½“æ§‹æˆ: $ARTICLE_OUTLINE
          SEOæˆ¦ç•¥: $SEO_STRATEGY
          
          è¦ä»¶:
          - è¦‹å‡ºã—æ§‹é€ ã‚’é©åˆ‡ã«ä½¿ç”¨ï¼ˆH2, H3ï¼‰
          - è‡ªç„¶ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é…ç½®
          - èª­ã¿ã‚„ã™ã„æ®µè½æ§‹æˆ
          - å…·ä½“ä¾‹ã‚„å®Ÿä¾‹ã‚’å«ã‚ã‚‹
          - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç–‘å•ã«ç­”ãˆã‚‹å†…å®¹
          
          ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚" > .logs/content-creation/section-1.md
          
          echo "âœ… Main section 1 writing completed"

  - id: "main-content-section-2"
    name: "ç¬¬2ç« ã®è©³ç´°åŸ·ç­†"
    phase: "content_creation"
    github_job: "write-section-2"
    type: "generation"
    implementation: "ai"
    duration_minutes: 5
    dependencies: ["introduction-writing"]
    parallel_group: "main_content_parallel"
    
    github_steps:
      - name: "Write main section 2"
        shell: "bash"
        script: |
          echo "âœï¸ Writing main section 2..."
          
          SECTION_2=$(echo "$ARTICLE_OUTLINE" | jq -r '.main_sections[1]')
          
          claude-code --prompt "ä»¥ä¸‹ã®æ§‹æˆã«åŸºã¥ã„ã¦ã€ç¬¬2ç« ã‚’è©³ç´°ã«åŸ·ç­†ã—ã¦ãã ã•ã„ï¼š
          
          ç« ã®æƒ…å ±: $SECTION_2
          å…¨ä½“æ§‹æˆ: $ARTICLE_OUTLINE
          SEOæˆ¦ç•¥: $SEO_STRATEGY
          
          è¦ä»¶:
          - è¦‹å‡ºã—æ§‹é€ ã‚’é©åˆ‡ã«ä½¿ç”¨ï¼ˆH2, H3ï¼‰
          - è‡ªç„¶ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é…ç½®
          - å‰ç« ã¨ã®è«–ç†çš„ãªã¤ãªãŒã‚Š
          - å®Ÿè·µçš„ãªå†…å®¹ã‚’å«ã‚ã‚‹
          
          ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚" > .logs/content-creation/section-2.md
          
          echo "âœ… Main section 2 writing completed"

  - id: "conclusion-writing"
    name: "ã¾ã¨ã‚ãƒ»çµè«–éƒ¨åˆ†ã®åŸ·ç­†"
    phase: "content_creation"  
    github_job: "write-conclusion"
    type: "generation"
    implementation: "ai"
    duration_minutes: 3
    dependencies: ["main-content-section-1", "main-content-section-2"]
    parallel_group: null
    
    github_steps:
      - name: "Write conclusion"
        shell: "bash"
        script: |
          echo "âœï¸ Writing article conclusion..."
          
          claude-code --prompt "ä»¥ä¸‹ã®å†…å®¹ã‚’åŸºã«ã€åŠ¹æœçš„ãªçµè«–éƒ¨åˆ†ã‚’åŸ·ç­†ã—ã¦ãã ã•ã„ï¼š
          
          è¨˜äº‹æ§‹æˆ: $ARTICLE_OUTLINE
          å°å…¥æ–‡: $(cat .logs/content-creation/introduction.md)
          ç¬¬1ç« : $(cat .logs/content-creation/section-1.md)
          ç¬¬2ç« : $(cat .logs/content-creation/section-2.md)
          
          è¦ä»¶:
          - è¨˜äº‹ã®è¦ç‚¹ã‚’ç°¡æ½”ã«ã¾ã¨ã‚
          - èª­è€…ã¸ã®å…·ä½“çš„ãªè¡Œå‹•ææ¡ˆ
          - è‡ªç„¶ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹
          - èª­è€…ã®ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å‘ä¸Š
          
          ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚" > .logs/content-creation/conclusion.md
          
          echo "âœ… Conclusion writing completed"

  - id: "article-assembly"
    name: "è¨˜äº‹å…¨ä½“ã®çµ„ã¿ç«‹ã¦"
    phase: "content_creation"
    github_job: "assemble-article"
    type: "integration"
    implementation: "script"
    duration_minutes: 2
    dependencies: ["conclusion-writing"]
    parallel_group: null
    
    github_steps:
      - name: "Assemble complete article"
        shell: "bash"
        script: |
          echo "ğŸ”— Assembling complete article..."
          
          mkdir -p .outputs
          
          # è¨˜äº‹å…¨ä½“ã‚’çµ„ã¿ç«‹ã¦
          cat > .outputs/complete-article.md << EOF
          # $(echo "$ARTICLE_OUTLINE" | jq -r '.article_title')
          
          $(cat .logs/content-creation/introduction.md)
          
          $(cat .logs/content-creation/section-1.md)
          
          $(cat .logs/content-creation/section-2.md)
          
          $(cat .logs/content-creation/conclusion.md)
          EOF
          
          # æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆ
          TOTAL_WORDS=$(wc -c < .outputs/complete-article.md)
          echo "Total article length: $TOTAL_WORDS characters"
          
          # ã‚¢ã‚»ãƒ³ãƒ–ãƒªçµæœã‚’ãƒ­ã‚°ä¿å­˜
          cat > .logs/content-creation/assembly-log.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "total_characters": $TOTAL_WORDS,
            "sections_included": ["introduction", "section-1", "section-2", "conclusion"],
            "assembly_status": "success"
          }
          EOF
          
          echo "âœ… Article assembly completed"

  # === å“è³ªä¿è¨¼æ®µéš ===
  - id: "keyword-density-check"
    name: "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦ã®æœ€é©åŒ–"
    phase: "quality_assurance"
    github_job: "keyword-optimization"
    type: "validation"
    implementation: "script"
    duration_minutes: 3
    dependencies: ["article-assembly"]
    parallel_group: "quality_checks"
    
    github_steps:
      - name: "Check and optimize keyword density"
        shell: "bash"
        script: |
          echo "ğŸ“Š Checking keyword density..."
          
          mkdir -p .logs/quality-checks
          
          # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦ã‚’ãƒã‚§ãƒƒã‚¯
          PRIMARY_KEYWORD=$(echo "$SEO_STRATEGY" | jq -r '.primary_keyword')
          ARTICLE_CONTENT=$(cat .outputs/complete-article.md)
          
          # æ–‡å­—æ•°ã¨å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
          TOTAL_CHARS=$(echo "$ARTICLE_CONTENT" | wc -c)
          KEYWORD_COUNT=$(echo "$ARTICLE_CONTENT" | grep -io "$PRIMARY_KEYWORD" | wc -l)
          DENSITY=$(echo "scale=2; $KEYWORD_COUNT * 100 / ($TOTAL_CHARS / 100)" | bc -l)
          
          # çµæœã‚’ãƒ­ã‚°ä¿å­˜
          cat > .logs/quality-checks/keyword-density.json << EOF
          {
            "primary_keyword": "$PRIMARY_KEYWORD",
            "total_characters": $TOTAL_CHARS,
            "keyword_occurrences": $KEYWORD_COUNT,
            "density_percentage": $DENSITY,
            "recommended_range": "1-3%",
            "status": "$([ $(echo "$DENSITY < 1" | bc -l) -eq 1 ] && echo "too_low" || [ $(echo "$DENSITY > 3" | bc -l) -eq 1 ] && echo "too_high" || echo "optimal")"
          }
          EOF
          
          echo "Keyword density: $DENSITY%"
          echo "âœ… Keyword density check completed"
          
    validation:
      criteria:
        - "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦ãŒ1-3%ã®ç¯„å›²å†…"
        - "ãƒ¡ã‚¤ãƒ³ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒé©åˆ‡ã«é…ç½®ã•ã‚Œã¦ã„ã‚‹"
      validation_script: |
        DENSITY=$(jq -r '.density_percentage' .logs/quality-checks/keyword-density.json)
        if (( $(echo "$DENSITY < 1" | bc -l) )); then
          echo "âš ï¸ WARNING: Keyword density too low ($DENSITY%)"
        elif (( $(echo "$DENSITY > 3" | bc -l) )); then
          echo "âš ï¸ WARNING: Keyword density too high ($DENSITY%)"
        else
          echo "âœ… Validation passed: Keyword density optimal ($DENSITY%)"
        fi

  - id: "readability-analysis"
    name: "èª­ã¿ã‚„ã™ã•ã®æ”¹å–„"
    phase: "quality_assurance"
    github_job: "readability-check"
    type: "validation"
    implementation: "ai"
    duration_minutes: 4
    dependencies: ["article-assembly"]
    parallel_group: "quality_checks"
    
    github_steps:
      - name: "Analyze and improve readability"
        shell: "bash"
        script: |
          echo "ğŸ“– Analyzing readability..."
          
          ARTICLE_CONTENT=$(cat .outputs/complete-article.md)
          
          # Claude Code ã§èª­ã¿ã‚„ã™ã•ã‚’åˆ†æãƒ»æ”¹å–„
          claude-code --prompt "ä»¥ä¸‹ã®è¨˜äº‹ã®èª­ã¿ã‚„ã™ã•ã‚’åˆ†æã—ã€æ”¹å–„æ¡ˆã‚’æç¤ºã—ã¦ãã ã•ã„ï¼š
          
          è¨˜äº‹å†…å®¹: $ARTICLE_CONTENT
          
          åˆ†æé …ç›®:
          - æ–‡ç« ã®é•·ã•ã¨è¤‡é›‘ã•
          - æ®µè½ã®æ§‹æˆ
          - å°‚é–€ç”¨èªã®ä½¿ç”¨
          - è«–ç†çš„ãªæµã‚Œ
          - è¦–è¦šçš„ãªèª­ã¿ã‚„ã™ã•
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"readability_score\": \"1-10ç‚¹ã§è©•ä¾¡\",
            \"strengths\": [\"è‰¯ã„ç‚¹ã®ãƒªã‚¹ãƒˆ\"],
            \"improvement_areas\": [\"æ”¹å–„ã™ã¹ãç®‡æ‰€\"],
            \"specific_suggestions\": [\"å…·ä½“çš„ãªæ”¹å–„æ¡ˆ\"],
            \"revised_sections\": {\"ã‚»ã‚¯ã‚·ãƒ§ãƒ³å\": \"æ”¹å–„ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ\"}
          }" > .logs/quality-checks/readability-analysis.json
          
          echo "âœ… Readability analysis completed"

  - id: "final-seo-optimization"
    name: "æœ€çµ‚SEOæœ€é©åŒ–"
    phase: "quality_assurance"
    github_job: "final-seo-check"
    type: "optimization"
    implementation: "ai"
    duration_minutes: 4
    dependencies: ["keyword-density-check", "readability-analysis"]
    parallel_group: null
    
    github_steps:
      - name: "Final SEO optimization"
        shell: "bash"
        script: |
          echo "ğŸ” Final SEO optimization..."
          
          ARTICLE_CONTENT=$(cat .outputs/complete-article.md)
          KEYWORD_ANALYSIS=$(cat .logs/quality-checks/keyword-density.json)
          READABILITY_ANALYSIS=$(cat .logs/quality-checks/readability-analysis.json)
          
          # æœ€çµ‚çš„ãªSEOæœ€é©åŒ–
          claude-code --prompt "ä»¥ä¸‹ã®æƒ…å ±ã‚’åŸºã«ã€è¨˜äº‹ã‚’æœ€çµ‚çš„ã«SEOæœ€é©åŒ–ã—ã¦ãã ã•ã„ï¼š
          
          ç¾åœ¨ã®è¨˜äº‹: $ARTICLE_CONTENT
          ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ: $KEYWORD_ANALYSIS
          èª­ã¿ã‚„ã™ã•åˆ†æ: $READABILITY_ANALYSIS
          SEOæˆ¦ç•¥: $SEO_STRATEGY
          
          æœ€é©åŒ–é …ç›®:
          - ã‚¿ã‚¤ãƒˆãƒ«ã®æœ€é©åŒ–
          - ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³
          - è¦‹å‡ºã—æ§‹é€ ã®èª¿æ•´
          - ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é…ç½®ã®æ”¹å–„
          - å†…éƒ¨ãƒªãƒ³ã‚¯ææ¡ˆ
          
          æœ€é©åŒ–ã•ã‚ŒãŸå®Œå…¨ãªè¨˜äº‹ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚" > .outputs/seo-optimized-article.md
          
          echo "âœ… Final SEO optimization completed"

  # === å…¬é–‹æº–å‚™æ®µéš ===
  - id: "meta-data-generation"
    name: "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ»SNSç”¨ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"
    phase: "publication_preparation"
    github_job: "generate-metadata"
    type: "generation"
    implementation: "ai"
    duration_minutes: 2
    dependencies: ["final-seo-optimization"]
    parallel_group: "publication_parallel"
    
    github_steps:
      - name: "Generate metadata"
        shell: "bash"
        script: |
          echo "ğŸ“‹ Generating metadata..."
          
          OPTIMIZED_ARTICLE=$(cat .outputs/seo-optimized-article.md)
          
          claude-code --prompt "ä»¥ä¸‹ã®è¨˜äº‹ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š
          
          è¨˜äº‹å†…å®¹: $OPTIMIZED_ARTICLE
          SEOæˆ¦ç•¥: $SEO_STRATEGY
          
          ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
          {
            \"meta_title\": \"SEOæœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ60æ–‡å­—ä»¥å†…ï¼‰\",
            \"meta_description\": \"èª¬æ˜æ–‡ï¼ˆ160æ–‡å­—ä»¥å†…ï¼‰\",
            \"og_title\": \"SNSç”¨ã‚¿ã‚¤ãƒˆãƒ«\",
            \"og_description\": \"SNSç”¨èª¬æ˜æ–‡\",
            \"twitter_title\": \"Twitterç”¨ã‚¿ã‚¤ãƒˆãƒ«\",
            \"twitter_description\": \"Twitterç”¨èª¬æ˜æ–‡\",
            \"tags\": [\"ã‚¿ã‚°ã®ãƒªã‚¹ãƒˆ\"],
            \"categories\": [\"ã‚«ãƒ†ã‚´ãƒªã®ãƒªã‚¹ãƒˆ\"],
            \"estimated_read_time\": \"èª­äº†æ™‚é–“ï¼ˆåˆ†ï¼‰\"
          }" > .outputs/metadata.json
          
          echo "âœ… Metadata generation completed"

  - id: "publication-package"
    name: "å…¬é–‹ç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆ"
    phase: "publication_preparation"
    github_job: "create-publication-package"
    type: "integration"
    implementation: "script"
    duration_minutes: 2
    dependencies: ["meta-data-generation"]
    parallel_group: null
    
    github_steps:
      - name: "Create publication package"
        shell: "bash"
        script: |
          echo "ğŸ“¦ Creating publication package..."
          
          mkdir -p .final-output
          
          # æœ€çµ‚ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ä½œæˆ
          cp .outputs/seo-optimized-article.md .final-output/
          cp .outputs/metadata.json .final-output/
          
          # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
          cat > .final-output/project-info.json << EOF
          {
            "project_name": "SEO Blog Article Creation",
            "created_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "github_run_id": "${{ github.run_id }}",
            "keywords": "$NORMALIZED_KEYWORDS",
            "total_duration_minutes": 60,
            "quality_scores": {
              "keyword_density": $(jq -r '.density_percentage' .logs/quality-checks/keyword-density.json || echo "null"),
              "readability_score": $(jq -r '.readability_score' .logs/quality-checks/readability-analysis.json || echo "null")
            },
            "files": [
              "seo-optimized-article.md",
              "metadata.json",
              "project-info.json"
            ]
          }
          EOF
          
          # ZIPãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ä½œæˆ
          cd .final-output && zip -r ../blog-article-package.zip . && cd ..
          
          echo "âœ… Publication package created"
          
    success_links:
      package_download: "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts"
      article_preview: "${{ github.server_url }}/${{ github.repository }}/blob/main/.final-output/seo-optimized-article.md"
      metadata_view: "${{ github.server_url }}/${{ github.repository }}/blob/main/.final-output/metadata.json"

# å®Ÿè¡Œãƒ•ãƒ­ãƒ¼è¨­è¨ˆ
execution_flow:
  - stage: 0
    name: "äº‹å‰ãƒ†ã‚¹ãƒˆ"
    parallel: false
    jobs: ["connectivity-test"]
    
  - stage: 1
    name: "æƒ…å ±åé›†"
    parallel: false
    jobs: ["keyword-processing", "search-intent-analysis"]
    
  - stage: 2  
    name: "ä¸¦åˆ—ãƒªã‚µãƒ¼ãƒ"
    parallel: true
    jobs: ["competitor-research", "user-questions"]
    
  - stage: 3
    name: "æˆ¦ç•¥ç«‹æ¡ˆ" 
    parallel: false
    jobs: ["content-gap-analysis", "seo-strategy", "article-outline"]
    
  - stage: 4
    name: "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆ"
    parallel: true
    jobs: ["write-introduction"]
    then:
      parallel: true
      jobs: ["write-section-1", "write-section-2"]
    then:
      parallel: false
      jobs: ["write-conclusion", "assemble-article"]
      
  - stage: 5
    name: "å“è³ªä¿è¨¼"
    parallel: true
    jobs: ["keyword-optimization", "readability-check"]
    then:
      parallel: false  
      jobs: ["final-seo-check"]
      
  - stage: 6
    name: "å…¬é–‹æº–å‚™"
    parallel: true
    jobs: ["generate-metadata"]
    then:
      parallel: false
      jobs: ["create-publication-package"]

# å…¨ä½“çš„ãªã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
global_error_handling:
  max_retries: 3
  retry_delay_seconds: 30
  critical_failure_notification: true
  fallback_mode: "simplified_article"
  
# æˆåŠŸåŸºæº–
success_criteria:
  - "SEOæœ€é©åŒ–ã•ã‚ŒãŸå®Œå…¨ãªè¨˜äº‹ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹"
  - "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¯†åº¦ãŒé©åˆ‡ãªç¯„å›²å†…"
  - "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒå®Œå…¨ã«ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹"
  - "ã™ã¹ã¦ã®å“è³ªãƒã‚§ãƒƒã‚¯ãŒãƒ‘ã‚¹ã—ã¦ã„ã‚‹"
  - "å…¬é–‹ç”¨ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹"

# ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆä¿æŒ
artifacts:
  - name: "blog-article-logs"
    path: ".logs/"
    retention_days: 7
  - name: "blog-article-outputs"  
    path: ".outputs/"
    retention_days: 30
  - name: "final-publication-package"
    path: ".final-output/"
    retention_days: 90