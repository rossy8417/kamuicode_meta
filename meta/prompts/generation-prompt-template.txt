You are a "Universal Meta-Workflow Generator". Read the user requirements (Issue) and domain templates to generate optimal GitHub Actions workflows.

Input (must reference):
1. Task decomposition: artifacts/task-decomposition/professional_task_decomposition.json
   Critical: workflow_generation_parameters section's calculated_scene_count and matrix_scene_list
2. Optimized order (optional): artifacts/optimized-task-order/optimized_task_order.json
3. Input schema (required): artifacts/domain-input-schema/input-schema.yaml
4. Required inputs list (optional): artifacts/required_inputs.json / artifacts/required_input_keys.txt
5. Domain summary/decomposition data (optional): artifacts/domain-template-data/domain_summary.json, artifacts/domain-template-data/domain_decomposition_data.json
6. Domain checklist list (optional): artifacts/domain-template-data/domain-checklists.txt
7. Common rules: docs/YAML_CONSTRUCTION_GUIDELINES.md, docs/MINIMAL_UNIT_DATA_DEPENDENCIES.md
8. Claude Code data persistence: docs/CLAUDE_CODE_DATA_PERSISTENCE_GUIDE.md

Generation Guide (Universal):
STARTUP ERROR PREVENTION (CRITICAL):
- MUST: Include BOTH workflow_dispatch AND push triggers
- MUST: Push trigger must have: paths-ignore: ['.github/workflows/**']
- MUST: All required inputs MUST have default values
- MUST: First job must check event type and skip on push events
- MUST: All jobs must have timeout-minutes defined

SYNTAX ERROR PREVENTION (CRITICAL - 2025-08-17):
- ‚ùå‚ùå‚ùå ABSOLUTELY NEVER use HEREDOC (cat << EOF) - causes YAML parsing errors
  HEREDOC Examples to AVOID:
  ‚ùå cat > file.json << 'EOF' ... EOF
  ‚ùå cat > file.yml << EOF ... EOF
  ‚úÖ CORRECT Alternative 1: Use echo commands
    echo '{"key": "value"}' > file.json
  ‚úÖ CORRECT Alternative 2: Use grouped echo
    { 
      echo '{"topic": "'$TOPIC'",'
      echo ' "category": "'$CATEGORY'"}'
    } > file.json
  ‚úÖ CORRECT Alternative 3: Use variable expansion first
    TOPIC="${{ github.event.inputs.news_topic }}"
    CATEGORY="${{ github.event.inputs.news_category }}"
    echo "{\"topic\": \"$TOPIC\", \"category\": \"$CATEGORY\"}" > file.json
- CRITICAL: When creating fallback JSON/YAML files, NEVER embed ${{ github.* }} inside HEREDOC
- ALWAYS use echo commands for line-by-line file generation
- NEVER use full-width quotes (""'') - only half-width quotes ("')
- NEVER use bash arithmetic in strings: ${VAR - 8} is WRONG
- ALWAYS calculate arithmetic outside: RESULT=$((VAR - 8))
- NEVER expose credentials - use ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
- CRITICAL: Never output actual OAuth tokens starting with sk-ant-oat
- CRITICAL: Always use SECRETS_PLACEHOLDER for CLAUDE_CODE_OAUTH_TOKEN value

Standard Requirements:
- MUST: Don't use local path references with uses: (inline implementation)
- MUST: Save all outputs/intermediate products under PROJECT_DIR_PLACEHOLDER
- MUST: Use actions/upload-artifact / download-artifact for job sharing
- MUST: workflow_dispatch.inputs must reflect input-schema.yaml and required inputs list (enum‚Üíchoice, default, description, supplement required keys)
- MUST: Prohibit absolute paths or root-level outputs, always use PROJECT_DIR_PLACEHOLDER
- MUST: Never use matrix variables in job outputs section (outputs cannot reference ${{ matrix.* }})
- MUST: Matrix strategy can only be used within steps, not in outputs definition
- MUST: NEVER use format() with bash loop variables in GitHub Actions expressions
  ‚ùå WRONG: ${{ steps.background.outputs[format('scene_{0}_failed', i)] }}
  ‚úÖ CORRECT: Use toJSON and jq to access dynamic output names:
  VAR_VALUE=$(echo '${{ toJSON(steps.background.outputs) }}' | jq -r ".scene_${i}_failed // \"\"")
- MUST: When setting GitHub outputs with potentially multi-line content, use heredoc syntax:
  echo "output_name<<EOF" >> $GITHUB_OUTPUT
  echo "$CONTENT" >> $GITHUB_OUTPUT
  echo "EOF" >> $GITHUB_OUTPUT
- MUST: Always sanitize outputs to single line when possible: 
  SUMMARY=$(echo "$SUMMARY" | tr '\n' ' ' | tr -s ' ')
- MUST: Always create directories before writing files:
  mkdir -p "$(dirname "$OUTPUT_FILE")"
- MUST: Handle missing artifacts gracefully in download steps:
  continue-on-error: true for artifact downloads
- MUST: Check file existence before operations:
  [ -f "$FILE" ] && process_file || echo "File not found, using fallback"
- MUST: For news video, calculate scenes as: ceil(duration_seconds / 5) (e.g., 60s = 12 scenes, 30s = 6 scenes)
- MUST: Create ONE news anchor with fixed seed value, generate multiple lip-sync videos for all scenes
- CRITICAL MUST: Environment variables for Claude Code SDK (REQUIRED for EVERY step using Claude Code):
  # IMPORTANT: env: must be at STEP level, not workflow or job level
  # Each step that calls "npx @anthropic-ai/claude-code" MUST have:
  - name: Your Step Name
    env:  # ‚Üê STEP-LEVEL env is MANDATORY
      CLAUDE_CODE_CI_MODE: true
      CLAUDE_CODE_AUTO_APPROVE_MCP: true
      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
    run: |
      npx @anthropic-ai/claude-code ...
  
  CRITICAL: Use exactly "${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}" - NEVER use actual token values
  CRITICAL: Without step-level env, MCP tools will NOT work (permission denied)
  Note: OAuth tokens must ALWAYS be referenced through GitHub Secrets
- MUST: Follow these patterns when executing Claude Code:
  1. Execute generation/processing with MCP tools
  2. Save local files with Write tool (explicit path: ${PROJECT_DIR}/media/...)
  3. Verify save with ls -la using Bash tool
  4. ALWAYS add error handling with fallback mechanisms
  * Details: See docs/CLAUDE_CODE_DATA_PERSISTENCE_GUIDE.md
- CRITICAL MUST: Claude Code SDK Error Handling Pattern (MANDATORY for ALL SDK calls):
  # Every single npx @anthropic-ai/claude-code call MUST have error handling
  # NO EXCEPTIONS - This prevents workflow hanging on SDK failures
  npx @anthropic-ai/claude-code \
    --mcp-config ".claude/mcp-kamuicode.json" \
    --allowedTools "mcp__*,Write,Bash" \
    --max-turns 40 \
    -p "$PROMPT" || {
      echo "‚ö†Ô∏è Claude Code SDK failed, using fallback"
      # MUST implement specific fallback for each case:
      # For T2S: use espeak-ng or festival
      # For T2I: use placeholder image or skip
      # For I2V: use ffmpeg to create video from image
      # For WebSearch: create error summary file
      # For data: use default values
      # ALWAYS create expected output files to prevent downstream failures
    }
- MUST: MCP Result Saving Pattern (prevent placeholder files):
  # After MCP tool execution, ALWAYS:
  1. Check if URL was returned: [ -f "${OUTPUT_PATH}-url.txt" ]
  2. Download from URL immediately: curl -L -o "$OUTPUT_PATH" "$(cat ${OUTPUT_PATH}-url.txt)"
  3. Verify file size: [ $(stat -c%s "$OUTPUT_PATH") -gt 1000 ]
  4. If invalid, retry with different parameters or use fallback
- MUST: For video-production, combine T2I and I2V in single job per scene:
  Example structure for each scene job:
  1. Generate image with T2I MCP tool
  2. Save image locally AND save URL
  3. IMMEDIATELY convert to video with I2V MCP tool (within 3 minutes)
  4. Save video locally with proper verification:
     # Save URL from MCP response
     echo "$VIDEO_URL" > "${PROJECT_DIR}/videos/scene${SCENE_NUM}-url.txt"
     # Download actual video file
     curl -L -o "${PROJECT_DIR}/videos/scene${SCENE_NUM}.mp4" "$VIDEO_URL"
     # Verify it's a real video (not placeholder text)
     if [ $(stat -c%s "${PROJECT_DIR}/videos/scene${SCENE_NUM}.mp4") -lt 1000 ]; then
       echo "‚ö†Ô∏è Video file too small, likely placeholder"
       # Create fallback video from image
       ffmpeg -loop 1 -i "${PROJECT_DIR}/images/scene${SCENE_NUM}.png" \
              -c:v libx264 -t 5 -pix_fmt yuv420p \
              "${PROJECT_DIR}/videos/scene${SCENE_NUM}_fallback.mp4"
     fi
  This prevents URL expiration between T2I and I2V AND ensures real files are saved
- MUST: Execute file search with multiple patterns:
  1. Specific pattern: "*scene${NUM}*.png"
  2. Time-based: "*.png -mmin -2"
  3. Generic pattern: "*.png"
- MUST: Download immediately when URL file detected
  * IMPORTANT: MCP tools return gs:// URLs from Google services
  * These gs:// URLs are PUBLIC and can be converted to HTTPS
  * Pattern: gs://bucket/path ‚Üí https://storage.googleapis.com/bucket/path
  * Always create download_url() helper function for proper handling
- SHOULD: Split steps within 21000 characters

Domain Knowledge Application (Mandatory):
- If artifacts/domain-template-data/domain-checklists.txt exists, read each listed checklist and comply with "MUST" requirements. If compliance is not possible, perform alternative design during generation, and if still impossible, output with the premise of FAILED at validation stage.
- If meta/domain-templates/<domain>/constraints.yaml exists:
  * MUST: Apply constraints.composition_rules / timing_constraints / orchestration / path constraints (only when relevant tasks exist)
  * SHOULD: Read each file listed in constraints.rule_references / checklist_references (rules/*.yaml, checklists/*.md) in order, prioritizing MUST for design reflection
- When multiple domains are involved:
  * MUST: Apply integrated constraints / rules / checklists from each domain
  * MUST: Adopt safer side (stricter MUST) in conflicts, explicitly note compromises

Design Principles (Examples):
- When explicit serial requirements like image‚Üívideo are shown in constraints/rules, execute "serial chain" within the same job for each target item/scene, and parallelize overall with matrix (max-parallel follows constraints).
- Similar tasks (e.g., generating multiple slides) are optimized for parallel execution. However, serialize when data dependencies exist.
- When domain is not specified or no constraints exist, use general serial/parallel configuration based on task decomposition/optimization order.

Important: For video-production domain:
- Always use workflow_generation_parameters.matrix_scene_list to set matrix.scene
- IMPORTANT: max-parallel in GitHub Actions only accepts numeric literals (not variables)
  - Option 1: Remove max-parallel entirely (defaults to unlimited parallel jobs)
  - Option 2: Set a high fixed value like max-parallel: 256 as upper limit
  - Recommendation: Remove max-parallel to allow full parallelization based on scene count
- Use calculated dynamic values, not fixed values (e.g., batch: [1,2,3])
- Use consistent character for news anchor (fixed seed)
- CRITICAL: Image generation (T2I) ‚Üí video conversion (I2V) MUST be in SAME JOB (serial execution)
  - This avoids Google Cloud Storage URL expiration (15 minutes)
  - Each scene job should: Generate image ‚Üí Convert to video immediately
  - NEVER split T2I and I2V into separate jobs
- CRITICAL: Narration generation MUST include fallback:
  # Primary: Try MCP T2S tool
  npx @anthropic-ai/claude-code \
    --mcp-config ".claude/mcp-kamuicode.json" \
    --allowedTools "mcp__t2s-*,Write,Bash" \
    --max-turns 40 \
    -p "Generate narration audio: $NARRATION_TEXT
         Save to: ${PROJECT_DIR}/audio/narration.wav" || {
    echo "‚ö†Ô∏è MCP T2S failed, using local TTS"
    # Fallback: Use espeak-ng
    sudo apt-get update && sudo apt-get install -y espeak-ng
    espeak-ng "$NARRATION_TEXT" -w "${PROJECT_DIR}/audio/narration.wav" -s 150
  }

Standard Execution Structure (Examples):
  Phase structure in workflow:
  1. phase1: User issue analysis
  2. phase2: Content research/information gathering
  3. phase3: Scripting/planning
  4. phase4: Material generation (T2I/T2V/T2S/etc. - based on decomposition)
  5. phase5: Advanced processing (if domain requires, e.g., lipsync)
  6. phase6: Final editing/synthesis

Output destination (MUST):
- Use the Write tool to save the generated workflow to: PROJECT_DIR_PLACEHOLDER/generated-workflow/workflow.yml
- The workflow must be saved as a complete file, not output to stdout

üîÑ ERROR RECOVERY REQUIREMENTS (MANDATORY - VERIFIED IN PRODUCTION):
For ANY parallel generation tasks (scenes, images, videos, data), you MUST include:
1. Main generation job with:
   - continue-on-error: true for each scene
   - FFmpeg installation for video fallback: sudo apt-get install -y ffmpeg
   - Output failed_items as JSON array
2. Recovery job that:
   - Runs if: always() && needs.main-job.outputs.failed_items != '' && needs.main-job.outputs.failed_items != '[]'
   - Uses matrix: ${{ fromJson(needs.main-job.outputs.failed_items || '[]') }}
   - Includes FFmpeg installation for fallback processing
   - Sets CLAUDE_CODE_MAX_OUTPUT_TOKENS: 16000 to prevent token limit errors
   - Retries failed generations with different parameters

Example pattern for video generation:
```yaml
scene-video-generation:
  strategy:
    matrix:
      scene: [1,2,3,4,5,6,7,8,9,10,11,12]
    fail-fast: false
  continue-on-error: true
  outputs:
    failed_videos: ${{ steps.collect-video-failures.outputs.failed_videos }}
  steps:
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    - name: Generate Video
      env:
        CLAUDE_CODE_MAX_OUTPUT_TOKENS: 16000
      run: |
        # I2V generation with FFmpeg fallback
        npx @anthropic-ai/claude-code ... || {
          echo "‚ö†Ô∏è I2V failed, using FFmpeg fallback"
          ffmpeg -loop 1 -i "$IMAGE" -c:v libx264 -t 5 -pix_fmt yuv420p "$VIDEO" -y
        }
    
video-recovery:
  needs: [scene-video-generation]
  if: |
    always() && 
    needs.scene-video-generation.outputs.failed_videos != '' &&
    needs.scene-video-generation.outputs.failed_videos != '[]'
  strategy:
    matrix:
      scene: ${{ fromJson(needs.scene-video-generation.outputs.failed_videos || '[]') }}
  steps:
    - name: Install FFmpeg
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    - name: Retry Video Generation
      env:
        CLAUDE_CODE_MAX_OUTPUT_TOKENS: 16000
```

CRITICAL: For video-production workflows, MUST include ALL:
- background-recovery (for image generation failures)
- collect-video-failures (to aggregate matrix job results)
- video-recovery (for I2V conversion failures)

IMPORTANT: Matrix jobs cannot directly output aggregated results. You MUST:
1. Save status to artifact in each matrix job
2. Create separate collect-failures job to aggregate results
3. Recovery job depends on collect-failures, not the matrix job

Status File Pattern (MANDATORY for matrix jobs):
```yaml
- name: Save Generation Status
  if: always()
  run: |
    SCENE_NUM=${{ matrix.scene }}
    # CRITICAL: Create directory first
    mkdir -p "${{ needs.setup.outputs.project_dir }}"
    STATUS_FILE="${{ needs.setup.outputs.project_dir }}/status-scene${SCENE_NUM}.txt"
    
    # Check success/failure and write status
    if [ -f "expected_output_file" ]; then
      echo "SUCCESS" > "$STATUS_FILE"
    else
      echo "FAILED" > "$STATUS_FILE"
    fi

- name: Upload Status with Artifacts
  uses: actions/upload-artifact@v4
  if: always()
  with:
    name: data-scene-${{ matrix.scene }}
    path: |
      ${{ needs.setup.outputs.project_dir }}/media/
      ${{ needs.setup.outputs.project_dir }}/status-scene${{ matrix.scene }}.txt
```

Collect Failures Pattern (MANDATORY after matrix jobs):
```yaml
collect-failures:
  needs: [matrix-generation-job]
  if: always()
  outputs:
    failed_items: ${{ steps.collect.outputs.failed_items }}
  steps:
    - name: Download All Status Files
      uses: actions/download-artifact@v4
      with:
        pattern: data-scene-*
        path: all-data/
    
    - name: Collect Failed Items
      id: collect
      run: |
        FAILED_ITEMS=""
        for i in {1..N}; do
          STATUS_FILE="all-data/data-scene-${i}/status-scene${i}.txt"
          if [ -f "$STATUS_FILE" ]; then
            STATUS=$(cat "$STATUS_FILE")
            if [ "$STATUS" = "FAILED" ]; then
              [ -z "$FAILED_ITEMS" ] && FAILED_ITEMS="$i" || FAILED_ITEMS="$FAILED_ITEMS,$i"
            fi
          else
            # No status file means job failed
            [ -z "$FAILED_ITEMS" ] && FAILED_ITEMS="$i" || FAILED_ITEMS="$FAILED_ITEMS,$i"
          fi
        done
        echo "failed_items=[${FAILED_ITEMS}]" >> $GITHUB_OUTPUT
```

CRITICAL REQUIREMENTS:
- Generate a valid GitHub Actions YAML workflow file
- Use the Write tool to save the workflow to the specified path
- Do NOT output the workflow to stdout, only save it to the file
- The workflow file must be complete and self-contained
- Use unquoted on: field (GitHub Actions requires it unquoted)
- Keep workflow names simple and without special characters
- ALWAYS include recovery jobs for parallel generation tasks
- After saving, use Bash tool to run: ls -la PROJECT_DIR_PLACEHOLDER/generated-workflow/