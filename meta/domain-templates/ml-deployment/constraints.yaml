# Rule references (domain rule files to load additionally)
rule_references:
  - path: "meta/domain-templates/ml-deployment/rules/task-breakdown.yaml"
    purpose: "Task decomposition into atomic steps"

# Checklist references (domain-specific MUST/SHOULD)
checklist_references:
  - path: "meta/domain-templates/ml-deployment/checklists/general.md"
    purpose: "MUST/SHOULD checklist for ML deployment"
# 機械学習デプロイメントの制約条件と計算式

model_serving_constraints:
  # 推論パフォーマンス
  inference_performance:
    latency_requirements:
      real_time:
        p50: "< 10ms"
        p95: "< 50ms"
        p99: "< 100ms"
        max: "< 200ms"
      
      batch:
        throughput: "> 1000 items/sec"
        max_batch_size: 1000
        timeout: "5 minutes"
      
      edge:
        cpu_only: "< 50ms"
        memory: "< 100MB"
        model_size: "< 50MB"
    
    optimization_techniques:
      quantization:
        int8: "~4x speedup, ~1% accuracy loss"
        fp16: "~2x speedup, ~0.1% accuracy loss"
        dynamic: "variable precision"
      
      pruning:
        structured: "channel/filter removal"
        unstructured: "individual weight removal"
        magnitude_based: "remove small weights"
      
      knowledge_distillation:
        teacher_student: "large model → small model"
        compression_ratio: "10-100x"
        accuracy_retention: "> 95%"
    
    hardware_acceleration:
      gpu:
        frameworks: ["TensorRT", "ONNX Runtime", "TensorFlow Lite GPU"]
        speedup: "10-100x vs CPU"
        
      tpu:
        frameworks: ["TensorFlow", "JAX", "PyTorch/XLA"]
        speedup: "100-1000x for large models"
        
      specialized:
        neural_engine: "Apple devices"
        npu: "Qualcomm, HiSilicon"
        inference_chips: "Google Edge TPU, Intel Movidius"

model_management:
  # モデルバージョニング
  versioning_strategy:
    semantic_versioning:
      major: "architecture changes"
      minor: "retrained with new data"
      patch: "hyperparameter tuning"
      
      example: "v2.3.1"
    
    metadata_tracking:
      required_fields:
        - "model_id"
        - "version"
        - "training_dataset_version"
        - "hyperparameters"
        - "metrics"
        - "created_at"
        - "created_by"
      
      optional_fields:
        - "feature_importance"
        - "confusion_matrix"
        - "roc_curves"
        - "sample_predictions"
    
    model_registry:
      storage_backend: ["S3", "GCS", "Azure Blob", "MinIO"]
      metadata_store: ["MLflow", "DVC", "Weights & Biases", "Neptune"]
      
      lifecycle_stages:
        - "experimentation"
        - "staging"
        - "production"
        - "archived"
  
  # モデル更新戦略
  deployment_strategies:
    blue_green:
      process: |
        1. 新モデルを別エンドポイントにデプロイ
        2. smoke testとvalidation実行
        3. トラフィックを即座に切り替え
        4. 問題があれば即座にロールバック
      
      rollback_time: "< 1分"
    
    canary:
      stages:
        - { traffic: "1%", duration: "1時間", metrics: ["latency", "error_rate"] }
        - { traffic: "5%", duration: "6時間", metrics: ["accuracy", "business_metrics"] }
        - { traffic: "20%", duration: "24時間", metrics: ["all"] }
        - { traffic: "50%", duration: "48時間", metrics: ["all"] }
        - { traffic: "100%", duration: "∞" }
      
      rollback_triggers:
        accuracy_drop: "> 2%"
        latency_increase: "> 20%"
        error_rate: "> 1%"
    
    shadow:
      description: "本番トラフィックで新モデルをテスト（結果は使用しない）"
      
      validation_period: "1-7 days"
      
      comparison_metrics:
        - "prediction_distribution"
        - "latency_profile"
        - "resource_usage"
        - "error_patterns"

feature_engineering:
  # 特徴量管理
  feature_store:
    architecture:
      offline_store:
        purpose: "バッチ特徴量計算・学習用"
        storage: ["Parquet", "Delta Lake", "Iceberg"]
        compute: ["Spark", "Beam", "Dask"]
        
      online_store:
        purpose: "低遅延推論用"
        storage: ["Redis", "DynamoDB", "Cassandra"]
        latency: "< 10ms"
        
      feature_registry:
        metadata: ["name", "type", "description", "owner", "SLA"]
        versioning: "必須"
        lineage: "データソースから特徴量まで"
    
    data_consistency:
      training_serving_skew:
        prevention: |
          // 同じ特徴量変換コードを使用
          def transform_features(raw_data):
            # この関数は訓練と推論で共有
            features = {}
            features['age_bucket'] = bucketize(raw_data['age'], [18, 25, 35, 50, 65])
            features['income_log'] = np.log1p(raw_data['income'])
            return features
        
        validation: "定期的な特徴量分布比較"
      
      point_in_time_correctness:
        description: "未来のデータリークを防ぐ"
        implementation: "タイムスタンプベースのジョイン"
  
  # 特徴量計算最適化
  computation_optimization:
    incremental_computation:
      description: "差分のみ計算"
      
      example: |
        // 日次集計の増分更新
        today_stats = compute_daily_stats(today_data)
        rolling_30d = update_rolling_window(previous_30d, today_stats, dropped_day)
    
    materialization_strategy:
      hot_features: "常に事前計算"
      warm_features: "キャッシュ + オンデマンド"
      cold_features: "オンデマンド計算"
    
    compute_cost_optimization:
      spot_instances: "バッチ処理用"
      serverless: "バースト的な計算"
      reserved_capacity: "定常的な負荷"

monitoring_observability:
  # モデルモニタリング
  model_metrics:
    performance_metrics:
      classification:
        - "accuracy"
        - "precision"
        - "recall"
        - "f1_score"
        - "auc_roc"
        - "confusion_matrix"
      
      regression:
        - "mae"
        - "rmse"
        - "r2_score"
        - "quantile_loss"
      
      business_metrics:
        - "revenue_impact"
        - "conversion_rate"
        - "user_satisfaction"
    
    drift_detection:
      data_drift:
        methods:
          - "KS test"
          - "Chi-squared test"
          - "Jensen-Shannon divergence"
          - "Population Stability Index (PSI)"
        
        thresholds:
          psi: "> 0.2 (significant drift)"
          ks_statistic: "> 0.1"
      
      concept_drift:
        detection: "performance degradation over time"
        
        sliding_window: |
          window_size = 1000
          if accuracy_window < accuracy_baseline - threshold:
            trigger_retraining()
      
      prediction_drift:
        monitor: ["prediction distribution", "confidence scores"]
        alert: "significant shift in output distribution"
  
  # アラート設定
  alerting_framework:
    severity_levels:
      critical:
        - "model serving down"
        - "accuracy drop > 5%"
        - "latency > 500ms"
        
      warning:
        - "accuracy drop > 2%"
        - "latency > 200ms"
        - "drift detected"
        
      info:
        - "new model deployed"
        - "retraining triggered"
        - "feature update"
    
    alert_fatigue_prevention:
      deduplication: "同一アラートは1時間に1回"
      aggregation: "関連アラートをグループ化"
      smart_routing: "適切な担当者へ"

infrastructure_scaling:
  # オートスケーリング
  scaling_strategies:
    horizontal_scaling:
      metrics:
        - "CPU utilization > 70%"
        - "Memory usage > 80%"
        - "Request queue depth > 100"
        - "Average latency > target"
      
      scale_out_policy:
        min_instances: 2
        max_instances: 100
        scale_up_rate: "100% / 5 minutes"
        scale_down_rate: "10% / 10 minutes"
    
    vertical_scaling:
      gpu_selection:
        inference_optimized: ["T4", "A10G", "V100"]
        
        sizing_guide:
          small_models: "T4 (16GB)"
          medium_models: "V100 (32GB)"
          large_models: "A100 (40-80GB)"
    
    serverless_options:
      aws_lambda:
        max_payload: "6MB"
        max_memory: "10GB"
        timeout: "15 minutes"
        cold_start: "1-5 seconds"
      
      sagemaker_serverless:
        max_concurrency: 200
        memory: "1-6GB"
        
      vertex_ai_prediction:
        auto_scaling: true
        min_replicas: 0
  
  # コスト最適化
  cost_optimization:
    instance_selection:
      spot_instances:
        use_cases: ["batch inference", "training"]
        savings: "70-90%"
        interruption_handling: "checkpointing required"
      
      reserved_instances:
        use_cases: ["baseline load"]
        savings: "30-70%"
        commitment: "1-3 years"
    
    model_optimization_roi:
      calculation: |
        optimization_cost = engineering_hours * hourly_rate + compute_cost
        monthly_savings = (original_cost - optimized_cost) * requests_per_month
        roi_months = optimization_cost / monthly_savings

security_compliance:
  # セキュリティ要件
  data_privacy:
    pii_handling:
      detection: "automated PII scanning"
      masking: "tokenization or encryption"
      audit_trail: "all access logged"
    
    model_privacy:
      differential_privacy:
        epsilon: "privacy budget"
        noise_addition: "calibrated to sensitivity"
      
      federated_learning:
        description: "train on distributed data"
        aggregation: "secure multi-party computation"
    
    inference_privacy:
      homomorphic_encryption: "compute on encrypted data"
      secure_enclaves: "TEE-based inference"
      
  # コンプライアンス
  regulatory_compliance:
    gdpr:
      right_to_explanation: "model interpretability required"
      right_to_be_forgotten: "remove training data influence"
      
    fairness_metrics:
      demographic_parity: "equal positive rates"
      equalized_odds: "equal TPR and FPR"
      
      bias_detection: |
        for sensitive_attribute in ['gender', 'race', 'age']:
          compute_fairness_metrics(predictions, sensitive_attribute)
    
    audit_requirements:
      model_lineage: "full training pipeline"
      data_lineage: "source to prediction"
      decision_logs: "all predictions stored"

edge_deployment:
  # エッジデプロイメント
  optimization_pipeline:
    model_conversion:
      frameworks:
        tensorflow_lite:
          quantization: ["dynamic", "full_integer", "float16"]
          delegates: ["GPU", "NNAPI", "CoreML"]
          
        onnx:
          optimization_level: ["basic", "extended", "layout"]
          execution_providers: ["CPU", "CUDA", "TensorRT", "OpenVINO"]
          
        coreml:
          precision: ["float32", "float16", "int8"]
          compute_units: ["CPU", "GPU", "NeuralEngine"]
    
    resource_constraints:
      mobile:
        cpu: "1-4 cores"
        memory: "50-200MB"
        storage: "10-100MB"
        battery: "minimize usage"
      
      iot:
        cpu: "single core"
        memory: "< 50MB"
        storage: "< 10MB"
        power: "< 1W"
  
  # 更新戦略
  model_updates:
    over_the_air:
      delta_updates: "only changed weights"
      compression: "70-90% size reduction"
      
    federated_updates:
      on_device_training: "personalization"
      aggregation: "periodic sync"
      
    cache_strategy:
      model_cache: "LRU with size limit"
      prediction_cache: "common inputs"