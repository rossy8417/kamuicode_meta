name: "Meta Workflow Executor v10 with Optimized Design"
run-name: "ðŸš€ Meta Workflow v10 | Issue #${{ inputs.issue_number }} | ${{ github.actor }}"

on:
  workflow_dispatch:
    inputs:
      issue_number:
        description: 'Issue number for workflow generation request'
        required: true
        default: '60'

permissions:
  contents: write
  issues: write
  actions: write
  pull-requests: write

env:
  CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
  CLAUDE_CODE_CI_MODE: true
  CLAUDE_CODE_AUTO_APPROVE_MCP: true

jobs:
  # ===========================================
  # PHASE 1: ISSUE VALIDATION
  # ===========================================
  
  validate-trigger:
    name: "ðŸ” Issue Validation"
    runs-on: ubuntu-latest
    outputs:
      issue_number: ${{ steps.extract.outputs.issue_number }}
      issue_title: ${{ steps.extract.outputs.issue_title }}
      request_type: ${{ steps.analyze.outputs.request_type }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Extract Issue Information
        id: extract
        run: |
          echo "ðŸ” Analyzing Issue #${{ inputs.issue_number }}..."
          
          # Get issue details using GitHub CLI
          ISSUE_DATA=$(gh issue view ${{ inputs.issue_number }} --json title,body,number --jq '{title: .title, body: .body, number: .number}')
          
          ISSUE_TITLE=$(echo "$ISSUE_DATA" | jq -r '.title')
          ISSUE_BODY=$(echo "$ISSUE_DATA" | jq -r '.body')
          ISSUE_NUMBER=$(echo "$ISSUE_DATA" | jq -r '.number')
          
          # Save to artifacts for next jobs
          mkdir -p artifacts
          echo "$ISSUE_TITLE" > artifacts/issue_title.txt
          echo "$ISSUE_BODY" > artifacts/issue_body.txt
          echo "$ISSUE_NUMBER" > artifacts/issue_number.txt
          
          # Output minimal data
          echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
          echo "issue_title=$ISSUE_TITLE" >> $GITHUB_OUTPUT
          
          echo "âœ… Issue #$ISSUE_NUMBER validated: $ISSUE_TITLE"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Analyze Request Type
        id: analyze
        run: |
          ISSUE_BODY=$(cat artifacts/issue_body.txt)
          
          REQUEST_TYPE="unknown"
          if echo "$ISSUE_BODY" | grep -qi "video\|å‹•ç”»"; then
            REQUEST_TYPE="video-generation"
          elif echo "$ISSUE_BODY" | grep -qi "image\|ç”»åƒ"; then
            REQUEST_TYPE="image-generation"
          elif echo "$ISSUE_BODY" | grep -qi "audio\|éŸ³å£°\|music"; then
            REQUEST_TYPE="audio-generation"
          fi
          
          echo "request_type=$REQUEST_TYPE" >> $GITHUB_OUTPUT
          
      - name: Upload Issue Data
        uses: actions/upload-artifact@v4
        with:
          name: issue-data
          path: artifacts/

  # ===========================================
  # PHASE 2: BASIC TASK DECOMPOSITION
  # ===========================================
  
  basic-task-decomposition:
    name: "ðŸ“‹ Basic Task Decomposition"
    runs-on: ubuntu-latest
    needs: ['validate-trigger']
    outputs:
      capabilities_count: ${{ steps.detect.outputs.capabilities_count }}
      has_video: ${{ steps.detect.outputs.has_video }}
      has_narration: ${{ steps.detect.outputs.has_narration }}
      complexity: ${{ steps.detect.outputs.complexity }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Issue Data
        uses: actions/download-artifact@v4
        with:
          name: issue-data
          path: artifacts/
          
      - name: Detect Capabilities
        id: detect
        run: |
          echo "ðŸ” Detecting required capabilities..."
          
          ISSUE_BODY=$(cat artifacts/issue_body.txt)
          
          # Temporary arrays to store detected capabilities
          RESEARCH_CAPS=""
          PLANNING_CAPS=""
          MEDIA_CAPS=""
          POST_CAPS=""
          
          # Detection phase (categorized)
          HAS_VIDEO=false
          HAS_NARRATION=false
          HAS_EXTERNAL_API=false
          EXTERNAL_CAPS=""
          
          # Video capabilities
          if echo "$ISSUE_BODY" | grep -qi "å‹•ç”»\|video\|ãƒ“ãƒ‡ã‚ª"; then
            HAS_VIDEO=true
            echo "has_video=true" >> $GITHUB_OUTPUT
            
            # Context-based additions
            if echo "$ISSUE_BODY" | grep -qi "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³\|narration\|éŸ³å£°ä»˜"; then
              HAS_NARRATION=true
              echo "has_narration=true" >> $GITHUB_OUTPUT
            fi
          fi
          
          # Research phase
          if echo "$ISSUE_BODY" | grep -qi "æ¤œç´¢\|èª¿æŸ»\|ãƒ‹ãƒ¥ãƒ¼ã‚¹\|æœ€æ–°"; then
            RESEARCH_CAPS="${RESEARCH_CAPS}web-search,"
          fi
          
          # External API detection - ALL 27 APIs
          # News & Weather
          if echo "$ISSUE_BODY" | grep -qi "ãƒ‹ãƒ¥ãƒ¼ã‚¹\|news\|è¨˜äº‹"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}newsapi,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "å¤©æ°—\|weather\|æ°—è±¡"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}weather,"
            HAS_EXTERNAL_API=true
          fi
          
          # YouTube
          if echo "$ISSUE_BODY" | grep -qi "youtube\|ãƒ¦ãƒ¼ãƒãƒ¥ãƒ¼ãƒ–"; then
            if echo "$ISSUE_BODY" | grep -qi "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\|upload\|æŠ•ç¨¿"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}youtube-upload,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "æƒ…å ±\|info\|å–å¾—"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}youtube-video-info,"
            fi
            HAS_EXTERNAL_API=true
          fi
          
          # AI Services
          if echo "$ISSUE_BODY" | grep -qi "è¦ç´„\|summarize\|ã¾ã¨ã‚"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-summarize,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "ç¿»è¨³\|translate\|translation"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-translate,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "gpt\|ãƒãƒ£ãƒƒãƒˆ\|ä¼šè©±"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-gpt,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "dall-e\|ç”»åƒç”Ÿæˆ\|image.*generation"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-image-gen,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "elevenlabs\|éŸ³å£°åˆæˆ"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}elevenlabs-tts,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "huggingface\|ãƒã‚®ãƒ³ã‚°ãƒ•ã‚§ã‚¤ã‚¹"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}huggingface-inference,"
            HAS_EXTERNAL_API=true
          fi
          
          # Communication
          if echo "$ISSUE_BODY" | grep -qi "slack\|ã‚¹ãƒ©ãƒƒã‚¯"; then
            if echo "$ISSUE_BODY" | grep -qi "é€šçŸ¥\|notify\|ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}slack-notify,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "ãƒ•ã‚¡ã‚¤ãƒ«\|file\|ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}slack-file-upload,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "discord\|ãƒ‡ã‚£ã‚¹ã‚³ãƒ¼ãƒ‰"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}discord-webhook,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "telegram\|ãƒ†ãƒ¬ã‚°ãƒ©ãƒ "; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}telegram-send-message,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "ãƒ¡ãƒ¼ãƒ«\|email\|mail"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}sendgrid-send-email,"
            HAS_EXTERNAL_API=true
          fi
          
          # Social Media
          if echo "$ISSUE_BODY" | grep -qi "twitter\|ãƒ„ã‚¤ãƒƒã‚¿ãƒ¼\|x\.com"; then
            if echo "$ISSUE_BODY" | grep -qi "æŠ•ç¨¿\|post\|ãƒ„ã‚¤ãƒ¼ãƒˆ"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}twitter-post,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "æ¤œç´¢\|search"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}twitter-search,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "reddit\|ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆ"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}reddit-search,"
            HAS_EXTERNAL_API=true
          fi
          
          # Data & Analytics
          if echo "$ISSUE_BODY" | grep -qi "google.*sheets\|ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆ"; then
            if echo "$ISSUE_BODY" | grep -qi "èª­ã¿\|read\|å–å¾—"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}google-sheets-read,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "æ›¸ã\|write\|ä¿å­˜"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}google-sheets-write,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "æ ª\|stock\|æ ªä¾¡"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}finnhub-stock-quote,"
            HAS_EXTERNAL_API=true
          fi
          
          # Development
          if echo "$ISSUE_BODY" | grep -qi "github\|ã‚®ãƒƒãƒˆãƒãƒ–"; then
            if echo "$ISSUE_BODY" | grep -qi "issue\|ã‚¤ã‚·ãƒ¥ãƒ¼"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-issue-create,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "ãƒªãƒã‚¸ãƒˆãƒª\|repository\|æ¤œç´¢"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-repo-search,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "workflow\|ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼\|å®Ÿè¡Œ"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-workflow-dispatch,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "release\|ãƒªãƒªãƒ¼ã‚¹"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-release-create,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "arxiv\|è«–æ–‡"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}arxiv-search,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "notion\|ãƒŽãƒ¼ã‚·ãƒ§ãƒ³"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}notion-create-page,"
            HAS_EXTERNAL_API=true
          fi
          
          # Planning phase
          if echo "$ISSUE_BODY" | grep -qi "ä¼ç”»\|è¨ˆç”»\|æ§‹æˆ"; then
            PLANNING_CAPS="${PLANNING_CAPS}planning,"
          fi
          
          # For video content, add detailed planning capabilities
          if [ "$HAS_VIDEO" = true ]; then
            PLANNING_CAPS="${PLANNING_CAPS}content-planning,"
            if [ "$HAS_NARRATION" = true ]; then
              PLANNING_CAPS="${PLANNING_CAPS}narration-creation,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "ã‚·ãƒ¼ãƒ³\|è¤‡æ•°\|ã‚¹ãƒˆãƒ¼ãƒªãƒ¼"; then
              PLANNING_CAPS="${PLANNING_CAPS}scene-composition,"
            fi
          fi
          
          # Media generation phase
          if [ "$HAS_VIDEO" = true ]; then
            MEDIA_CAPS="${MEDIA_CAPS}image-generation,"
            if [ "$HAS_NARRATION" = true ]; then
              MEDIA_CAPS="${MEDIA_CAPS}text-to-speech,"
            fi
            MEDIA_CAPS="${MEDIA_CAPS}video-generation,"
          fi
          
          # Post-processing phase
          if [ "$HAS_VIDEO" = true ] && [ "$HAS_NARRATION" = true ]; then
            POST_CAPS="${POST_CAPS}lipsync,subtitle-overlay,"
          fi
          
          # Combine in logical order
          CAPABILITIES="${RESEARCH_CAPS}${PLANNING_CAPS}${MEDIA_CAPS}${POST_CAPS}${EXTERNAL_CAPS}"
          
          # Save capabilities
          mkdir -p metadata
          echo "$CAPABILITIES" > metadata/capabilities.txt
          
          # Count and complexity
          CAP_COUNT=$(echo "$CAPABILITIES" | tr ',' '\n' | grep -v '^$' | wc -l)
          echo "capabilities_count=$CAP_COUNT" >> $GITHUB_OUTPUT
          
          if [ "$CAP_COUNT" -gt 6 ]; then
            echo "complexity=complex" >> $GITHUB_OUTPUT
          elif [ "$CAP_COUNT" -gt 3 ]; then
            echo "complexity=medium" >> $GITHUB_OUTPUT
          else
            echo "complexity=simple" >> $GITHUB_OUTPUT
          fi
          
          echo "âœ… Detected $CAP_COUNT capabilities"
          
      - name: Upload Capabilities
        uses: actions/upload-artifact@v4
        with:
          name: capabilities-data
          path: metadata/

  # ===========================================
  # PHASE 3: DETAILED ANALYSIS
  # ===========================================
  
  detailed-analysis:
    name: "ðŸ§  Detailed Analysis"
    runs-on: ubuntu-latest
    needs: ['validate-trigger', 'basic-task-decomposition']
    outputs:
      orchestrator_match: ${{ steps.orchestrator.outputs.match_found }}
      execution_pattern: ${{ steps.orchestrator.outputs.pattern }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          
      - name: Install Dependencies
        run: |
          pip install pyyaml
          
      - name: Download Previous Data
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Orchestrator Analysis
        id: orchestrator
        run: |
          echo "ðŸ” Running orchestrator analysis..."
          
          # Copy analyzer script
          if [ -f scripts/orchestrator_analyzer.py ]; then
            cp scripts/orchestrator_analyzer.py .
            
            # Run analysis
            export USER_REQUEST=$(cat artifacts/issue-data/issue_title.txt)
            export CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
            
            python orchestrator_analyzer.py || echo "âš ï¸ Orchestrator analysis failed"
            
            if [ -f projects/current-session/metadata/orchestrator_analysis.json ]; then
              echo "match_found=true" >> $GITHUB_OUTPUT
              # Extract pattern
              PATTERN=$(jq -r '.execution_pattern // "sequential"' projects/current-session/metadata/orchestrator_analysis.json)
              echo "pattern=$PATTERN" >> $GITHUB_OUTPUT
            else
              echo "match_found=false" >> $GITHUB_OUTPUT
              echo "pattern=sequential" >> $GITHUB_OUTPUT
            fi
          fi
          
      - name: Generate Mermaid Diagram
        run: |
          echo "ðŸ“Š Generating execution flow diagram..."
          
          # Create metadata directory
          mkdir -p metadata
          
          CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
          
          # Generate basic Mermaid diagram
          echo "graph TD" > metadata/mermaid.txt
          echo "    Start[é–‹å§‹]" >> metadata/mermaid.txt
          
          IFS=',' read -ra CAP_ARRAY <<< "$CAPABILITIES"
          PREV_NODE="Start"
          NODE_COUNT=1
          
          for cap in "${CAP_ARRAY[@]}"; do
            if [ -n "$cap" ]; then
              NODE_NAME="Node$NODE_COUNT"
              
              case "$cap" in
                "web-search") LABEL="ðŸ” Webæ¤œç´¢" ;;
                "planning") LABEL="ðŸ“‹ ä¼ç”»ãƒ»è¨ˆç”»" ;;
                "content-planning") LABEL="ðŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¼ç”»" ;;
                "narration-creation") LABEL="ðŸŽ¤ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ" ;;
                "scene-composition") LABEL="ðŸŽ¨ ã‚·ãƒ¼ãƒ³æ§‹æˆ" ;;
                "image-generation") LABEL="ðŸ–¼ï¸ ç”»åƒç”Ÿæˆ" ;;
                "text-to-speech") LABEL="ðŸ—£ï¸ éŸ³å£°åˆæˆ" ;;
                "video-generation") LABEL="ðŸŽ¬ å‹•ç”»ç”Ÿæˆ" ;;
                "lipsync") LABEL="ðŸ‘„ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯" ;;
                "subtitle-overlay") LABEL="ðŸ“ å­—å¹•è¿½åŠ " ;;
                "newsapi") LABEL="ðŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹API" ;;
                "weather") LABEL="ðŸŒ¤ï¸ å¤©æ°—API" ;;
                "youtube-upload") LABEL="ðŸ“¹ YouTubeã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰" ;;
                "openai-summarize") LABEL="ðŸ“„ AIè¦ç´„" ;;
                "openai-translate") LABEL="ðŸŒ AIç¿»è¨³" ;;
                "openai-gpt") LABEL="ðŸ¤– ChatGPT" ;;
                "openai-image-gen") LABEL="ðŸŽ¨ DALL-Eç”»åƒç”Ÿæˆ" ;;
                "elevenlabs-tts") LABEL="ðŸŽ™ï¸ ElevenLabséŸ³å£°" ;;
                "huggingface-inference") LABEL="ðŸ¤— HuggingFaceæŽ¨è«–" ;;
                "slack-notify") LABEL="ðŸ’¬ Slacké€šçŸ¥" ;;
                "slack-file-upload") LABEL="ðŸ“Ž Slackãƒ•ã‚¡ã‚¤ãƒ«" ;;
                "discord-webhook") LABEL="ðŸŽ® Discordé€šçŸ¥" ;;
                "telegram-send-message") LABEL="âœˆï¸ Telegramé€ä¿¡" ;;
                "sendgrid-send-email") LABEL="ðŸ“§ ãƒ¡ãƒ¼ãƒ«é€ä¿¡" ;;
                "twitter-post") LABEL="ðŸ¦ TwitteræŠ•ç¨¿" ;;
                "twitter-search") LABEL="ðŸ” Twitteræ¤œç´¢" ;;
                "reddit-search") LABEL="ðŸ” Redditæ¤œç´¢" ;;
                "google-sheets-read") LABEL="ðŸ“Š Sheetsèª­è¾¼" ;;
                "google-sheets-write") LABEL="ðŸ“Š Sheetsæ›¸è¾¼" ;;
                "finnhub-stock-quote") LABEL="ðŸ“ˆ æ ªä¾¡å–å¾—" ;;
                "github-issue-create") LABEL="ðŸ› Issueä½œæˆ" ;;
                "github-repo-search") LABEL="ðŸ” ãƒªãƒã‚¸ãƒˆãƒªæ¤œç´¢" ;;
                "github-workflow-dispatch") LABEL="âš™ï¸ Workflowå®Ÿè¡Œ" ;;
                "github-release-create") LABEL="ðŸ·ï¸ Releaseä½œæˆ" ;;
                "arxiv-search") LABEL="ðŸ“š è«–æ–‡æ¤œç´¢" ;;
                "notion-create-page") LABEL="ðŸ“„ Notionãƒšãƒ¼ã‚¸" ;;
                *) LABEL="$cap" ;;
              esac
              
              echo "    $NODE_NAME[$LABEL]" >> metadata/mermaid.txt
              echo "    $PREV_NODE --> $NODE_NAME" >> metadata/mermaid.txt
              
              PREV_NODE=$NODE_NAME
              NODE_COUNT=$((NODE_COUNT + 1))
            fi
          done
          
          echo "    $PREV_NODE --> End[å®Œäº†]" >> metadata/mermaid.txt
          
      - name: Upload Analysis Results
        uses: actions/upload-artifact@v4
        with:
          name: analysis-data
          path: |
            metadata/
            projects/current-session/metadata/

  # ===========================================
  # PHASE 4: WORKFLOW GENERATION
  # ===========================================
  
  workflow-generation:
    name: "ðŸš€ Workflow Generation"
    runs-on: ubuntu-latest
    needs: ['validate-trigger', 'basic-task-decomposition', 'detailed-analysis']
    outputs:
      workflow_id: ${{ steps.generate.outputs.workflow_id }}
      total_jobs: ${{ steps.generate.outputs.total_jobs }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Generate Workflow
        id: generate
        run: |
          echo "ðŸš€ Generating optimized workflow..."
          
          ISSUE_NUMBER=$(cat artifacts/issue-data/issue_number.txt)
          CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
          COMPLEXITY="${{ needs['basic-task-decomposition'].outputs.complexity }}"
          
          # Create workflow directory
          mkdir -p generated-workflows
          WORKFLOW_FILE="generated-workflows/dynamic-workflow-${ISSUE_NUMBER}.yml"
          
          # Generate workflow header
          echo 'name: "ðŸŽ¯ Dynamic Workflow - Issue #${{ github.event.inputs.issue_number }}"' > "$WORKFLOW_FILE"
          echo 'run-name: "ðŸ“Š Dynamic | ${{ github.actor }} | Issue #${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          echo 'on:' >> "$WORKFLOW_FILE"
          echo '  workflow_dispatch:' >> "$WORKFLOW_FILE"
          echo '    inputs:' >> "$WORKFLOW_FILE"
          echo '      issue_number:' >> "$WORKFLOW_FILE"
          echo '        description: "Source issue number"' >> "$WORKFLOW_FILE"
          echo '        required: true' >> "$WORKFLOW_FILE"
          echo "        default: \"$ISSUE_NUMBER\"" >> "$WORKFLOW_FILE"
          echo '      branch_name:' >> "$WORKFLOW_FILE"
          echo '        description: "Working branch name"' >> "$WORKFLOW_FILE"
          echo '        required: false' >> "$WORKFLOW_FILE"
          echo "        default: \"issue-${ISSUE_NUMBER}\"" >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          
          # Add permissions and environment
          echo 'permissions:' >> "$WORKFLOW_FILE"
          echo '  contents: write' >> "$WORKFLOW_FILE"
          echo '  actions: write' >> "$WORKFLOW_FILE"
          echo '  issues: write' >> "$WORKFLOW_FILE"
          echo '  pull-requests: write' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          echo 'env:' >> "$WORKFLOW_FILE"
          echo '  CLAUDE_CODE_CI_MODE: true' >> "$WORKFLOW_FILE"
          echo '  CLAUDE_CODE_AUTO_APPROVE_MCP: true' >> "$WORKFLOW_FILE"
          echo '  CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}' >> "$WORKFLOW_FILE"
          
          # Add environment variables for external APIs if needed
          if echo "$CAPABILITIES" | grep -q "newsapi"; then
            echo '  NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "weather"; then
            echo '  OPENWEATHERMAP_API_KEY: ${{ secrets.OPENWEATHERMAP_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "youtube"; then
            echo '  YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "openai"; then
            echo '  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "elevenlabs"; then
            echo '  ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "huggingface"; then
            echo '  HUGGINGFACE_API_KEY: ${{ secrets.HUGGINGFACE_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "slack"; then
            echo '  SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "discord"; then
            echo '  DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "telegram"; then
            echo '  TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "sendgrid"; then
            echo '  SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "twitter"; then
            echo '  TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "reddit"; then
            echo '  REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}' >> "$WORKFLOW_FILE"
            echo '  REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "google-sheets"; then
            echo '  GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "finnhub"; then
            echo '  FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "github-"; then
            echo '  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "notion"; then
            echo '  NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          
          echo '' >> "$WORKFLOW_FILE"
          
          # Add jobs based on capabilities
          echo 'jobs:' >> "$WORKFLOW_FILE"
          
          # Setup job (minimal to avoid expression limits)
          echo '  setup:' >> "$WORKFLOW_FILE"
          echo '    name: "ðŸš€ Setup"' >> "$WORKFLOW_FILE"
          echo '    runs-on: ubuntu-latest' >> "$WORKFLOW_FILE"
          echo '    outputs:' >> "$WORKFLOW_FILE"
          echo '      project_dir: ${{ steps.setup.outputs.project_dir }}' >> "$WORKFLOW_FILE"
          echo '      timestamp: ${{ steps.setup.outputs.timestamp }}' >> "$WORKFLOW_FILE"
          echo '    steps:' >> "$WORKFLOW_FILE"
          echo '      - name: Checkout Repository' >> "$WORKFLOW_FILE"
          echo '        uses: actions/checkout@v4' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          echo '      - name: Create Project Structure' >> "$WORKFLOW_FILE"
          echo '        id: setup' >> "$WORKFLOW_FILE"
          echo '        run: |' >> "$WORKFLOW_FILE"
          echo '          TIMESTAMP=$(date +%Y%m%d-%H%M%S)' >> "$WORKFLOW_FILE"
          echo '          PROJECT_DIR="projects/issue-${{ github.event.inputs.issue_number }}-$TIMESTAMP"' >> "$WORKFLOW_FILE"
          echo '          mkdir -p "$PROJECT_DIR"/{logs,metadata,temp,final,media}' >> "$WORKFLOW_FILE"
          echo '          echo "project_dir=$PROJECT_DIR" >> $GITHUB_OUTPUT' >> "$WORKFLOW_FILE"
          echo '          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT' >> "$WORKFLOW_FILE"
          echo '          echo "âœ… Project structure created: $PROJECT_DIR"' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          
          # Group capabilities and create phases
          IFS=',' read -ra CAP_ARRAY <<< "$CAPABILITIES"
          
          # Capability mapping to minimal units
          declare -A UNIT_MAP
          UNIT_MAP["web-search"]="./minimal-units/planning/web-search.yml"
          UNIT_MAP["data-analysis"]="./minimal-units/planning/data-analysis.yml"
          UNIT_MAP["planning"]="./minimal-units/planning/planning-ccsdk.yml"
          UNIT_MAP["content-planning"]="./minimal-units/planning/content-planning.yml"
          UNIT_MAP["scene-composition"]="./minimal-units/planning/scene-composition.yml"
          UNIT_MAP["image-generation"]="./minimal-units/media/image/t2i-imagen3.yml"
          UNIT_MAP["video-generation"]="./minimal-units/media/video/t2v-veo3.yml"
          UNIT_MAP["audio-generation"]="./minimal-units/media/audio/bgm-generate-mcp.yml"
          UNIT_MAP["lipsync"]="./minimal-units/postprod/lipsync-generation.yml"
          UNIT_MAP["subtitle-overlay"]="./minimal-units/postprod/subtitle-overlay.yml"
          UNIT_MAP["video-editing"]="./minimal-units/postprod/video-concat.yml"
          # External API units - ALL 27
          UNIT_MAP["newsapi"]="./minimal-units/external/newsapi-fetch.yml"
          UNIT_MAP["weather"]="./minimal-units/external/weather-fetch.yml"
          UNIT_MAP["youtube-upload"]="./minimal-units/external/youtube-upload.yml"
          UNIT_MAP["youtube-video-info"]="./minimal-units/external/youtube-video-info.yml"
          UNIT_MAP["openai-gpt"]="./minimal-units/external/openai-gpt.yml"
          UNIT_MAP["openai-summarize"]="./minimal-units/external/openai-summarize.yml"
          UNIT_MAP["openai-translate"]="./minimal-units/external/openai-translate.yml"
          UNIT_MAP["openai-image-gen"]="./minimal-units/external/openai-image-gen.yml"
          UNIT_MAP["elevenlabs-tts"]="./minimal-units/external/elevenlabs-tts.yml"
          UNIT_MAP["huggingface-inference"]="./minimal-units/external/huggingface-inference.yml"
          UNIT_MAP["slack-notify"]="./minimal-units/external/slack-notify.yml"
          UNIT_MAP["slack-file-upload"]="./minimal-units/external/slack-file-upload.yml"
          UNIT_MAP["discord-webhook"]="./minimal-units/external/discord-webhook.yml"
          UNIT_MAP["telegram-send-message"]="./minimal-units/external/telegram-send-message.yml"
          UNIT_MAP["sendgrid-send-email"]="./minimal-units/external/sendgrid-send-email.yml"
          UNIT_MAP["twitter-post"]="./minimal-units/external/twitter-post.yml"
          UNIT_MAP["twitter-search"]="./minimal-units/external/twitter-search.yml"
          UNIT_MAP["reddit-search"]="./minimal-units/external/reddit-search.yml"
          UNIT_MAP["google-sheets-read"]="./minimal-units/external/google-sheets-read.yml"
          UNIT_MAP["google-sheets-write"]="./minimal-units/external/google-sheets-write.yml"
          UNIT_MAP["finnhub-stock-quote"]="./minimal-units/external/finnhub-stock-quote.yml"
          UNIT_MAP["github-issue-create"]="./minimal-units/external/github-issue-create.yml"
          UNIT_MAP["github-repo-search"]="./minimal-units/external/github-repo-search.yml"
          UNIT_MAP["github-workflow-dispatch"]="./minimal-units/external/github-workflow-dispatch.yml"
          UNIT_MAP["github-release-create"]="./minimal-units/external/github-release-create.yml"
          UNIT_MAP["arxiv-search"]="./minimal-units/external/arxiv-search.yml"
          UNIT_MAP["notion-create-page"]="./minimal-units/external/notion-create-page.yml"
          
          # Phase separation
          RESEARCH_JOBS=""
          PLANNING_JOBS=""
          MEDIA_JOBS=""
          POST_JOBS=""
          EXTERNAL_JOBS=""
          
          for cap in "${CAP_ARRAY[@]}"; do
            if [ -n "$cap" ]; then
              case "$cap" in
                web-search|data-analysis)
                  RESEARCH_JOBS="${RESEARCH_JOBS}${cap},"
                  ;;
                planning|scene-composition|content-planning|narration-creation)
                  PLANNING_JOBS="${PLANNING_JOBS}${cap},"
                  ;;
                image-generation|video-generation|audio-generation|text-to-speech)
                  MEDIA_JOBS="${MEDIA_JOBS}${cap},"
                  ;;
                lipsync|subtitle-overlay|video-editing|video-concat)
                  POST_JOBS="${POST_JOBS}${cap},"
                  ;;
                newsapi|weather|youtube-upload|youtube-video-info|openai-gpt|openai-summarize|openai-translate|openai-image-gen|elevenlabs-tts|huggingface-inference|slack-notify|slack-file-upload|discord-webhook|telegram-send-message|sendgrid-send-email|twitter-post|twitter-search|reddit-search|google-sheets-read|google-sheets-write|finnhub-stock-quote|github-issue-create|github-repo-search|github-workflow-dispatch|github-release-create|arxiv-search|notion-create-page)
                  EXTERNAL_JOBS="${EXTERNAL_JOBS}${cap},"
                  ;;
              esac
            fi
          done
          
          # Research phase (sequential)
          if [ -n "$RESEARCH_JOBS" ]; then
            IFS=',' read -ra RESEARCH_ARRAY <<< "$RESEARCH_JOBS"
            JOB_INDEX=1
            for research_job in "${RESEARCH_ARRAY[@]}"; do
              if [ -n "$research_job" ]; then
                JOB_NAME="research_${JOB_INDEX}"
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"ðŸ” Research: ${research_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$research_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$research_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    uses: ./minimal-units/planning/web-search.yml" >> "$WORKFLOW_FILE"
                fi
                if [ $JOB_INDEX -eq 1 ]; then
                  echo '    needs: setup' >> "$WORKFLOW_FILE"
                else
                  PREV_JOB="research_$((JOB_INDEX - 1))"
                  echo "    needs: [setup, ${PREV_JOB}]" >> "$WORKFLOW_FILE"
                fi
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      query: "${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/metadata"' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
                JOB_INDEX=$((JOB_INDEX + 1))
              fi
            done
          fi
          
          # Planning phase (sequential with dependency)
          if [ -n "$PLANNING_JOBS" ]; then
            IFS=',' read -ra PLANNING_ARRAY <<< "$PLANNING_JOBS"
            JOB_INDEX=1
            PREV_PHASE_JOB=""
            if [ -n "$RESEARCH_JOBS" ]; then
              RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              PREV_PHASE_JOB="research_${RESEARCH_COUNT}"
            fi
            
            for planning_job in "${PLANNING_ARRAY[@]}"; do
              if [ -n "$planning_job" ]; then
                JOB_NAME="planning_${JOB_INDEX}"
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"ðŸ“‹ Planning: ${planning_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$planning_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$planning_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    uses: ./minimal-units/planning/planning-ccsdk.yml" >> "$WORKFLOW_FILE"
                fi
                
                # Dependencies
                if [ $JOB_INDEX -eq 1 ]; then
                  if [ -n "$PREV_PHASE_JOB" ]; then
                    echo "    needs: [setup, ${PREV_PHASE_JOB}]" >> "$WORKFLOW_FILE"
                  else
                    echo '    needs: setup' >> "$WORKFLOW_FILE"
                  fi
                else
                  PREV_JOB="planning_$((JOB_INDEX - 1))"
                  echo "    needs: ${PREV_JOB}" >> "$WORKFLOW_FILE"
                fi
                
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      concept: "Issue #${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/metadata"' >> "$WORKFLOW_FILE"
                echo '    secrets:' >> "$WORKFLOW_FILE"
                echo '      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
                JOB_INDEX=$((JOB_INDEX + 1))
              fi
            done
          fi
          
          # Media generation phase (parallel execution)
          if [ -n "$MEDIA_JOBS" ]; then
            # Determine dependencies for media jobs
            MEDIA_DEPS="setup"
            if [ -n "$PLANNING_JOBS" ]; then
              PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              MEDIA_DEPS="planning_${PLANNING_COUNT}"
            elif [ -n "$RESEARCH_JOBS" ]; then
              RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              MEDIA_DEPS="research_${RESEARCH_COUNT}"
            fi
            
            IFS=',' read -ra MEDIA_ARRAY <<< "$MEDIA_JOBS"
            for media_job in "${MEDIA_ARRAY[@]}"; do
              if [ -n "$media_job" ]; then
                JOB_NAME=$(echo "$media_job" | tr '-' '_')
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"ðŸŽ¨ ${media_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$media_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$media_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    runs-on: ubuntu-latest" >> "$WORKFLOW_FILE"
                fi
                echo "    needs: ${MEDIA_DEPS}" >> "$WORKFLOW_FILE"
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      prompt: "Generated content for issue #${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/media"' >> "$WORKFLOW_FILE"
                echo '    secrets:' >> "$WORKFLOW_FILE"
                echo '      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
              fi
            done
          fi
          
          # Post-processing phase (sequential)
          if [ -n "$POST_JOBS" ]; then
            # Collect all media job names for dependencies
            MEDIA_JOB_NAMES=""
            if [ -n "$MEDIA_JOBS" ]; then
              IFS=',' read -ra MEDIA_ARRAY <<< "$MEDIA_JOBS"
              for media_job in "${MEDIA_ARRAY[@]}"; do
                if [ -n "$media_job" ]; then
                  JOB_NAME=$(echo "$media_job" | tr '-' '_')
                  MEDIA_JOB_NAMES="${MEDIA_JOB_NAMES}${JOB_NAME}, "
                fi
              done
              MEDIA_JOB_NAMES="${MEDIA_JOB_NAMES%, }"
            fi
            
            IFS=',' read -ra POST_ARRAY <<< "$POST_JOBS"
            JOB_INDEX=1
            for post_job in "${POST_ARRAY[@]}"; do
              if [ -n "$post_job" ]; then
                JOB_NAME="post_${JOB_INDEX}"
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"ðŸ”§ Post: ${post_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$post_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$post_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    runs-on: ubuntu-latest" >> "$WORKFLOW_FILE"
                fi
                
                # Dependencies
                if [ $JOB_INDEX -eq 1 ]; then
                  if [ -n "$MEDIA_JOB_NAMES" ]; then
                    echo "    needs: [${MEDIA_JOB_NAMES}]" >> "$WORKFLOW_FILE"
                  else
                    echo '    needs: setup' >> "$WORKFLOW_FILE"
                  fi
                else
                  PREV_JOB="post_$((JOB_INDEX - 1))"
                  echo "    needs: ${PREV_JOB}" >> "$WORKFLOW_FILE"
                fi
                
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      video_list: "placeholder"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/final"' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
                JOB_INDEX=$((JOB_INDEX + 1))
              fi
            done
          fi
          
          # External API phase (mixed parallel/sequential)
          if [ -n "$EXTERNAL_JOBS" ]; then
            # Determine dependencies for external API jobs
            EXTERNAL_DEPS="setup"
            if [ -n "$POST_JOBS" ]; then
              POST_COUNT=$(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              EXTERNAL_DEPS="post_${POST_COUNT}"
            elif [ -n "$MEDIA_JOBS" ]; then
              EXTERNAL_DEPS="${MEDIA_JOB_NAMES}"
            elif [ -n "$PLANNING_JOBS" ]; then
              PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              EXTERNAL_DEPS="planning_${PLANNING_COUNT}"
            elif [ -n "$RESEARCH_JOBS" ]; then
              RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              EXTERNAL_DEPS="research_${RESEARCH_COUNT}"
            fi
            
            IFS=',' read -ra EXTERNAL_ARRAY <<< "$EXTERNAL_JOBS"
            for external_job in "${EXTERNAL_ARRAY[@]}"; do
              if [ -n "$external_job" ]; then
                JOB_NAME=$(echo "$external_job" | tr '-' '_')
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"ðŸŒ External API: ${external_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$external_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$external_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    runs-on: ubuntu-latest" >> "$WORKFLOW_FILE"
                fi
                
                # Dependencies based on type
                case "$external_job" in
                  # Input APIs - can run early/parallel
                  newsapi|weather|youtube-video-info|reddit-search|arxiv-search|github-repo-search|finnhub-stock-quote|google-sheets-read)
                    echo "    needs: setup" >> "$WORKFLOW_FILE"
                    ;;
                  
                  # Processing APIs - need content first
                  openai-gpt|openai-summarize|openai-translate|openai-image-gen|huggingface-inference)
                    if [ -n "$PLANNING_JOBS" ]; then
                      PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
                      echo "    needs: planning_${PLANNING_COUNT}" >> "$WORKFLOW_FILE"
                    else
                      echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    fi
                    ;;
                  
                  # Audio generation - after planning
                  elevenlabs-tts)
                    if [ -n "$PLANNING_JOBS" ]; then
                      PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
                      echo "    needs: planning_${PLANNING_COUNT}" >> "$WORKFLOW_FILE"
                    else
                      echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    fi
                    ;;
                  
                  # Output APIs - need final content
                  youtube-upload|slack-notify|slack-file-upload|discord-webhook|telegram-send-message|sendgrid-send-email|twitter-post|google-sheets-write|github-issue-create|github-workflow-dispatch|github-release-create|notion-create-page)
                    if [ -n "$POST_JOBS" ]; then
                      POST_COUNT=$(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
                      echo "    needs: post_${POST_COUNT}" >> "$WORKFLOW_FILE"
                    elif [ -n "$MEDIA_JOBS" ]; then
                      echo "    needs: [${MEDIA_JOB_NAMES}]" >> "$WORKFLOW_FILE"
                    else
                      echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    fi
                    ;;
                  
                  # Search APIs - can run early
                  twitter-search)
                    echo "    needs: setup" >> "$WORKFLOW_FILE"
                    ;;
                  
                  *)
                    echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    ;;
                esac
                
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      project_dir: "${{ needs.setup.outputs.project_dir }}"' >> "$WORKFLOW_FILE"
                echo '    secrets:' >> "$WORKFLOW_FILE"
                
                # Add required secrets based on API type
                case "$external_job" in
                  newsapi)
                    echo '      NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  weather)
                    echo '      OPENWEATHERMAP_API_KEY: ${{ secrets.OPENWEATHERMAP_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  youtube-upload|youtube-video-info)
                    echo '      YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  openai-*)
                    echo '      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  elevenlabs-tts)
                    echo '      ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  huggingface-inference)
                    echo '      HUGGINGFACE_API_KEY: ${{ secrets.HUGGINGFACE_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  slack-notify|slack-file-upload)
                    echo '      SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
                    ;;
                  discord-webhook)
                    echo '      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}' >> "$WORKFLOW_FILE"
                    ;;
                  telegram-send-message)
                    echo '      TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
                    ;;
                  sendgrid-send-email)
                    echo '      SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  twitter-post|twitter-search)
                    echo '      TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  reddit-search)
                    echo '      REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}' >> "$WORKFLOW_FILE"
                    echo '      REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}' >> "$WORKFLOW_FILE"
                    ;;
                  google-sheets-read|google-sheets-write)
                    echo '      GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}' >> "$WORKFLOW_FILE"
                    ;;
                  finnhub-stock-quote)
                    echo '      FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  github-*)
                    echo '      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}' >> "$WORKFLOW_FILE"
                    ;;
                  arxiv-search)
                    # arXiv API doesn't require authentication
                    ;;
                  notion-create-page)
                    echo '      NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                esac
                
                echo '' >> "$WORKFLOW_FILE"
              fi
            done
          fi
          
          # Summary job
          echo '  summary:' >> "$WORKFLOW_FILE"
          echo '    name: "ðŸ“Š Summary"' >> "$WORKFLOW_FILE"
          echo '    runs-on: ubuntu-latest' >> "$WORKFLOW_FILE"
          echo '    if: always()' >> "$WORKFLOW_FILE"
          
          # Build needs list for summary
          ALL_JOBS="setup"
          if [ -n "$RESEARCH_JOBS" ]; then
            RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
            for i in $(seq 1 $RESEARCH_COUNT); do
              ALL_JOBS="${ALL_JOBS}, research_${i}"
            done
          fi
          if [ -n "$PLANNING_JOBS" ]; then
            PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
            for i in $(seq 1 $PLANNING_COUNT); do
              ALL_JOBS="${ALL_JOBS}, planning_${i}"
            done
          fi
          if [ -n "$MEDIA_JOBS" ]; then
            IFS=',' read -ra MEDIA_ARRAY <<< "$MEDIA_JOBS"
            for media_job in "${MEDIA_ARRAY[@]}"; do
              if [ -n "$media_job" ]; then
                JOB_NAME=$(echo "$media_job" | tr '-' '_')
                ALL_JOBS="${ALL_JOBS}, ${JOB_NAME}"
              fi
            done
          fi
          if [ -n "$POST_JOBS" ]; then
            POST_COUNT=$(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
            for i in $(seq 1 $POST_COUNT); do
              ALL_JOBS="${ALL_JOBS}, post_${i}"
            done
          fi
          if [ -n "$EXTERNAL_JOBS" ]; then
            IFS=',' read -ra EXTERNAL_ARRAY <<< "$EXTERNAL_JOBS"
            for external_job in "${EXTERNAL_ARRAY[@]}"; do
              if [ -n "$external_job" ]; then
                JOB_NAME=$(echo "$external_job" | tr '-' '_')
                ALL_JOBS="${ALL_JOBS}, ${JOB_NAME}"
              fi
            done
          fi
          
          echo "    needs: [${ALL_JOBS}]" >> "$WORKFLOW_FILE"
          echo '    steps:' >> "$WORKFLOW_FILE"
          echo '      - name: Generate Summary' >> "$WORKFLOW_FILE"
          echo '        run: |' >> "$WORKFLOW_FILE"
          echo '          echo "# ðŸŽ¯ Workflow Execution Summary" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          echo '          echo "- **Issue**: #${{ github.event.inputs.issue_number }}" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          echo '          echo "- **Status**: Completed" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          echo '          echo "- **Project**: ${{ needs.setup.outputs.project_dir }}" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          
          # Count total jobs
          JOB_COUNT=2  # setup + summary
          [ -n "$RESEARCH_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$PLANNING_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$MEDIA_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$MEDIA_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$POST_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$EXTERNAL_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$EXTERNAL_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          
          echo "workflow_id=${ISSUE_NUMBER}-$(date +%Y%m%d-%H%M%S)" >> $GITHUB_OUTPUT
          echo "total_jobs=$JOB_COUNT" >> $GITHUB_OUTPUT
          
          echo "âœ… Generated workflow with $JOB_COUNT jobs"
          
      - name: Upload Generated Workflow
        uses: actions/upload-artifact@v4
        with:
          name: generated-workflow
          path: generated-workflows/

  # ===========================================
  # PHASE 5: WORKFLOW VALIDATION
  # ===========================================
  
  workflow-validation:
    name: "âœ… Workflow Validation"
    runs-on: ubuntu-latest
    needs: ['workflow-generation']
    outputs:
      validation_passed: ${{ steps.validate.outputs.passed }}
      optimization_needed: ${{ steps.validate.outputs.needs_optimization }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Generated Workflow
        uses: actions/download-artifact@v4
        with:
          name: generated-workflow
          path: generated-workflows/
          
      - name: Validate Workflow
        id: validate
        run: |
          echo "âœ… Validating generated workflow..."
          
          WORKFLOW_FILE=$(find generated-workflows -name '*.yml' | head -1)
          
          # Check YAML syntax
          python3 -c "import yaml; yaml.safe_load(open('$WORKFLOW_FILE'))" 2>/dev/null
          if [ $? -eq 0 ]; then
            echo "âœ… YAML syntax valid"
            SYNTAX_OK=true
          else
            echo "âŒ YAML syntax error"
            SYNTAX_OK=false
          fi
          
          # Check for long expressions
          LONG_LINES=$(grep -n '\${{' "$WORKFLOW_FILE" | grep '}}' | awk 'length($0) > 500' | wc -l)
          if [ "$LONG_LINES" -gt 0 ]; then
            echo "âš ï¸ Found $LONG_LINES long expression lines"
            echo "needs_optimization=true" >> $GITHUB_OUTPUT
          else
            echo "âœ… No long expressions found"
            echo "needs_optimization=false" >> $GITHUB_OUTPUT
          fi
          
          # Check job count
          JOB_COUNT=$(grep -c "^  [a-zA-Z][a-zA-Z0-9_-]*:" "$WORKFLOW_FILE" | grep -v "^on:" | grep -v "^jobs:" || echo 0)
          if [ "$JOB_COUNT" -gt 10 ]; then
            echo "âš ï¸ Too many jobs ($JOB_COUNT), consider parallelization"
            echo "needs_optimization=true" >> $GITHUB_OUTPUT
          fi
          
          if [ "$SYNTAX_OK" = true ]; then
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "passed=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Prepare Optimization Script Part 1
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "ðŸ“Š Preparing optimization script (Part 1)..."
          
          # Create optimization script header using echo commands
          echo '#!/usr/bin/env python3' > optimize_part1.py
          echo 'import yaml' >> optimize_part1.py
          echo 'import sys' >> optimize_part1.py
          echo 'import json' >> optimize_part1.py
          echo 'from typing import List, Dict, Any' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def load_workflow(file_path: str) -> Dict:' >> optimize_part1.py
          echo '    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""' >> optimize_part1.py
          echo '    with open(file_path, "r", encoding="utf-8") as f:' >> optimize_part1.py
          echo '        return yaml.safe_load(f)' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def analyze_node_length(job_content: Dict) -> int:' >> optimize_part1.py
          echo '    """ãƒŽãƒ¼ãƒ‰ã®é•·ã•ã‚’åˆ†æž"""' >> optimize_part1.py
          echo '    content_str = yaml.dump(job_content)' >> optimize_part1.py
          echo '    return len(content_str)' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def identify_long_nodes(workflow: Dict, threshold: int = 15000) -> List[str]:' >> optimize_part1.py
          echo '    """é•·ã„ãƒŽãƒ¼ãƒ‰ã‚’ç‰¹å®š"""' >> optimize_part1.py
          echo '    long_nodes = []' >> optimize_part1.py
          echo '    if "jobs" in workflow:' >> optimize_part1.py
          echo '        for job_name, job_content in workflow["jobs"].items():' >> optimize_part1.py
          echo '            length = analyze_node_length(job_content)' >> optimize_part1.py
          echo '            if length > threshold:' >> optimize_part1.py
          echo '                long_nodes.append(job_name)' >> optimize_part1.py
          echo '                print(f"âš ï¸ Long node detected: {job_name} ({length} chars)")' >> optimize_part1.py
          echo '    return long_nodes' >> optimize_part1.py
          
      - name: Prepare Optimization Script Part 2
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "ðŸ“Š Preparing optimization script (Part 2)..."
          
          # Add node splitting logic using echo commands
          echo '' >> optimize_part1.py
          echo 'def split_long_node(job_name: str, job_content: Dict) -> Dict[str, Dict]:' >> optimize_part1.py
          echo '    """é•·ã„ãƒŽãƒ¼ãƒ‰ã‚’åˆ†å‰²"""' >> optimize_part1.py
          echo '    split_jobs = {}' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    if "steps" in job_content and len(job_content["steps"]) > 10:' >> optimize_part1.py
          echo '        # ã‚¹ãƒ†ãƒƒãƒ—æ•°ãŒå¤šã„å ´åˆã¯åˆ†å‰²' >> optimize_part1.py
          echo '        steps = job_content["steps"]' >> optimize_part1.py
          echo '        mid_point = len(steps) // 2' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        # Part 1' >> optimize_part1.py
          echo '        job1 = job_content.copy()' >> optimize_part1.py
          echo '        job1["steps"] = steps[:mid_point]' >> optimize_part1.py
          echo '        split_jobs[f"{job_name}_part1"] = job1' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        # Part 2' >> optimize_part1.py
          echo '        job2 = job_content.copy()' >> optimize_part1.py
          echo '        job2["steps"] = steps[mid_point:]' >> optimize_part1.py
          echo '        if "needs" in job1:' >> optimize_part1.py
          echo '            job2["needs"] = [f"{job_name}_part1"]' >> optimize_part1.py
          echo '        split_jobs[f"{job_name}_part2"] = job2' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        print(f"âœ… Split {job_name} into 2 parts")' >> optimize_part1.py
          echo '    else:' >> optimize_part1.py
          echo '        split_jobs[job_name] = job_content' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    return split_jobs' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def identify_parallel_opportunities(workflow: Dict) -> List[List[str]]:' >> optimize_part1.py
          echo '    """ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªã‚¸ãƒ§ãƒ–ã‚’ç‰¹å®š"""' >> optimize_part1.py
          echo '    parallel_groups = []' >> optimize_part1.py
          echo '    jobs = workflow.get("jobs", {})' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # ä¾å­˜é–¢ä¿‚ã®ãªã„ã‚¸ãƒ§ãƒ–ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–' >> optimize_part1.py
          echo '    no_deps = []' >> optimize_part1.py
          echo '    for job_name, job_content in jobs.items():' >> optimize_part1.py
          echo '        if "needs" not in job_content or not job_content["needs"]:' >> optimize_part1.py
          echo '            no_deps.append(job_name)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    if len(no_deps) > 1:' >> optimize_part1.py
          echo '        parallel_groups.append(no_deps)' >> optimize_part1.py
          echo '        print(f"ðŸš€ Found parallel group: {no_deps}")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    return parallel_groups' >> optimize_part1.py

      - name: Prepare Optimization Script Part 3
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "ðŸ“Š Preparing optimization script (Part 3)..."
          
          # Add main optimization logic using echo commands
          echo '' >> optimize_part1.py
          echo 'def optimize_workflow(workflow: Dict) -> Dict:' >> optimize_part1.py
          echo '    """ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æœ€é©åŒ–"""' >> optimize_part1.py
          echo '    optimized = workflow.copy()' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # 1. é•·ã„ãƒŽãƒ¼ãƒ‰ã‚’ç‰¹å®šã—ã¦åˆ†å‰²' >> optimize_part1.py
          echo '    long_nodes = identify_long_nodes(workflow)' >> optimize_part1.py
          echo '    if long_nodes:' >> optimize_part1.py
          echo '        print(f"\\nðŸ” Found {len(long_nodes)} long nodes to split")' >> optimize_part1.py
          echo '        new_jobs = {}' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        for job_name, job_content in workflow.get("jobs", {}).items():' >> optimize_part1.py
          echo '            if job_name in long_nodes:' >> optimize_part1.py
          echo '                split_jobs = split_long_node(job_name, job_content)' >> optimize_part1.py
          echo '                new_jobs.update(split_jobs)' >> optimize_part1.py
          echo '            else:' >> optimize_part1.py
          echo '                new_jobs[job_name] = job_content' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        optimized["jobs"] = new_jobs' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # 2. ä¸¦åˆ—å®Ÿè¡Œã®æœ€é©åŒ–' >> optimize_part1.py
          echo '    parallel_groups = identify_parallel_opportunities(optimized)' >> optimize_part1.py
          echo '    if parallel_groups:' >> optimize_part1.py
          echo '        print(f"\\nðŸš€ Identified {len(parallel_groups)} parallel execution opportunities")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    return optimized' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo '# ãƒ¡ã‚¤ãƒ³å‡¦ç†' >> optimize_part1.py
          echo 'if __name__ == "__main__":' >> optimize_part1.py
          echo '    if len(sys.argv) < 2:' >> optimize_part1.py
          echo '        print("Usage: python optimize.py <workflow.yml>")' >> optimize_part1.py
          echo '        sys.exit(1)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    workflow_file = sys.argv[1]' >> optimize_part1.py
          echo '    print(f"ðŸ”§ Optimizing workflow: {workflow_file}")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’èª­ã¿è¾¼ã¿' >> optimize_part1.py
          echo '    workflow = load_workflow(workflow_file)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # æœ€é©åŒ–' >> optimize_part1.py
          echo '    optimized = optimize_workflow(workflow)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # çµæžœã‚’ä¿å­˜' >> optimize_part1.py
          echo '    output_file = workflow_file.replace(".yml", "_optimized.yml")' >> optimize_part1.py
          echo '    with open(output_file, "w", encoding="utf-8") as f:' >> optimize_part1.py
          echo '        yaml.dump(optimized, f, default_flow_style=False, sort_keys=False)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    print(f"âœ… Optimized workflow saved to: {output_file}")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›' >> optimize_part1.py
          echo '    metadata = {' >> optimize_part1.py
          echo '        "original_jobs": len(workflow.get("jobs", {})),' >> optimize_part1.py
          echo '        "optimized_jobs": len(optimized.get("jobs", {})),' >> optimize_part1.py
          echo '        "long_nodes_split": len(identify_long_nodes(workflow)),' >> optimize_part1.py
          echo '        "parallel_groups": len(identify_parallel_opportunities(optimized))' >> optimize_part1.py
          echo '    }' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    with open("optimization_metadata.json", "w") as f:' >> optimize_part1.py
          echo '        json.dump(metadata, f, indent=2)' >> optimize_part1.py

      - name: Execute Optimization
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "ðŸš€ Executing optimization..."
          
          # Combine script parts
          mv optimize_part1.py optimize.py
          
          # Find and optimize workflow
          WORKFLOW_FILE=$(find generated-workflows -name '*.yml' | head -1)
          
          if [ -f "$WORKFLOW_FILE" ]; then
            python optimize.py "$WORKFLOW_FILE"
            
            # Check if optimization was successful
            OPTIMIZED_FILE="${WORKFLOW_FILE%.yml}_optimized.yml"
            if [ -f "$OPTIMIZED_FILE" ]; then
              # Replace original with optimized version
              mv "$OPTIMIZED_FILE" "$WORKFLOW_FILE"
              echo "âœ… Workflow optimized successfully"
              
              # Read metadata
              if [ -f "optimization_metadata.json" ]; then
                cat optimization_metadata.json
              fi
            else
              echo "âš ï¸ Optimization did not produce output"
            fi
          else
            echo "âš ï¸ No workflow file found to optimize"
          fi
          
      - name: Upload Validated Workflow
        uses: actions/upload-artifact@v4
        with:
          name: validated-workflow
          path: generated-workflows/

  # ===========================================
  # PHASE 6: DEPLOYMENT
  # ===========================================
  
  deployment:
    name: "ðŸ“¦ Deployment"
    runs-on: ubuntu-latest
    needs: ['workflow-validation', 'workflow-generation']
    if: ${{ needs['workflow-validation'].outputs.validation_passed == 'true' }}
    outputs:
      deployed_path: ${{ steps.deploy.outputs.path }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Validated Workflow
        uses: actions/download-artifact@v4
        with:
          name: validated-workflow
          path: generated-workflows/
          
      - name: Deploy Workflow
        id: deploy
        run: |
          echo "ðŸ“¦ Deploying workflow..."
          
          WORKFLOW_FILE=$(find generated-workflows -name '*.yml' | head -1)
          DEPLOYMENT_NAME="issue-${{ inputs.issue_number }}-$(date +%Y%m%d-%H%M%S).yml.disabled"
          DEPLOYMENT_PATH=".github/workflows/generated/$DEPLOYMENT_NAME"
          
          # Create deployment directory
          mkdir -p .github/workflows/generated
          
          # Copy workflow
          cp "$WORKFLOW_FILE" "$DEPLOYMENT_PATH"
          
          echo "path=$DEPLOYMENT_PATH" >> $GITHUB_OUTPUT
          
          # Commit and push
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add "$DEPLOYMENT_PATH"
          git commit -m "ðŸ¤– Generated workflow for Issue #${{ inputs.issue_number }}" || echo "No changes"
          git push origin main || echo "Push failed"
          
          echo "âœ… Deployed to: $DEPLOYMENT_PATH"

  # ===========================================
  # PHASE 7: EXECUTION SUMMARY
  # ===========================================
  
  execution-summary:
    name: "ðŸ“Š Execution Summary"
    runs-on: ubuntu-latest
    needs: ['validate-trigger', 'basic-task-decomposition', 'detailed-analysis', 'workflow-generation', 'workflow-validation', 'deployment']
    if: ${{ always() }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Generate Summary
        env:
          ISSUE_NUMBER: ${{ inputs.issue_number }}
          ISSUE_TITLE: ${{ needs['validate-trigger'].outputs.issue_title }}
          REQUEST_TYPE: ${{ needs['validate-trigger'].outputs.request_type }}
          CAPABILITIES_COUNT: ${{ needs['basic-task-decomposition'].outputs.capabilities_count }}
          COMPLEXITY: ${{ needs['basic-task-decomposition'].outputs.complexity }}
          HAS_VIDEO: ${{ needs['basic-task-decomposition'].outputs.has_video }}
          HAS_NARRATION: ${{ needs['basic-task-decomposition'].outputs.has_narration }}
          ORCHESTRATOR_MATCH: ${{ needs['detailed-analysis'].outputs.orchestrator_match }}
          EXECUTION_PATTERN: ${{ needs['detailed-analysis'].outputs.execution_pattern }}
          TOTAL_JOBS: ${{ needs['workflow-generation'].outputs.total_jobs }}
          VALIDATION_PASSED: ${{ needs['workflow-validation'].outputs.validation_passed }}
          OPTIMIZATION_NEEDED: ${{ needs['workflow-validation'].outputs.optimization_needed }}
          DEPLOYED_PATH: ${{ needs.deployment.outputs.deployed_path }}
        run: |
          echo "# ðŸ“Š Meta Workflow v10 å®Ÿè¡Œçµæžœ" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## ðŸ“‹ ãƒªã‚¯ã‚¨ã‚¹ãƒˆæƒ…å ±" >> $GITHUB_STEP_SUMMARY
          echo "- **Issue**: #$ISSUE_NUMBER" >> $GITHUB_STEP_SUMMARY
          echo "- **ã‚¿ã‚¤ãƒˆãƒ«**: $ISSUE_TITLE" >> $GITHUB_STEP_SUMMARY
          echo "- **ã‚¿ã‚¤ãƒ—**: $REQUEST_TYPE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## ðŸ” ã‚¿ã‚¹ã‚¯åˆ†è§£çµæžœ" >> $GITHUB_STEP_SUMMARY
          echo "### æ¤œå‡ºã•ã‚ŒãŸèƒ½åŠ›" >> $GITHUB_STEP_SUMMARY
          if [ -f "artifacts/capabilities-data/capabilities.txt" ]; then
            CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
            IFS=',' read -ra CAP_ARRAY <<< "$CAPABILITIES"
            for cap in "${CAP_ARRAY[@]}"; do
              if [ -n "$cap" ]; then
                echo "- âœ… $cap" >> $GITHUB_STEP_SUMMARY
              fi
            done
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### ã‚¿ã‚¹ã‚¯åˆ†è§£ã®è©³ç´°" >> $GITHUB_STEP_SUMMARY
          echo "- **æ¤œå‡ºèƒ½åŠ›æ•°**: $CAPABILITIES_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **è¤‡é›‘åº¦**: $COMPLEXITY" >> $GITHUB_STEP_SUMMARY
          
          # Show fine-grained decomposition
          if [ "$HAS_VIDEO" == "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "#### ðŸŽ¬ å‹•ç”»ç”Ÿæˆã‚¿ã‚¹ã‚¯ã®è©³ç´°åˆ†è§£:" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ“‹ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¼ç”» (content-planning)" >> $GITHUB_STEP_SUMMARY
            if [ "$HAS_NARRATION" == "true" ]; then
              echo "- ðŸŽ¤ ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ (narration-creation)" >> $GITHUB_STEP_SUMMARY
              echo "- ðŸ‘„ ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯å‡¦ç† (lipsync)" >> $GITHUB_STEP_SUMMARY
              echo "- ðŸ“ å­—å¹•ã‚ªãƒ¼ãƒãƒ¼ãƒ¬ã‚¤ (subtitle-overlay)" >> $GITHUB_STEP_SUMMARY
            fi
            echo "- ðŸŽ¨ ã‚·ãƒ¼ãƒ³æ§‹æˆ (scene-composition)" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸ–¼ï¸ ç”»åƒç”Ÿæˆ (image-generation)" >> $GITHUB_STEP_SUMMARY
            echo "- ðŸŽ¬ å‹•ç”»ç”Ÿæˆ (video-generation)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## ðŸ§  ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼åˆ†æž" >> $GITHUB_STEP_SUMMARY
          echo "- **ãƒžãƒƒãƒãƒ³ã‚°**: $ORCHESTRATOR_MATCH" >> $GITHUB_STEP_SUMMARY
          echo "- **å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³**: $EXECUTION_PATTERN" >> $GITHUB_STEP_SUMMARY
          
          # Show Mermaid diagram if exists
          if [ -f "artifacts/analysis-data/metadata/mermaid.txt" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“Š å®Ÿè¡Œãƒ•ãƒ­ãƒ¼å›³" >> $GITHUB_STEP_SUMMARY
            echo '```mermaid' >> $GITHUB_STEP_SUMMARY
            cat artifacts/analysis-data/metadata/mermaid.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## ðŸš€ ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ" >> $GITHUB_STEP_SUMMARY
          echo "- **ç”Ÿæˆã‚¸ãƒ§ãƒ–æ•°**: $TOTAL_JOBS" >> $GITHUB_STEP_SUMMARY
          echo "- **æ¤œè¨¼çµæžœ**: $VALIDATION_PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- **æœ€é©åŒ–**: $OPTIMIZATION_NEEDED" >> $GITHUB_STEP_SUMMARY
          
          # Show job structure
          if [ -d "artifacts/generated-workflow" ] && [ -n "$(ls -A artifacts/generated-workflow/*.yml 2>/dev/null)" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸ“‹ ç”Ÿæˆã•ã‚ŒãŸã‚¸ãƒ§ãƒ–æ§‹é€ " >> $GITHUB_STEP_SUMMARY
            WORKFLOW_FILE=$(find artifacts/generated-workflow -name '*.yml' | head -1)
            # Extract job names
            grep -E "^  [a-zA-Z_-]+:" "$WORKFLOW_FILE" | grep -v "^  workflow_dispatch:" | sed 's/://g' | while read job; do
              echo "- $job" >> $GITHUB_STEP_SUMMARY
            done
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$DEPLOYED_PATH" != "" ]; then
            echo "## âœ… ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†" >> $GITHUB_STEP_SUMMARY
            echo "- **ä¿å­˜å ´æ‰€**: \`$DEPLOYED_PATH\`" >> $GITHUB_STEP_SUMMARY
            echo "- **ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³**: .disabledæ‹¡å¼µå­ã‚’å‰Šé™¤ã—ã¦æœ‰åŠ¹åŒ–" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### ðŸŽ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—" >> $GITHUB_STEP_SUMMARY
            echo "1. ç”Ÿæˆã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç¢ºèª" >> $GITHUB_STEP_SUMMARY
            echo "2. \`.disabled\`æ‹¡å¼µå­ã‚’å‰Šé™¤ã—ã¦æœ‰åŠ¹åŒ–" >> $GITHUB_STEP_SUMMARY
            echo "3. Actions ã‚¿ãƒ–ã‹ã‚‰æ‰‹å‹•å®Ÿè¡Œ" >> $GITHUB_STEP_SUMMARY
          else
            echo "## âš ï¸ ãƒ‡ãƒ—ãƒ­ã‚¤ã‚¹ã‚­ãƒƒãƒ—" >> $GITHUB_STEP_SUMMARY
            echo "æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚" >> $GITHUB_STEP_SUMMARY
          fi