name: "Meta Workflow Executor v10 with Optimized Design"
run-name: "üöÄ Meta Workflow v10 | Issue #${{ inputs.issue_number }} | ${{ github.actor }}"

on:
  workflow_dispatch:
    inputs:
      issue_number:
        description: 'Issue number for workflow generation request'
        required: true
        default: '60'

permissions:
  contents: write
  issues: write
  actions: write
  pull-requests: write

env:
  CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
  CLAUDE_CODE_CI_MODE: true
  CLAUDE_CODE_AUTO_APPROVE_MCP: true

jobs:
  # ===========================================
  # PHASE 1: ISSUE VALIDATION
  # ===========================================
  
  validate-trigger:
    name: "üîç Issue Validation"
    runs-on: ubuntu-latest
    outputs:
      issue_number: ${{ steps.extract.outputs.issue_number }}
      issue_title: ${{ steps.extract.outputs.issue_title }}
      request_type: ${{ steps.analyze.outputs.request_type }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Extract Issue Information
        id: extract
        run: |
          echo "üîç Analyzing Issue #${{ inputs.issue_number }}..."
          
          # Get issue details using GitHub CLI
          ISSUE_DATA=$(gh issue view ${{ inputs.issue_number }} --json title,body,number --jq '{title: .title, body: .body, number: .number}')
          
          ISSUE_TITLE=$(echo "$ISSUE_DATA" | jq -r '.title')
          ISSUE_BODY=$(echo "$ISSUE_DATA" | jq -r '.body')
          ISSUE_NUMBER=$(echo "$ISSUE_DATA" | jq -r '.number')
          
          # Save to artifacts for next jobs
          mkdir -p artifacts
          echo "$ISSUE_TITLE" > artifacts/issue_title.txt
          echo "$ISSUE_BODY" > artifacts/issue_body.txt
          echo "$ISSUE_NUMBER" > artifacts/issue_number.txt
          
          # Output minimal data
          echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
          echo "issue_title=$ISSUE_TITLE" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Issue #$ISSUE_NUMBER validated: $ISSUE_TITLE"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Analyze Request Type
        id: analyze
        run: |
          ISSUE_BODY=$(cat artifacts/issue_body.txt)
          
          REQUEST_TYPE="unknown"
          if echo "$ISSUE_BODY" | grep -qi "video\|ÂãïÁîª"; then
            REQUEST_TYPE="video-generation"
          elif echo "$ISSUE_BODY" | grep -qi "image\|ÁîªÂÉè"; then
            REQUEST_TYPE="image-generation"
          elif echo "$ISSUE_BODY" | grep -qi "audio\|Èü≥Â£∞\|music"; then
            REQUEST_TYPE="audio-generation"
          fi
          
          echo "request_type=$REQUEST_TYPE" >> $GITHUB_OUTPUT
          
      - name: Upload Issue Data
        uses: actions/upload-artifact@v4
        with:
          name: issue-data
          path: artifacts/

  # ===========================================
  # PHASE 2: BASIC TASK DECOMPOSITION
  # ===========================================
  
  basic-task-decomposition:
    name: "üìã Basic Task Decomposition"
    runs-on: ubuntu-latest
    needs: ['validate-trigger']
    outputs:
      capabilities_count: ${{ steps.detect.outputs.capabilities_count }}
      has_video: ${{ steps.detect.outputs.has_video }}
      has_narration: ${{ steps.detect.outputs.has_narration }}
      complexity: ${{ steps.detect.outputs.complexity }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Issue Data
        uses: actions/download-artifact@v4
        with:
          name: issue-data
          path: artifacts/
          
      - name: Detect Capabilities
        id: detect
        run: |
          echo "üîç Detecting required capabilities..."
          
          ISSUE_BODY=$(cat artifacts/issue_body.txt)
          
          # Temporary arrays to store detected capabilities
          RESEARCH_CAPS=""
          PLANNING_CAPS=""
          MEDIA_CAPS=""
          POST_CAPS=""
          
          # Detection phase (categorized)
          HAS_VIDEO=false
          HAS_NARRATION=false
          HAS_EXTERNAL_API=false
          EXTERNAL_CAPS=""
          
          # Video capabilities
          if echo "$ISSUE_BODY" | grep -qi "ÂãïÁîª\|video\|„Éì„Éá„Ç™"; then
            HAS_VIDEO=true
            echo "has_video=true" >> $GITHUB_OUTPUT
            
            # Context-based additions
            if echo "$ISSUE_BODY" | grep -qi "„Éä„É¨„Éº„Ç∑„Éß„É≥\|narration\|Èü≥Â£∞‰ªò"; then
              HAS_NARRATION=true
              echo "has_narration=true" >> $GITHUB_OUTPUT
            fi
          fi
          
          # Research phase
          if echo "$ISSUE_BODY" | grep -qi "Ê§úÁ¥¢\|Ë™øÊüª\|„Éã„É•„Éº„Çπ\|ÊúÄÊñ∞"; then
            RESEARCH_CAPS="${RESEARCH_CAPS}web-search,"
          fi
          
          # External API detection - ALL 27 APIs
          # News & Weather
          if echo "$ISSUE_BODY" | grep -qi "„Éã„É•„Éº„Çπ\|news\|Ë®ò‰∫ã"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}newsapi,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "Â§©Ê∞ó\|weather\|Ê∞óË±°"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}weather,"
            HAS_EXTERNAL_API=true
          fi
          
          # YouTube
          if echo "$ISSUE_BODY" | grep -qi "youtube\|„É¶„Éº„ÉÅ„É•„Éº„Éñ"; then
            if echo "$ISSUE_BODY" | grep -qi "„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ\|upload\|ÊäïÁ®ø"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}youtube-upload,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "ÊÉÖÂ†±\|info\|ÂèñÂæó"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}youtube-video-info,"
            fi
            HAS_EXTERNAL_API=true
          fi
          
          # AI Services
          if echo "$ISSUE_BODY" | grep -qi "Ë¶ÅÁ¥Ñ\|summarize\|„Åæ„Å®„ÇÅ"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-summarize,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "ÁøªË®≥\|translate\|translation"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-translate,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "gpt\|„ÉÅ„É£„ÉÉ„Éà\|‰ºöË©±"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-gpt,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "dall-e\|ÁîªÂÉèÁîüÊàê\|image.*generation"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}openai-image-gen,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "elevenlabs\|Èü≥Â£∞ÂêàÊàê"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}elevenlabs-tts,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "huggingface\|„Éè„ÇÆ„É≥„Ç∞„Éï„Çß„Ç§„Çπ"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}huggingface-inference,"
            HAS_EXTERNAL_API=true
          fi
          
          # Communication
          if echo "$ISSUE_BODY" | grep -qi "slack\|„Çπ„É©„ÉÉ„ÇØ"; then
            if echo "$ISSUE_BODY" | grep -qi "ÈÄöÁü•\|notify\|„É°„ÉÉ„Çª„Éº„Ç∏"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}slack-notify,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "„Éï„Ç°„Ç§„É´\|file\|„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}slack-file-upload,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "discord\|„Éá„Ç£„Çπ„Ç≥„Éº„Éâ"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}discord-webhook,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "telegram\|„ÉÜ„É¨„Ç∞„É©„É†"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}telegram-send-message,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "„É°„Éº„É´\|email\|mail"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}sendgrid-send-email,"
            HAS_EXTERNAL_API=true
          fi
          
          # Social Media
          if echo "$ISSUE_BODY" | grep -qi "twitter\|„ÉÑ„Ç§„ÉÉ„Çø„Éº\|x\.com"; then
            if echo "$ISSUE_BODY" | grep -qi "ÊäïÁ®ø\|post\|„ÉÑ„Ç§„Éº„Éà"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}twitter-post,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "Ê§úÁ¥¢\|search"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}twitter-search,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "reddit\|„É¨„Éá„Ç£„ÉÉ„Éà"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}reddit-search,"
            HAS_EXTERNAL_API=true
          fi
          
          # Data & Analytics
          if echo "$ISSUE_BODY" | grep -qi "google.*sheets\|„Çπ„Éó„É¨„ÉÉ„Éâ„Ç∑„Éº„Éà"; then
            if echo "$ISSUE_BODY" | grep -qi "Ë™≠„Åø\|read\|ÂèñÂæó"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}google-sheets-read,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "Êõ∏„Åç\|write\|‰øùÂ≠ò"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}google-sheets-write,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "Ê†™\|stock\|Ê†™‰æ°"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}finnhub-stock-quote,"
            HAS_EXTERNAL_API=true
          fi
          
          # Development
          if echo "$ISSUE_BODY" | grep -qi "github\|„ÇÆ„ÉÉ„Éà„Éè„Éñ"; then
            if echo "$ISSUE_BODY" | grep -qi "issue\|„Ç§„Ç∑„É•„Éº"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-issue-create,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "„É™„Éù„Ç∏„Éà„É™\|repository\|Ê§úÁ¥¢"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-repo-search,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "workflow\|„ÉØ„Éº„ÇØ„Éï„É≠„Éº\|ÂÆüË°å"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-workflow-dispatch,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "release\|„É™„É™„Éº„Çπ"; then
              EXTERNAL_CAPS="${EXTERNAL_CAPS}github-release-create,"
            fi
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "arxiv\|Ë´ñÊñá"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}arxiv-search,"
            HAS_EXTERNAL_API=true
          fi
          if echo "$ISSUE_BODY" | grep -qi "notion\|„Éé„Éº„Ç∑„Éß„É≥"; then
            EXTERNAL_CAPS="${EXTERNAL_CAPS}notion-create-page,"
            HAS_EXTERNAL_API=true
          fi
          
          # Planning phase
          if echo "$ISSUE_BODY" | grep -qi "‰ºÅÁîª\|Ë®àÁîª\|ÊßãÊàê"; then
            PLANNING_CAPS="${PLANNING_CAPS}planning,"
          fi
          
          # For video content, add detailed planning capabilities
          if [ "$HAS_VIDEO" = true ]; then
            PLANNING_CAPS="${PLANNING_CAPS}content-planning,"
            if [ "$HAS_NARRATION" = true ]; then
              PLANNING_CAPS="${PLANNING_CAPS}narration-creation,"
            fi
            if echo "$ISSUE_BODY" | grep -qi "„Ç∑„Éº„É≥\|Ë§áÊï∞\|„Çπ„Éà„Éº„É™„Éº"; then
              PLANNING_CAPS="${PLANNING_CAPS}scene-composition,"
            fi
          fi
          
          # Media generation phase
          if [ "$HAS_VIDEO" = true ]; then
            MEDIA_CAPS="${MEDIA_CAPS}image-generation,"
            if [ "$HAS_NARRATION" = true ]; then
              MEDIA_CAPS="${MEDIA_CAPS}text-to-speech,"
            fi
            MEDIA_CAPS="${MEDIA_CAPS}video-generation,"
          fi
          
          # Post-processing phase
          if [ "$HAS_VIDEO" = true ] && [ "$HAS_NARRATION" = true ]; then
            POST_CAPS="${POST_CAPS}lipsync,subtitle-overlay,"
          fi
          
          # Combine in logical order
          CAPABILITIES="${RESEARCH_CAPS}${PLANNING_CAPS}${MEDIA_CAPS}${POST_CAPS}${EXTERNAL_CAPS}"
          
          # Save capabilities
          mkdir -p metadata
          echo "$CAPABILITIES" > metadata/capabilities.txt
          
          # Count and complexity
          CAP_COUNT=$(echo "$CAPABILITIES" | tr ',' '\n' | grep -v '^$' | wc -l)
          echo "capabilities_count=$CAP_COUNT" >> $GITHUB_OUTPUT
          
          if [ "$CAP_COUNT" -gt 6 ]; then
            echo "complexity=complex" >> $GITHUB_OUTPUT
          elif [ "$CAP_COUNT" -gt 3 ]; then
            echo "complexity=medium" >> $GITHUB_OUTPUT
          else
            echo "complexity=simple" >> $GITHUB_OUTPUT
          fi
          
          echo "‚úÖ Detected $CAP_COUNT capabilities"
          
      - name: Upload Capabilities
        uses: actions/upload-artifact@v4
        with:
          name: capabilities-data
          path: metadata/

  # ===========================================
  # PHASE 3: DETAILED ANALYSIS
  # ===========================================
  
  detailed-analysis:
    name: "üß† Detailed Analysis"
    runs-on: ubuntu-latest
    needs: ['validate-trigger', 'basic-task-decomposition']
    outputs:
      orchestrator_match: ${{ steps.orchestrator.outputs.match_found }}
      execution_pattern: ${{ steps.orchestrator.outputs.pattern }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          
      - name: Install Dependencies
        run: |
          pip install pyyaml
          
      - name: Download Previous Data
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Orchestrator Analysis
        id: orchestrator
        run: |
          echo "üîç Running orchestrator analysis..."
          
          # Copy analyzer script
          if [ -f scripts/orchestrator_analyzer.py ]; then
            cp scripts/orchestrator_analyzer.py .
            
            # Run analysis
            export USER_REQUEST=$(cat artifacts/issue-data/issue_title.txt)
            export CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
            
            python orchestrator_analyzer.py || echo "‚ö†Ô∏è Orchestrator analysis failed"
            
            if [ -f projects/current-session/metadata/orchestrator_analysis.json ]; then
              echo "match_found=true" >> $GITHUB_OUTPUT
              # Extract pattern
              PATTERN=$(jq -r '.execution_pattern // "sequential"' projects/current-session/metadata/orchestrator_analysis.json)
              echo "pattern=$PATTERN" >> $GITHUB_OUTPUT
            else
              echo "match_found=false" >> $GITHUB_OUTPUT
              echo "pattern=sequential" >> $GITHUB_OUTPUT
            fi
          fi
          
      - name: Generate Mermaid Diagram
        run: |
          echo "üìä Generating execution flow diagram..."
          
          # Create metadata directory
          mkdir -p metadata
          
          CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
          
          # Generate basic Mermaid diagram
          echo "graph TD" > metadata/mermaid.txt
          echo "    Start[ÈñãÂßã]" >> metadata/mermaid.txt
          
          IFS=',' read -ra CAP_ARRAY <<< "$CAPABILITIES"
          PREV_NODE="Start"
          NODE_COUNT=1
          
          for cap in "${CAP_ARRAY[@]}"; do
            if [ -n "$cap" ]; then
              NODE_NAME="Node$NODE_COUNT"
              
              case "$cap" in
                "web-search") LABEL="üîç WebÊ§úÁ¥¢" ;;
                "planning") LABEL="üìã ‰ºÅÁîª„ÉªË®àÁîª" ;;
                "content-planning") LABEL="üìù „Ç≥„É≥„ÉÜ„É≥„ÉÑ‰ºÅÁîª" ;;
                "narration-creation") LABEL="üé§ „Éä„É¨„Éº„Ç∑„Éß„É≥‰ΩúÊàê" ;;
                "scene-composition") LABEL="üé® „Ç∑„Éº„É≥ÊßãÊàê" ;;
                "image-generation") LABEL="üñºÔ∏è ÁîªÂÉèÁîüÊàê" ;;
                "text-to-speech") LABEL="üó£Ô∏è Èü≥Â£∞ÂêàÊàê" ;;
                "video-generation") LABEL="üé¨ ÂãïÁîªÁîüÊàê" ;;
                "lipsync") LABEL="üëÑ „É™„ÉÉ„Éó„Ç∑„É≥„ÇØ" ;;
                "subtitle-overlay") LABEL="üìù Â≠óÂπïËøΩÂä†" ;;
                "newsapi") LABEL="üì∞ „Éã„É•„Éº„ÇπAPI" ;;
                "weather") LABEL="üå§Ô∏è Â§©Ê∞óAPI" ;;
                "youtube-upload") LABEL="üìπ YouTube„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ" ;;
                "openai-summarize") LABEL="üìÑ AIË¶ÅÁ¥Ñ" ;;
                "openai-translate") LABEL="üåê AIÁøªË®≥" ;;
                "openai-gpt") LABEL="ü§ñ ChatGPT" ;;
                "openai-image-gen") LABEL="üé® DALL-EÁîªÂÉèÁîüÊàê" ;;
                "elevenlabs-tts") LABEL="üéôÔ∏è ElevenLabsÈü≥Â£∞" ;;
                "huggingface-inference") LABEL="ü§ó HuggingFaceÊé®Ë´ñ" ;;
                "slack-notify") LABEL="üí¨ SlackÈÄöÁü•" ;;
                "slack-file-upload") LABEL="üìé Slack„Éï„Ç°„Ç§„É´" ;;
                "discord-webhook") LABEL="üéÆ DiscordÈÄöÁü•" ;;
                "telegram-send-message") LABEL="‚úàÔ∏è TelegramÈÄÅ‰ø°" ;;
                "sendgrid-send-email") LABEL="üìß „É°„Éº„É´ÈÄÅ‰ø°" ;;
                "twitter-post") LABEL="üê¶ TwitterÊäïÁ®ø" ;;
                "twitter-search") LABEL="üîç TwitterÊ§úÁ¥¢" ;;
                "reddit-search") LABEL="üîç RedditÊ§úÁ¥¢" ;;
                "google-sheets-read") LABEL="üìä SheetsË™≠Ëæº" ;;
                "google-sheets-write") LABEL="üìä SheetsÊõ∏Ëæº" ;;
                "finnhub-stock-quote") LABEL="üìà Ê†™‰æ°ÂèñÂæó" ;;
                "github-issue-create") LABEL="üêõ Issue‰ΩúÊàê" ;;
                "github-repo-search") LABEL="üîç „É™„Éù„Ç∏„Éà„É™Ê§úÁ¥¢" ;;
                "github-workflow-dispatch") LABEL="‚öôÔ∏è WorkflowÂÆüË°å" ;;
                "github-release-create") LABEL="üè∑Ô∏è Release‰ΩúÊàê" ;;
                "arxiv-search") LABEL="üìö Ë´ñÊñáÊ§úÁ¥¢" ;;
                "notion-create-page") LABEL="üìÑ Notion„Éö„Éº„Ç∏" ;;
                *) LABEL="$cap" ;;
              esac
              
              echo "    $NODE_NAME[$LABEL]" >> metadata/mermaid.txt
              echo "    $PREV_NODE --> $NODE_NAME" >> metadata/mermaid.txt
              
              PREV_NODE=$NODE_NAME
              NODE_COUNT=$((NODE_COUNT + 1))
            fi
          done
          
          echo "    $PREV_NODE --> End[ÂÆå‰∫Ü]" >> metadata/mermaid.txt
          
      - name: Upload Analysis Results
        uses: actions/upload-artifact@v4
        with:
          name: analysis-data
          path: |
            metadata/
            projects/current-session/metadata/

  # ===========================================
  # PHASE 4: WORKFLOW GENERATION
  # ===========================================
  
  workflow-generation:
    name: "üöÄ Workflow Generation"
    runs-on: ubuntu-latest
    needs: ['validate-trigger', 'basic-task-decomposition', 'detailed-analysis']
    outputs:
      workflow_id: ${{ steps.generate.outputs.workflow_id }}
      total_jobs: ${{ steps.generate.outputs.total_jobs }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Generate Workflow
        id: generate
        run: |
          echo "üöÄ Generating optimized workflow..."
          
          ISSUE_NUMBER=$(cat artifacts/issue-data/issue_number.txt)
          CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
          COMPLEXITY="${{ needs['basic-task-decomposition'].outputs.complexity }}"
          
          # Create workflow directory
          mkdir -p generated-workflows
          WORKFLOW_FILE="generated-workflows/dynamic-workflow-${ISSUE_NUMBER}.yml"
          
          # Generate workflow header
          echo 'name: "üéØ Dynamic Workflow - Issue #${{ github.event.inputs.issue_number }}"' > "$WORKFLOW_FILE"
          echo 'run-name: "üìä Dynamic | ${{ github.actor }} | Issue #${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          echo 'on:' >> "$WORKFLOW_FILE"
          echo '  workflow_dispatch:' >> "$WORKFLOW_FILE"
          echo '    inputs:' >> "$WORKFLOW_FILE"
          echo '      issue_number:' >> "$WORKFLOW_FILE"
          echo '        description: "Source issue number"' >> "$WORKFLOW_FILE"
          echo '        required: true' >> "$WORKFLOW_FILE"
          echo "        default: \"$ISSUE_NUMBER\"" >> "$WORKFLOW_FILE"
          echo '      branch_name:' >> "$WORKFLOW_FILE"
          echo '        description: "Working branch name"' >> "$WORKFLOW_FILE"
          echo '        required: false' >> "$WORKFLOW_FILE"
          echo "        default: \"issue-${ISSUE_NUMBER}\"" >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          
          # Add permissions and environment
          echo 'permissions:' >> "$WORKFLOW_FILE"
          echo '  contents: write' >> "$WORKFLOW_FILE"
          echo '  actions: write' >> "$WORKFLOW_FILE"
          echo '  issues: write' >> "$WORKFLOW_FILE"
          echo '  pull-requests: write' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          echo 'env:' >> "$WORKFLOW_FILE"
          echo '  CLAUDE_CODE_CI_MODE: true' >> "$WORKFLOW_FILE"
          echo '  CLAUDE_CODE_AUTO_APPROVE_MCP: true' >> "$WORKFLOW_FILE"
          echo '  CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}' >> "$WORKFLOW_FILE"
          
          # Add environment variables for external APIs if needed
          if echo "$CAPABILITIES" | grep -q "newsapi"; then
            echo '  NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "weather"; then
            echo '  OPENWEATHERMAP_API_KEY: ${{ secrets.OPENWEATHERMAP_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "youtube"; then
            echo '  YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "openai"; then
            echo '  OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "elevenlabs"; then
            echo '  ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "huggingface"; then
            echo '  HUGGINGFACE_API_KEY: ${{ secrets.HUGGINGFACE_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "slack"; then
            echo '  SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "discord"; then
            echo '  DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "telegram"; then
            echo '  TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "sendgrid"; then
            echo '  SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "twitter"; then
            echo '  TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "reddit"; then
            echo '  REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}' >> "$WORKFLOW_FILE"
            echo '  REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "google-sheets"; then
            echo '  GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "finnhub"; then
            echo '  FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "github-"; then
            echo '  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}' >> "$WORKFLOW_FILE"
          fi
          if echo "$CAPABILITIES" | grep -q "notion"; then
            echo '  NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}' >> "$WORKFLOW_FILE"
          fi
          
          echo '' >> "$WORKFLOW_FILE"
          
          # Add jobs based on capabilities
          echo 'jobs:' >> "$WORKFLOW_FILE"
          
          # Setup job (minimal to avoid expression limits)
          echo '  setup:' >> "$WORKFLOW_FILE"
          echo '    name: "üöÄ Setup"' >> "$WORKFLOW_FILE"
          echo '    runs-on: ubuntu-latest' >> "$WORKFLOW_FILE"
          echo '    outputs:' >> "$WORKFLOW_FILE"
          echo '      project_dir: ${{ steps.setup.outputs.project_dir }}' >> "$WORKFLOW_FILE"
          echo '      timestamp: ${{ steps.setup.outputs.timestamp }}' >> "$WORKFLOW_FILE"
          echo '    steps:' >> "$WORKFLOW_FILE"
          echo '      - name: Checkout Repository' >> "$WORKFLOW_FILE"
          echo '        uses: actions/checkout@v4' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          echo '      - name: Create Project Structure' >> "$WORKFLOW_FILE"
          echo '        id: setup' >> "$WORKFLOW_FILE"
          echo '        run: |' >> "$WORKFLOW_FILE"
          echo '          TIMESTAMP=$(date +%Y%m%d-%H%M%S)' >> "$WORKFLOW_FILE"
          echo '          PROJECT_DIR="projects/issue-${{ github.event.inputs.issue_number }}-$TIMESTAMP"' >> "$WORKFLOW_FILE"
          echo '          mkdir -p "$PROJECT_DIR"/{logs,metadata,temp,final,media}' >> "$WORKFLOW_FILE"
          echo '          echo "project_dir=$PROJECT_DIR" >> $GITHUB_OUTPUT' >> "$WORKFLOW_FILE"
          echo '          echo "timestamp=$TIMESTAMP" >> $GITHUB_OUTPUT' >> "$WORKFLOW_FILE"
          echo '          echo "‚úÖ Project structure created: $PROJECT_DIR"' >> "$WORKFLOW_FILE"
          echo '' >> "$WORKFLOW_FILE"
          
          # Group capabilities and create phases
          IFS=',' read -ra CAP_ARRAY <<< "$CAPABILITIES"
          
          # Capability mapping to minimal units
          declare -A UNIT_MAP
          UNIT_MAP["web-search"]="./minimal-units/planning/web-search.yml"
          UNIT_MAP["data-analysis"]="./minimal-units/planning/data-analysis.yml"
          UNIT_MAP["planning"]="./minimal-units/planning/planning-ccsdk.yml"
          UNIT_MAP["content-planning"]="./minimal-units/planning/content-planning.yml"
          UNIT_MAP["scene-composition"]="./minimal-units/planning/scene-composition.yml"
          UNIT_MAP["image-generation"]="./minimal-units/media/image/t2i-imagen3.yml"
          UNIT_MAP["video-generation"]="./minimal-units/media/video/t2v-veo3.yml"
          UNIT_MAP["audio-generation"]="./minimal-units/media/audio/bgm-generate-mcp.yml"
          UNIT_MAP["lipsync"]="./minimal-units/postprod/lipsync-generation.yml"
          UNIT_MAP["subtitle-overlay"]="./minimal-units/postprod/subtitle-overlay.yml"
          UNIT_MAP["video-editing"]="./minimal-units/postprod/video-concat.yml"
          # External API units - ALL 27
          UNIT_MAP["newsapi"]="./minimal-units/external/newsapi-fetch.yml"
          UNIT_MAP["weather"]="./minimal-units/external/weather-fetch.yml"
          UNIT_MAP["youtube-upload"]="./minimal-units/external/youtube-upload.yml"
          UNIT_MAP["youtube-video-info"]="./minimal-units/external/youtube-video-info.yml"
          UNIT_MAP["openai-gpt"]="./minimal-units/external/openai-gpt.yml"
          UNIT_MAP["openai-summarize"]="./minimal-units/external/openai-summarize.yml"
          UNIT_MAP["openai-translate"]="./minimal-units/external/openai-translate.yml"
          UNIT_MAP["openai-image-gen"]="./minimal-units/external/openai-image-gen.yml"
          UNIT_MAP["elevenlabs-tts"]="./minimal-units/external/elevenlabs-tts.yml"
          UNIT_MAP["huggingface-inference"]="./minimal-units/external/huggingface-inference.yml"
          UNIT_MAP["slack-notify"]="./minimal-units/external/slack-notify.yml"
          UNIT_MAP["slack-file-upload"]="./minimal-units/external/slack-file-upload.yml"
          UNIT_MAP["discord-webhook"]="./minimal-units/external/discord-webhook.yml"
          UNIT_MAP["telegram-send-message"]="./minimal-units/external/telegram-send-message.yml"
          UNIT_MAP["sendgrid-send-email"]="./minimal-units/external/sendgrid-send-email.yml"
          UNIT_MAP["twitter-post"]="./minimal-units/external/twitter-post.yml"
          UNIT_MAP["twitter-search"]="./minimal-units/external/twitter-search.yml"
          UNIT_MAP["reddit-search"]="./minimal-units/external/reddit-search.yml"
          UNIT_MAP["google-sheets-read"]="./minimal-units/external/google-sheets-read.yml"
          UNIT_MAP["google-sheets-write"]="./minimal-units/external/google-sheets-write.yml"
          UNIT_MAP["finnhub-stock-quote"]="./minimal-units/external/finnhub-stock-quote.yml"
          UNIT_MAP["github-issue-create"]="./minimal-units/external/github-issue-create.yml"
          UNIT_MAP["github-repo-search"]="./minimal-units/external/github-repo-search.yml"
          UNIT_MAP["github-workflow-dispatch"]="./minimal-units/external/github-workflow-dispatch.yml"
          UNIT_MAP["github-release-create"]="./minimal-units/external/github-release-create.yml"
          UNIT_MAP["arxiv-search"]="./minimal-units/external/arxiv-search.yml"
          UNIT_MAP["notion-create-page"]="./minimal-units/external/notion-create-page.yml"
          
          # Phase separation
          RESEARCH_JOBS=""
          PLANNING_JOBS=""
          MEDIA_JOBS=""
          POST_JOBS=""
          EXTERNAL_JOBS=""
          
          for cap in "${CAP_ARRAY[@]}"; do
            if [ -n "$cap" ]; then
              case "$cap" in
                web-search|data-analysis)
                  RESEARCH_JOBS="${RESEARCH_JOBS}${cap},"
                  ;;
                planning|scene-composition|content-planning|narration-creation)
                  PLANNING_JOBS="${PLANNING_JOBS}${cap},"
                  ;;
                image-generation|video-generation|audio-generation|text-to-speech)
                  MEDIA_JOBS="${MEDIA_JOBS}${cap},"
                  ;;
                lipsync|subtitle-overlay|video-editing|video-concat)
                  POST_JOBS="${POST_JOBS}${cap},"
                  ;;
                newsapi|weather|youtube-upload|youtube-video-info|openai-gpt|openai-summarize|openai-translate|openai-image-gen|elevenlabs-tts|huggingface-inference|slack-notify|slack-file-upload|discord-webhook|telegram-send-message|sendgrid-send-email|twitter-post|twitter-search|reddit-search|google-sheets-read|google-sheets-write|finnhub-stock-quote|github-issue-create|github-repo-search|github-workflow-dispatch|github-release-create|arxiv-search|notion-create-page)
                  EXTERNAL_JOBS="${EXTERNAL_JOBS}${cap},"
                  ;;
              esac
            fi
          done
          
          # Research phase (sequential)
          if [ -n "$RESEARCH_JOBS" ]; then
            IFS=',' read -ra RESEARCH_ARRAY <<< "$RESEARCH_JOBS"
            JOB_INDEX=1
            for research_job in "${RESEARCH_ARRAY[@]}"; do
              if [ -n "$research_job" ]; then
                JOB_NAME="research_${JOB_INDEX}"
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"üîç Research: ${research_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$research_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$research_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    uses: ./minimal-units/planning/web-search.yml" >> "$WORKFLOW_FILE"
                fi
                if [ $JOB_INDEX -eq 1 ]; then
                  echo '    needs: setup' >> "$WORKFLOW_FILE"
                else
                  PREV_JOB="research_$((JOB_INDEX - 1))"
                  echo "    needs: [setup, ${PREV_JOB}]" >> "$WORKFLOW_FILE"
                fi
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      query: "${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/metadata"' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
                JOB_INDEX=$((JOB_INDEX + 1))
              fi
            done
          fi
          
          # Planning phase (sequential with dependency)
          if [ -n "$PLANNING_JOBS" ]; then
            IFS=',' read -ra PLANNING_ARRAY <<< "$PLANNING_JOBS"
            JOB_INDEX=1
            PREV_PHASE_JOB=""
            if [ -n "$RESEARCH_JOBS" ]; then
              RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              PREV_PHASE_JOB="research_${RESEARCH_COUNT}"
            fi
            
            for planning_job in "${PLANNING_ARRAY[@]}"; do
              if [ -n "$planning_job" ]; then
                JOB_NAME="planning_${JOB_INDEX}"
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"üìã Planning: ${planning_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$planning_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$planning_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    uses: ./minimal-units/planning/planning-ccsdk.yml" >> "$WORKFLOW_FILE"
                fi
                
                # Dependencies
                if [ $JOB_INDEX -eq 1 ]; then
                  if [ -n "$PREV_PHASE_JOB" ]; then
                    echo "    needs: [setup, ${PREV_PHASE_JOB}]" >> "$WORKFLOW_FILE"
                  else
                    echo '    needs: setup' >> "$WORKFLOW_FILE"
                  fi
                else
                  PREV_JOB="planning_$((JOB_INDEX - 1))"
                  echo "    needs: ${PREV_JOB}" >> "$WORKFLOW_FILE"
                fi
                
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      concept: "Issue #${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/metadata"' >> "$WORKFLOW_FILE"
                echo '    secrets:' >> "$WORKFLOW_FILE"
                echo '      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
                JOB_INDEX=$((JOB_INDEX + 1))
              fi
            done
          fi
          
          # Media generation phase (parallel execution)
          if [ -n "$MEDIA_JOBS" ]; then
            # Determine dependencies for media jobs
            MEDIA_DEPS="setup"
            if [ -n "$PLANNING_JOBS" ]; then
              PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              MEDIA_DEPS="planning_${PLANNING_COUNT}"
            elif [ -n "$RESEARCH_JOBS" ]; then
              RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              MEDIA_DEPS="research_${RESEARCH_COUNT}"
            fi
            
            IFS=',' read -ra MEDIA_ARRAY <<< "$MEDIA_JOBS"
            for media_job in "${MEDIA_ARRAY[@]}"; do
              if [ -n "$media_job" ]; then
                JOB_NAME=$(echo "$media_job" | tr '-' '_')
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"üé® ${media_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$media_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$media_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    runs-on: ubuntu-latest" >> "$WORKFLOW_FILE"
                fi
                echo "    needs: ${MEDIA_DEPS}" >> "$WORKFLOW_FILE"
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      prompt: "Generated content for issue #${{ github.event.inputs.issue_number }}"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/media"' >> "$WORKFLOW_FILE"
                echo '    secrets:' >> "$WORKFLOW_FILE"
                echo '      CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
              fi
            done
          fi
          
          # Post-processing phase (sequential)
          if [ -n "$POST_JOBS" ]; then
            # Collect all media job names for dependencies
            MEDIA_JOB_NAMES=""
            if [ -n "$MEDIA_JOBS" ]; then
              IFS=',' read -ra MEDIA_ARRAY <<< "$MEDIA_JOBS"
              for media_job in "${MEDIA_ARRAY[@]}"; do
                if [ -n "$media_job" ]; then
                  JOB_NAME=$(echo "$media_job" | tr '-' '_')
                  MEDIA_JOB_NAMES="${MEDIA_JOB_NAMES}${JOB_NAME}, "
                fi
              done
              MEDIA_JOB_NAMES="${MEDIA_JOB_NAMES%, }"
            fi
            
            IFS=',' read -ra POST_ARRAY <<< "$POST_JOBS"
            JOB_INDEX=1
            for post_job in "${POST_ARRAY[@]}"; do
              if [ -n "$post_job" ]; then
                JOB_NAME="post_${JOB_INDEX}"
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"üîß Post: ${post_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$post_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$post_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    runs-on: ubuntu-latest" >> "$WORKFLOW_FILE"
                fi
                
                # Dependencies
                if [ $JOB_INDEX -eq 1 ]; then
                  if [ -n "$MEDIA_JOB_NAMES" ]; then
                    echo "    needs: [${MEDIA_JOB_NAMES}]" >> "$WORKFLOW_FILE"
                  else
                    echo '    needs: setup' >> "$WORKFLOW_FILE"
                  fi
                else
                  PREV_JOB="post_$((JOB_INDEX - 1))"
                  echo "    needs: ${PREV_JOB}" >> "$WORKFLOW_FILE"
                fi
                
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      video_list: "placeholder"' >> "$WORKFLOW_FILE"
                echo '      output_dir: "${{ needs.setup.outputs.project_dir }}/final"' >> "$WORKFLOW_FILE"
                echo '' >> "$WORKFLOW_FILE"
                JOB_INDEX=$((JOB_INDEX + 1))
              fi
            done
          fi
          
          # External API phase (mixed parallel/sequential)
          if [ -n "$EXTERNAL_JOBS" ]; then
            # Determine dependencies for external API jobs
            EXTERNAL_DEPS="setup"
            if [ -n "$POST_JOBS" ]; then
              POST_COUNT=$(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              EXTERNAL_DEPS="post_${POST_COUNT}"
            elif [ -n "$MEDIA_JOBS" ]; then
              EXTERNAL_DEPS="${MEDIA_JOB_NAMES}"
            elif [ -n "$PLANNING_JOBS" ]; then
              PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              EXTERNAL_DEPS="planning_${PLANNING_COUNT}"
            elif [ -n "$RESEARCH_JOBS" ]; then
              RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
              EXTERNAL_DEPS="research_${RESEARCH_COUNT}"
            fi
            
            IFS=',' read -ra EXTERNAL_ARRAY <<< "$EXTERNAL_JOBS"
            for external_job in "${EXTERNAL_ARRAY[@]}"; do
              if [ -n "$external_job" ]; then
                JOB_NAME=$(echo "$external_job" | tr '-' '_')
                echo "  ${JOB_NAME}:" >> "$WORKFLOW_FILE"
                echo "    name: \"üåê External API: ${external_job}\"" >> "$WORKFLOW_FILE"
                if [ -f "${UNIT_MAP[$external_job]}" ]; then
                  echo "    uses: ${UNIT_MAP[$external_job]}" >> "$WORKFLOW_FILE"
                else
                  echo "    runs-on: ubuntu-latest" >> "$WORKFLOW_FILE"
                fi
                
                # Dependencies based on type
                case "$external_job" in
                  # Input APIs - can run early/parallel
                  newsapi|weather|youtube-video-info|reddit-search|arxiv-search|github-repo-search|finnhub-stock-quote|google-sheets-read)
                    echo "    needs: setup" >> "$WORKFLOW_FILE"
                    ;;
                  
                  # Processing APIs - need content first
                  openai-gpt|openai-summarize|openai-translate|openai-image-gen|huggingface-inference)
                    if [ -n "$PLANNING_JOBS" ]; then
                      PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
                      echo "    needs: planning_${PLANNING_COUNT}" >> "$WORKFLOW_FILE"
                    else
                      echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    fi
                    ;;
                  
                  # Audio generation - after planning
                  elevenlabs-tts)
                    if [ -n "$PLANNING_JOBS" ]; then
                      PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
                      echo "    needs: planning_${PLANNING_COUNT}" >> "$WORKFLOW_FILE"
                    else
                      echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    fi
                    ;;
                  
                  # Output APIs - need final content
                  youtube-upload|slack-notify|slack-file-upload|discord-webhook|telegram-send-message|sendgrid-send-email|twitter-post|google-sheets-write|github-issue-create|github-workflow-dispatch|github-release-create|notion-create-page)
                    if [ -n "$POST_JOBS" ]; then
                      POST_COUNT=$(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
                      echo "    needs: post_${POST_COUNT}" >> "$WORKFLOW_FILE"
                    elif [ -n "$MEDIA_JOBS" ]; then
                      echo "    needs: [${MEDIA_JOB_NAMES}]" >> "$WORKFLOW_FILE"
                    else
                      echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    fi
                    ;;
                  
                  # Search APIs - can run early
                  twitter-search)
                    echo "    needs: setup" >> "$WORKFLOW_FILE"
                    ;;
                  
                  *)
                    echo "    needs: ${EXTERNAL_DEPS}" >> "$WORKFLOW_FILE"
                    ;;
                esac
                
                echo '    with:' >> "$WORKFLOW_FILE"
                echo '      project_dir: "${{ needs.setup.outputs.project_dir }}"' >> "$WORKFLOW_FILE"
                echo '    secrets:' >> "$WORKFLOW_FILE"
                
                # Add required secrets based on API type
                case "$external_job" in
                  newsapi)
                    echo '      NEWSAPI_KEY: ${{ secrets.NEWSAPI_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  weather)
                    echo '      OPENWEATHERMAP_API_KEY: ${{ secrets.OPENWEATHERMAP_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  youtube-upload|youtube-video-info)
                    echo '      YOUTUBE_API_KEY: ${{ secrets.YOUTUBE_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  openai-*)
                    echo '      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  elevenlabs-tts)
                    echo '      ELEVENLABS_API_KEY: ${{ secrets.ELEVENLABS_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  huggingface-inference)
                    echo '      HUGGINGFACE_API_KEY: ${{ secrets.HUGGINGFACE_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  slack-notify|slack-file-upload)
                    echo '      SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
                    ;;
                  discord-webhook)
                    echo '      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}' >> "$WORKFLOW_FILE"
                    ;;
                  telegram-send-message)
                    echo '      TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}' >> "$WORKFLOW_FILE"
                    ;;
                  sendgrid-send-email)
                    echo '      SENDGRID_API_KEY: ${{ secrets.SENDGRID_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  twitter-post|twitter-search)
                    echo '      TWITTER_API_KEY: ${{ secrets.TWITTER_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  reddit-search)
                    echo '      REDDIT_CLIENT_ID: ${{ secrets.REDDIT_CLIENT_ID }}' >> "$WORKFLOW_FILE"
                    echo '      REDDIT_CLIENT_SECRET: ${{ secrets.REDDIT_CLIENT_SECRET }}' >> "$WORKFLOW_FILE"
                    ;;
                  google-sheets-read|google-sheets-write)
                    echo '      GOOGLE_SHEETS_CREDENTIALS: ${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}' >> "$WORKFLOW_FILE"
                    ;;
                  finnhub-stock-quote)
                    echo '      FINNHUB_API_KEY: ${{ secrets.FINNHUB_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                  github-*)
                    echo '      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}' >> "$WORKFLOW_FILE"
                    ;;
                  arxiv-search)
                    # arXiv API doesn't require authentication
                    ;;
                  notion-create-page)
                    echo '      NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}' >> "$WORKFLOW_FILE"
                    ;;
                esac
                
                echo '' >> "$WORKFLOW_FILE"
              fi
            done
          fi
          
          # Summary job
          echo '  summary:' >> "$WORKFLOW_FILE"
          echo '    name: "üìä Summary"' >> "$WORKFLOW_FILE"
          echo '    runs-on: ubuntu-latest' >> "$WORKFLOW_FILE"
          echo '    if: always()' >> "$WORKFLOW_FILE"
          
          # Build needs list for summary
          ALL_JOBS="setup"
          if [ -n "$RESEARCH_JOBS" ]; then
            RESEARCH_COUNT=$(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
            for i in $(seq 1 $RESEARCH_COUNT); do
              ALL_JOBS="${ALL_JOBS}, research_${i}"
            done
          fi
          if [ -n "$PLANNING_JOBS" ]; then
            PLANNING_COUNT=$(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
            for i in $(seq 1 $PLANNING_COUNT); do
              ALL_JOBS="${ALL_JOBS}, planning_${i}"
            done
          fi
          if [ -n "$MEDIA_JOBS" ]; then
            IFS=',' read -ra MEDIA_ARRAY <<< "$MEDIA_JOBS"
            for media_job in "${MEDIA_ARRAY[@]}"; do
              if [ -n "$media_job" ]; then
                JOB_NAME=$(echo "$media_job" | tr '-' '_')
                ALL_JOBS="${ALL_JOBS}, ${JOB_NAME}"
              fi
            done
          fi
          if [ -n "$POST_JOBS" ]; then
            POST_COUNT=$(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)
            for i in $(seq 1 $POST_COUNT); do
              ALL_JOBS="${ALL_JOBS}, post_${i}"
            done
          fi
          if [ -n "$EXTERNAL_JOBS" ]; then
            IFS=',' read -ra EXTERNAL_ARRAY <<< "$EXTERNAL_JOBS"
            for external_job in "${EXTERNAL_ARRAY[@]}"; do
              if [ -n "$external_job" ]; then
                JOB_NAME=$(echo "$external_job" | tr '-' '_')
                ALL_JOBS="${ALL_JOBS}, ${JOB_NAME}"
              fi
            done
          fi
          
          echo "    needs: [${ALL_JOBS}]" >> "$WORKFLOW_FILE"
          echo '    steps:' >> "$WORKFLOW_FILE"
          echo '      - name: Generate Summary' >> "$WORKFLOW_FILE"
          echo '        run: |' >> "$WORKFLOW_FILE"
          echo '          echo "# üéØ Workflow Execution Summary" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          echo '          echo "- **Issue**: #${{ github.event.inputs.issue_number }}" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          echo '          echo "- **Status**: Completed" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          echo '          echo "- **Project**: ${{ needs.setup.outputs.project_dir }}" >> $GITHUB_STEP_SUMMARY' >> "$WORKFLOW_FILE"
          
          # Count total jobs
          JOB_COUNT=2  # setup + summary
          [ -n "$RESEARCH_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$RESEARCH_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$PLANNING_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$PLANNING_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$MEDIA_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$MEDIA_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$POST_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$POST_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          [ -n "$EXTERNAL_JOBS" ] && JOB_COUNT=$((JOB_COUNT + $(echo "$EXTERNAL_JOBS" | tr ',' '\n' | grep -v '^$' | wc -l)))
          
          echo "workflow_id=${ISSUE_NUMBER}-$(date +%Y%m%d-%H%M%S)" >> $GITHUB_OUTPUT
          echo "total_jobs=$JOB_COUNT" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Generated workflow with $JOB_COUNT jobs"
          
      - name: Upload Generated Workflow
        uses: actions/upload-artifact@v4
        with:
          name: generated-workflow
          path: generated-workflows/

  # ===========================================
  # PHASE 5: WORKFLOW VALIDATION
  # ===========================================
  
  workflow-validation:
    name: "‚úÖ Workflow Validation"
    runs-on: ubuntu-latest
    needs: ['workflow-generation']
    outputs:
      validation_passed: ${{ steps.validate.outputs.passed }}
      optimization_needed: ${{ steps.validate.outputs.needs_optimization }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Generated Workflow
        uses: actions/download-artifact@v4
        with:
          name: generated-workflow
          path: generated-workflows/
          
      - name: Validate Workflow
        id: validate
        run: |
          echo "‚úÖ Validating generated workflow..."
          
          WORKFLOW_FILE=$(find generated-workflows -name '*.yml' | head -1)
          
          # Check YAML syntax
          python3 -c "import yaml; yaml.safe_load(open('$WORKFLOW_FILE'))" 2>/dev/null
          if [ $? -eq 0 ]; then
            echo "‚úÖ YAML syntax valid"
            SYNTAX_OK=true
          else
            echo "‚ùå YAML syntax error"
            SYNTAX_OK=false
          fi
          
          # Check for long expressions
          LONG_LINES=$(grep -n '\${{' "$WORKFLOW_FILE" | grep '}}' | awk 'length($0) > 500' | wc -l)
          if [ "$LONG_LINES" -gt 0 ]; then
            echo "‚ö†Ô∏è Found $LONG_LINES long expression lines"
            echo "needs_optimization=true" >> $GITHUB_OUTPUT
          else
            echo "‚úÖ No long expressions found"
            echo "needs_optimization=false" >> $GITHUB_OUTPUT
          fi
          
          # Check job count
          JOB_COUNT=$(grep -c "^  [a-zA-Z][a-zA-Z0-9_-]*:" "$WORKFLOW_FILE" | grep -v "^on:" | grep -v "^jobs:" || echo 0)
          if [ "$JOB_COUNT" -gt 10 ]; then
            echo "‚ö†Ô∏è Too many jobs ($JOB_COUNT), consider parallelization"
            echo "needs_optimization=true" >> $GITHUB_OUTPUT
          fi
          
          if [ "$SYNTAX_OK" = true ]; then
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "passed=false" >> $GITHUB_OUTPUT
          fi
          
      - name: Prepare Optimization Script Part 1
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "üìä Preparing optimization script (Part 1)..."
          
          # Create optimization script header using echo commands
          echo '#!/usr/bin/env python3' > optimize_part1.py
          echo 'import yaml' >> optimize_part1.py
          echo 'import sys' >> optimize_part1.py
          echo 'import json' >> optimize_part1.py
          echo 'from typing import List, Dict, Any' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def load_workflow(file_path: str) -> Dict:' >> optimize_part1.py
          echo '    """„ÉØ„Éº„ÇØ„Éï„É≠„Éº„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„ÇÄ"""' >> optimize_part1.py
          echo '    with open(file_path, "r", encoding="utf-8") as f:' >> optimize_part1.py
          echo '        return yaml.safe_load(f)' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def analyze_node_length(job_content: Dict) -> int:' >> optimize_part1.py
          echo '    """„Éé„Éº„Éâ„ÅÆÈï∑„Åï„ÇíÂàÜÊûê"""' >> optimize_part1.py
          echo '    content_str = yaml.dump(job_content)' >> optimize_part1.py
          echo '    return len(content_str)' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def identify_long_nodes(workflow: Dict, threshold: int = 15000) -> List[str]:' >> optimize_part1.py
          echo '    """Èï∑„ÅÑ„Éé„Éº„Éâ„ÇíÁâπÂÆö"""' >> optimize_part1.py
          echo '    long_nodes = []' >> optimize_part1.py
          echo '    if "jobs" in workflow:' >> optimize_part1.py
          echo '        for job_name, job_content in workflow["jobs"].items():' >> optimize_part1.py
          echo '            length = analyze_node_length(job_content)' >> optimize_part1.py
          echo '            if length > threshold:' >> optimize_part1.py
          echo '                long_nodes.append(job_name)' >> optimize_part1.py
          echo '                print(f"‚ö†Ô∏è Long node detected: {job_name} ({length} chars)")' >> optimize_part1.py
          echo '    return long_nodes' >> optimize_part1.py
          
      - name: Prepare Optimization Script Part 2
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "üìä Preparing optimization script (Part 2)..."
          
          # Add node splitting logic using echo commands
          echo '' >> optimize_part1.py
          echo 'def split_long_node(job_name: str, job_content: Dict) -> Dict[str, Dict]:' >> optimize_part1.py
          echo '    """Èï∑„ÅÑ„Éé„Éº„Éâ„ÇíÂàÜÂâ≤"""' >> optimize_part1.py
          echo '    split_jobs = {}' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    if "steps" in job_content and len(job_content["steps"]) > 10:' >> optimize_part1.py
          echo '        # „Çπ„ÉÜ„ÉÉ„ÉóÊï∞„ÅåÂ§ö„ÅÑÂ†¥Âêà„ÅØÂàÜÂâ≤' >> optimize_part1.py
          echo '        steps = job_content["steps"]' >> optimize_part1.py
          echo '        mid_point = len(steps) // 2' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        # Part 1' >> optimize_part1.py
          echo '        job1 = job_content.copy()' >> optimize_part1.py
          echo '        job1["steps"] = steps[:mid_point]' >> optimize_part1.py
          echo '        split_jobs[f"{job_name}_part1"] = job1' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        # Part 2' >> optimize_part1.py
          echo '        job2 = job_content.copy()' >> optimize_part1.py
          echo '        job2["steps"] = steps[mid_point:]' >> optimize_part1.py
          echo '        if "needs" in job1:' >> optimize_part1.py
          echo '            job2["needs"] = [f"{job_name}_part1"]' >> optimize_part1.py
          echo '        split_jobs[f"{job_name}_part2"] = job2' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        print(f"‚úÖ Split {job_name} into 2 parts")' >> optimize_part1.py
          echo '    else:' >> optimize_part1.py
          echo '        split_jobs[job_name] = job_content' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    return split_jobs' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo 'def identify_parallel_opportunities(workflow: Dict) -> List[List[str]]:' >> optimize_part1.py
          echo '    """‰∏¶ÂàóÂÆüË°åÂèØËÉΩ„Å™„Ç∏„Éß„Éñ„ÇíÁâπÂÆö"""' >> optimize_part1.py
          echo '    parallel_groups = []' >> optimize_part1.py
          echo '    jobs = workflow.get("jobs", {})' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # ‰æùÂ≠òÈñ¢‰øÇ„ÅÆ„Å™„ÅÑ„Ç∏„Éß„Éñ„Çí„Ç∞„É´„Éº„ÉóÂåñ' >> optimize_part1.py
          echo '    no_deps = []' >> optimize_part1.py
          echo '    for job_name, job_content in jobs.items():' >> optimize_part1.py
          echo '        if "needs" not in job_content or not job_content["needs"]:' >> optimize_part1.py
          echo '            no_deps.append(job_name)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    if len(no_deps) > 1:' >> optimize_part1.py
          echo '        parallel_groups.append(no_deps)' >> optimize_part1.py
          echo '        print(f"üöÄ Found parallel group: {no_deps}")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    return parallel_groups' >> optimize_part1.py

      - name: Prepare Optimization Script Part 3
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "üìä Preparing optimization script (Part 3)..."
          
          # Add main optimization logic using echo commands
          echo '' >> optimize_part1.py
          echo 'def optimize_workflow(workflow: Dict) -> Dict:' >> optimize_part1.py
          echo '    """„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÇíÊúÄÈÅ©Âåñ"""' >> optimize_part1.py
          echo '    optimized = workflow.copy()' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # 1. Èï∑„ÅÑ„Éé„Éº„Éâ„ÇíÁâπÂÆö„Åó„Å¶ÂàÜÂâ≤' >> optimize_part1.py
          echo '    long_nodes = identify_long_nodes(workflow)' >> optimize_part1.py
          echo '    if long_nodes:' >> optimize_part1.py
          echo '        print(f"\\nüîç Found {len(long_nodes)} long nodes to split")' >> optimize_part1.py
          echo '        new_jobs = {}' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        for job_name, job_content in workflow.get("jobs", {}).items():' >> optimize_part1.py
          echo '            if job_name in long_nodes:' >> optimize_part1.py
          echo '                split_jobs = split_long_node(job_name, job_content)' >> optimize_part1.py
          echo '                new_jobs.update(split_jobs)' >> optimize_part1.py
          echo '            else:' >> optimize_part1.py
          echo '                new_jobs[job_name] = job_content' >> optimize_part1.py
          echo '        ' >> optimize_part1.py
          echo '        optimized["jobs"] = new_jobs' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # 2. ‰∏¶ÂàóÂÆüË°å„ÅÆÊúÄÈÅ©Âåñ' >> optimize_part1.py
          echo '    parallel_groups = identify_parallel_opportunities(optimized)' >> optimize_part1.py
          echo '    if parallel_groups:' >> optimize_part1.py
          echo '        print(f"\\nüöÄ Identified {len(parallel_groups)} parallel execution opportunities")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    return optimized' >> optimize_part1.py
          echo '' >> optimize_part1.py
          echo '# „É°„Ç§„É≥Âá¶ÁêÜ' >> optimize_part1.py
          echo 'if __name__ == "__main__":' >> optimize_part1.py
          echo '    if len(sys.argv) < 2:' >> optimize_part1.py
          echo '        print("Usage: python optimize.py <workflow.yml>")' >> optimize_part1.py
          echo '        sys.exit(1)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    workflow_file = sys.argv[1]' >> optimize_part1.py
          echo '    print(f"üîß Optimizing workflow: {workflow_file}")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # „ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÇíË™≠„ÅøËæº„Åø' >> optimize_part1.py
          echo '    workflow = load_workflow(workflow_file)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # ÊúÄÈÅ©Âåñ' >> optimize_part1.py
          echo '    optimized = optimize_workflow(workflow)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # ÁµêÊûú„Çí‰øùÂ≠ò' >> optimize_part1.py
          echo '    output_file = workflow_file.replace(".yml", "_optimized.yml")' >> optimize_part1.py
          echo '    with open(output_file, "w", encoding="utf-8") as f:' >> optimize_part1.py
          echo '        yaml.dump(optimized, f, default_flow_style=False, sort_keys=False)' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    print(f"‚úÖ Optimized workflow saved to: {output_file}")' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    # „É°„Çø„Éá„Éº„Çø„ÇíÂá∫Âäõ' >> optimize_part1.py
          echo '    metadata = {' >> optimize_part1.py
          echo '        "original_jobs": len(workflow.get("jobs", {})),' >> optimize_part1.py
          echo '        "optimized_jobs": len(optimized.get("jobs", {})),' >> optimize_part1.py
          echo '        "long_nodes_split": len(identify_long_nodes(workflow)),' >> optimize_part1.py
          echo '        "parallel_groups": len(identify_parallel_opportunities(optimized))' >> optimize_part1.py
          echo '    }' >> optimize_part1.py
          echo '    ' >> optimize_part1.py
          echo '    with open("optimization_metadata.json", "w") as f:' >> optimize_part1.py
          echo '        json.dump(metadata, f, indent=2)' >> optimize_part1.py

      - name: Execute Optimization
        if: ${{ steps.validate.outputs.needs_optimization == 'true' }}
        run: |
          echo "üöÄ Executing optimization..."
          
          # Combine script parts
          mv optimize_part1.py optimize.py
          
          # Find and optimize workflow
          WORKFLOW_FILE=$(find generated-workflows -name '*.yml' | head -1)
          
          if [ -f "$WORKFLOW_FILE" ]; then
            python optimize.py "$WORKFLOW_FILE"
            
            # Check if optimization was successful
            OPTIMIZED_FILE="${WORKFLOW_FILE%.yml}_optimized.yml"
            if [ -f "$OPTIMIZED_FILE" ]; then
              # Replace original with optimized version
              mv "$OPTIMIZED_FILE" "$WORKFLOW_FILE"
              echo "‚úÖ Workflow optimized successfully"
              
              # Read metadata
              if [ -f "optimization_metadata.json" ]; then
                cat optimization_metadata.json
              fi
            else
              echo "‚ö†Ô∏è Optimization did not produce output"
            fi
          else
            echo "‚ö†Ô∏è No workflow file found to optimize"
          fi
          
      - name: Upload Validated Workflow
        uses: actions/upload-artifact@v4
        with:
          name: validated-workflow
          path: generated-workflows/

  # ===========================================
  # PHASE 6: DEPLOYMENT
  # ===========================================
  
  deployment:
    name: "üì¶ Deployment"
    runs-on: ubuntu-latest
    needs: ['workflow-validation', 'workflow-generation']
    if: ${{ needs['workflow-validation'].outputs.validation_passed == 'true' }}
    outputs:
      deployed_path: ${{ steps.deploy.outputs.path }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Validated Workflow
        uses: actions/download-artifact@v4
        with:
          name: validated-workflow
          path: generated-workflows/
          
      - name: Deploy Workflow
        id: deploy
        run: |
          echo "üì¶ Deploying workflow..."
          
          WORKFLOW_FILE=$(find generated-workflows -name '*.yml' | head -1)
          DEPLOYMENT_NAME="issue-${{ inputs.issue_number }}-$(date +%Y%m%d-%H%M%S).yml.disabled"
          DEPLOYMENT_PATH=".github/workflows/generated/$DEPLOYMENT_NAME"
          
          # Create deployment directory
          mkdir -p .github/workflows/generated
          
          # Copy workflow
          cp "$WORKFLOW_FILE" "$DEPLOYMENT_PATH"
          
          echo "path=$DEPLOYMENT_PATH" >> $GITHUB_OUTPUT
          
          # Commit and push
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add "$DEPLOYMENT_PATH"
          git commit -m "ü§ñ Generated workflow for Issue #${{ inputs.issue_number }}" || echo "No changes"
          git push origin main || echo "Push failed"
          
          echo "‚úÖ Deployed to: $DEPLOYMENT_PATH"

  # ===========================================
  # PHASE 7: EXECUTION SUMMARY
  # ===========================================
  
  execution-summary:
    name: "üìä Execution Summary"
    runs-on: ubuntu-latest
    needs: ['validate-trigger', 'basic-task-decomposition', 'detailed-analysis', 'workflow-generation', 'workflow-validation', 'deployment']
    if: ${{ always() }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Generate Summary
        env:
          ISSUE_NUMBER: ${{ inputs.issue_number }}
          ISSUE_TITLE: ${{ needs['validate-trigger'].outputs.issue_title }}
          REQUEST_TYPE: ${{ needs['validate-trigger'].outputs.request_type }}
          CAPABILITIES_COUNT: ${{ needs['basic-task-decomposition'].outputs.capabilities_count }}
          COMPLEXITY: ${{ needs['basic-task-decomposition'].outputs.complexity }}
          HAS_VIDEO: ${{ needs['basic-task-decomposition'].outputs.has_video }}
          HAS_NARRATION: ${{ needs['basic-task-decomposition'].outputs.has_narration }}
          ORCHESTRATOR_MATCH: ${{ needs['detailed-analysis'].outputs.orchestrator_match }}
          EXECUTION_PATTERN: ${{ needs['detailed-analysis'].outputs.execution_pattern }}
          TOTAL_JOBS: ${{ needs['workflow-generation'].outputs.total_jobs }}
          VALIDATION_PASSED: ${{ needs['workflow-validation'].outputs.validation_passed }}
          OPTIMIZATION_NEEDED: ${{ needs['workflow-validation'].outputs.optimization_needed }}
          DEPLOYED_PATH: ${{ needs.deployment.outputs.deployed_path }}
        run: |
          echo "# üìä Meta Workflow v10 ÂÆüË°åÁµêÊûú" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## üìã „É™„ÇØ„Ç®„Çπ„ÉàÊÉÖÂ†±" >> $GITHUB_STEP_SUMMARY
          echo "- **Issue**: #$ISSUE_NUMBER" >> $GITHUB_STEP_SUMMARY
          echo "- **„Çø„Ç§„Éà„É´**: $ISSUE_TITLE" >> $GITHUB_STEP_SUMMARY
          echo "- **„Çø„Ç§„Éó**: $REQUEST_TYPE" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## üîç „Çø„Çπ„ÇØÂàÜËß£ÁµêÊûú" >> $GITHUB_STEP_SUMMARY
          echo "### Ê§úÂá∫„Åï„Çå„ÅüËÉΩÂäõ" >> $GITHUB_STEP_SUMMARY
          if [ -f "artifacts/capabilities-data/capabilities.txt" ]; then
            CAPABILITIES=$(cat artifacts/capabilities-data/capabilities.txt)
            IFS=',' read -ra CAP_ARRAY <<< "$CAPABILITIES"
            for cap in "${CAP_ARRAY[@]}"; do
              if [ -n "$cap" ]; then
                echo "- ‚úÖ $cap" >> $GITHUB_STEP_SUMMARY
              fi
            done
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "### „Çø„Çπ„ÇØÂàÜËß£„ÅÆË©≥Á¥∞" >> $GITHUB_STEP_SUMMARY
          echo "- **Ê§úÂá∫ËÉΩÂäõÊï∞**: $CAPABILITIES_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **Ë§áÈõëÂ∫¶**: $COMPLEXITY" >> $GITHUB_STEP_SUMMARY
          
          # Show fine-grained decomposition
          if [ "$HAS_VIDEO" == "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "#### üé¨ ÂãïÁîªÁîüÊàê„Çø„Çπ„ÇØ„ÅÆË©≥Á¥∞ÂàÜËß£:" >> $GITHUB_STEP_SUMMARY
            echo "- üìã „Ç≥„É≥„ÉÜ„É≥„ÉÑ‰ºÅÁîª (content-planning)" >> $GITHUB_STEP_SUMMARY
            if [ "$HAS_NARRATION" == "true" ]; then
              echo "- üé§ „Éä„É¨„Éº„Ç∑„Éß„É≥‰ΩúÊàê (narration-creation)" >> $GITHUB_STEP_SUMMARY
              echo "- üëÑ „É™„ÉÉ„Éó„Ç∑„É≥„ÇØÂá¶ÁêÜ (lipsync)" >> $GITHUB_STEP_SUMMARY
              echo "- üìù Â≠óÂπï„Ç™„Éº„Éê„Éº„É¨„Ç§ (subtitle-overlay)" >> $GITHUB_STEP_SUMMARY
            fi
            echo "- üé® „Ç∑„Éº„É≥ÊßãÊàê (scene-composition)" >> $GITHUB_STEP_SUMMARY
            echo "- üñºÔ∏è ÁîªÂÉèÁîüÊàê (image-generation)" >> $GITHUB_STEP_SUMMARY
            echo "- üé¨ ÂãïÁîªÁîüÊàê (video-generation)" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## üß† „Ç™„Éº„Ç±„Çπ„Éà„É¨„Éº„Çø„ÉºÂàÜÊûê" >> $GITHUB_STEP_SUMMARY
          echo "- **„Éû„ÉÉ„ÉÅ„É≥„Ç∞**: $ORCHESTRATOR_MATCH" >> $GITHUB_STEP_SUMMARY
          echo "- **ÂÆüË°å„Éë„Çø„Éº„É≥**: $EXECUTION_PATTERN" >> $GITHUB_STEP_SUMMARY
          
          # Show Mermaid diagram if exists
          if [ -f "artifacts/analysis-data/metadata/mermaid.txt" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### üìä ÂÆüË°å„Éï„É≠„ÉºÂõ≥" >> $GITHUB_STEP_SUMMARY
            echo '```mermaid' >> $GITHUB_STEP_SUMMARY
            cat artifacts/analysis-data/metadata/mermaid.txt >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## üöÄ „ÉØ„Éº„ÇØ„Éï„É≠„ÉºÁîüÊàê" >> $GITHUB_STEP_SUMMARY
          echo "- **ÁîüÊàê„Ç∏„Éß„ÉñÊï∞**: $TOTAL_JOBS" >> $GITHUB_STEP_SUMMARY
          echo "- **Ê§úË®ºÁµêÊûú**: $VALIDATION_PASSED" >> $GITHUB_STEP_SUMMARY
          echo "- **ÊúÄÈÅ©Âåñ**: $OPTIMIZATION_NEEDED" >> $GITHUB_STEP_SUMMARY
          
          # Show job structure
          if [ -d "artifacts/generated-workflow" ] && [ -n "$(ls -A artifacts/generated-workflow/*.yml 2>/dev/null)" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### üìã ÁîüÊàê„Åï„Çå„Åü„Ç∏„Éß„ÉñÊßãÈÄ†" >> $GITHUB_STEP_SUMMARY
            WORKFLOW_FILE=$(find artifacts/generated-workflow -name '*.yml' | head -1)
            # Extract job names
            grep -E "^  [a-zA-Z_-]+:" "$WORKFLOW_FILE" | grep -v "^  workflow_dispatch:" | sed 's/://g' | while read job; do
              echo "- $job" >> $GITHUB_STEP_SUMMARY
            done
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "$DEPLOYED_PATH" != "" ]; then
            echo "## ‚úÖ „Éá„Éó„É≠„Ç§ÂÆå‰∫Ü" >> $GITHUB_STEP_SUMMARY
            echo "- **‰øùÂ≠òÂ†¥ÊâÄ**: \`$DEPLOYED_PATH\`" >> $GITHUB_STEP_SUMMARY
            echo "- **„Ç¢„ÇØ„ÉÜ„Ç£„Éô„Éº„Ç∑„Éß„É≥**: .disabledÊã°ÂºµÂ≠ê„ÇíÂâäÈô§„Åó„Å¶ÊúâÂäπÂåñ" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "### üéØ Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó" >> $GITHUB_STEP_SUMMARY
            echo "1. ÁîüÊàê„Åï„Çå„Åü„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÇíÁ¢∫Ë™ç" >> $GITHUB_STEP_SUMMARY
            echo "2. \`.disabled\`Êã°ÂºµÂ≠ê„ÇíÂâäÈô§„Åó„Å¶ÊúâÂäπÂåñ" >> $GITHUB_STEP_SUMMARY
            echo "3. Actions „Çø„Éñ„Åã„ÇâÊâãÂãïÂÆüË°å" >> $GITHUB_STEP_SUMMARY
          else
            echo "## ‚ö†Ô∏è „Éá„Éó„É≠„Ç§„Çπ„Ç≠„ÉÉ„Éó" >> $GITHUB_STEP_SUMMARY
            echo "Ê§úË®º„Ç®„É©„Éº„ÅÆ„Åü„ÇÅ„Éá„Éó„É≠„Ç§„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åó„Åü„ÄÇ" >> $GITHUB_STEP_SUMMARY
          fi