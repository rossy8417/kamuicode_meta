name: "Meta Workflow Executor v12 with Domain Templates"
run-name: "ğŸš€ Meta Workflow v12 | Issue #${{ inputs.issue_number || github.event.issue.number }} | ${{ github.actor }}"

on:
  workflow_dispatch:
    inputs:
      issue_number:
        description: 'Issue number for workflow generation request'
        required: true
        default: '66'
  
  issue_comment:
    types: [created]

permissions:
  contents: write
  issues: write
  actions: write
  pull-requests: write

env:
  CLAUDE_CODE_OAUTH_TOKEN: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
  CLAUDE_CODE_CI_MODE: true
  CLAUDE_CODE_AUTO_APPROVE_MCP: true

jobs:
  bootstrap:
    name: "ğŸ§© Bootstrap Diagnostics"
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'issue_comment' && github.event.issue.pull_request == null && contains(github.event.comment.body, '/execute'))
    steps:
      - name: Print event context
        run: |
          echo "event_name=${{ github.event_name }}"
          echo "actor=${{ github.actor }}"
          echo "ref=${{ github.ref }}"
          echo "issue_number_from_dispatch=${{ inputs.issue_number }}"
          echo "is_issue_comment=$([ "${{ github.event_name }}" = "issue_comment" ] && echo true || echo false)"
          if [ "${{ github.event_name }}" = "issue_comment" ]; then
            echo "comment_body<<EOT"
            echo "${{ github.event.comment.body }}"
            echo "EOT"
          fi
          echo "Bootstrap OK"
  
  # ===========================================
  # PHASE 1: ISSUE VALIDATION & DOMAIN DETECTION
  # ===========================================
  
  validate-and-detect:
    name: "ğŸ” Issue Validation & Domain Detection"
    runs-on: ubuntu-latest
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'issue_comment' && github.event.issue.pull_request == null && contains(github.event.comment.body, '/execute'))
    outputs:
      issue_number: ${{ steps.extract.outputs.issue_number }}
      issue_title: ${{ steps.extract.outputs.issue_title }}
      primary_domain: ${{ steps.detect.outputs.primary_domain }}
      detected_domains: ${{ steps.detect.outputs.detected_domains }}
      domain_count: ${{ steps.detect.outputs.domain_count }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          
      - name: Install Dependencies
        run: |
          pip install pyyaml

      - name: Install jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq
        
      - name: Extract Issue Information
        id: extract
        run: |
          # Check if this is a valid trigger
          if [ "${{ github.event_name }}" == "issue_comment" ]; then
            # Use printf to safely handle quotes in comment body
            COMMENT_BODY=$(printf '%s' '${{ github.event.comment.body }}')
            # For issue comments, check if it's a start command
            if ! echo "$COMMENT_BODY" | grep -qE '(/start|/execute|^start$|^å®Ÿè¡Œ$|^execute$)'; then
              echo "::notice::Skipping - Comment does not contain start command"
              echo "issue_number=skip" >> $GITHUB_OUTPUT
              exit 0
            fi
            ISSUE_NUMBER="${{ github.event.issue.number }}"
          else
            # workflow_dispatch always proceeds
            ISSUE_NUMBER="${{ inputs.issue_number }}"
          fi
          
          echo "ğŸ” Analyzing Issue #$ISSUE_NUMBER..."
          
          # Get issue details using GitHub CLI
          ISSUE_DATA=$(gh issue view $ISSUE_NUMBER --json title,body,number --jq '{title: .title, body: .body, number: .number}')
          
          ISSUE_TITLE=$(echo "$ISSUE_DATA" | jq -r '.title')
          ISSUE_BODY=$(echo "$ISSUE_DATA" | jq -r '.body')
          ISSUE_NUMBER=$(echo "$ISSUE_DATA" | jq -r '.number')
          
          # Save to artifacts for next jobs
          mkdir -p artifacts
          echo "$ISSUE_TITLE" > artifacts/issue_title.txt
          echo "$ISSUE_BODY" > artifacts/issue_body.txt
          echo "$ISSUE_NUMBER" > artifacts/issue_number.txt
          
          # Combine title and body for domain detection
          echo -e "$ISSUE_TITLE\n\n$ISSUE_BODY" > artifacts/issue_content.txt
          
          # Output minimal data
          echo "issue_number=$ISSUE_NUMBER" >> $GITHUB_OUTPUT
          echo "issue_title=$ISSUE_TITLE" >> $GITHUB_OUTPUT
          
          echo "âœ… Issue #$ISSUE_NUMBER validated: $ISSUE_TITLE"
          
          # Initialize Progressive Report in GitHub Actions Summary
          cat >> $GITHUB_STEP_SUMMARY << 'EOF'
          # ğŸ¯ Meta Workflow v12 å®Ÿè¡Œãƒ¬ãƒãƒ¼ãƒˆ
          
          ## ğŸ“‹ å®Ÿè¡Œæ¦‚è¦
          EOF
          
          echo "- **Issueç•ªå·**: #${ISSUE_NUMBER}" >> $GITHUB_STEP_SUMMARY
          echo "- **Issue ã‚¿ã‚¤ãƒˆãƒ«**: ${ISSUE_TITLE}" >> $GITHUB_STEP_SUMMARY  
          echo "- **å®Ÿè¡Œé–‹å§‹æ™‚åˆ»**: $(date '+%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')" >> $GITHUB_STEP_SUMMARY
          echo "- **å®Ÿè¡ŒçŠ¶æ³**: ğŸ”„ é€²è¡Œä¸­..." >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## ğŸ” Phase 1: Issueæ¤œè¨¼ & ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… Issue #${ISSUE_NUMBER} å†…å®¹å–å¾—å®Œäº†" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º: (pending)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Detect Domain from Issue
        id: detect
        run: |
          echo "ğŸ¯ Detecting domain from issue content..."
          
          python scripts/domain-template-loader.py \
            --action detect \
            --issue artifacts/issue_content.txt \
            --output artifacts/domain_detection.json
          
          # Extract results
          PRIMARY_DOMAIN=$(jq -r '.primary_domain' artifacts/domain_detection.json)
          DETECTED_DOMAINS=$(jq -c '.detected_domains' artifacts/domain_detection.json)
          DOMAIN_COUNT=$(jq '.detected_domains | length' artifacts/domain_detection.json)
          
          echo "primary_domain=$PRIMARY_DOMAIN" >> $GITHUB_OUTPUT
          echo "detected_domains=$DETECTED_DOMAINS" >> $GITHUB_OUTPUT
          echo "domain_count=$DOMAIN_COUNT" >> $GITHUB_OUTPUT
          
          echo "âœ… Primary domain detected: $PRIMARY_DOMAIN"
          echo "ğŸ“Š Total domains detected: $DOMAIN_COUNT"
          
      - name: Upload Issue and Domain Data
        uses: actions/upload-artifact@v4
        with:
          name: issue-domain-data
          path: artifacts/

      - name: Report Detected Domain
        run: |
          PRIMARY_DOMAIN="${{ steps.detect.outputs.primary_domain }}"
          COUNT="${{ steps.detect.outputs.domain_count }}"
          echo "- âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³æ¤œå‡º: ${PRIMARY_DOMAIN} (${COUNT} detected)" >> $GITHUB_STEP_SUMMARY

  # ===========================================
  # PHASE 2: DOMAIN TEMPLATE LOADING
  # ===========================================
  
  load-domain-templates:
    name: "ğŸ“š Load Domain Templates"
    runs-on: ubuntu-latest
    needs: ['validate-and-detect']
    if: |
      needs.validate-and-detect.outputs.issue_number != 'skip' &&
      needs.validate-and-detect.outputs.primary_domain != 'null'
    outputs:
      template_summary: ${{ steps.load.outputs.template_summary }}
      chunk_count: ${{ steps.load.outputs.chunk_count }}
      input_schema: ${{ steps.inputs.outputs.input_schema }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          
      - name: Install Dependencies
        run: |
          pip install pyyaml

      - name: Install jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          
      - name: Download Issue Data
        uses: actions/download-artifact@v4
        with:
          name: issue-domain-data
          path: artifacts/
          
      - name: Load Primary Domain Template
        id: load
        run: |
          PRIMARY_DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          echo "ğŸ“š Loading template for domain: $PRIMARY_DOMAIN"
          
          # Get domain summary for task decomposition
          python scripts/domain-template-loader.py \
            --action summary-for-decomposition \
            --domain "$PRIMARY_DOMAIN" \
            --output artifacts/domain_decomposition_data.json
          
          # Also get standard summary for reference
          python scripts/domain-template-loader.py \
            --action summary \
            --domain "$PRIMARY_DOMAIN" \
            --output artifacts/domain_summary.json
          
          # Split template into chunks
          python scripts/domain-template-loader.py \
            --action split \
            --domain "$PRIMARY_DOMAIN" \
            --output artifacts/template_chunks.json
          
          # Extract basic info only (not full JSON)
          DOMAIN_NAME=$(jq -r '.domain_info.name' artifacts/domain_decomposition_data.json)
          EXPERT_ROLE=$(jq -r '.domain_info.expert' artifacts/domain_decomposition_data.json)
          CHUNK_COUNT=$(jq '.total_chunks' artifacts/template_chunks.json)
          
          echo "domain_name=$DOMAIN_NAME" >> $GITHUB_OUTPUT
          echo "expert_role=$EXPERT_ROLE" >> $GITHUB_OUTPUT
          echo "chunk_count=$CHUNK_COUNT" >> $GITHUB_OUTPUT
          
          echo "âœ… Template loaded: $DOMAIN_NAME ($CHUNK_COUNT chunks)"
          
          # Add Phase 2 Report
          echo "## ğŸ“š Phase 2: ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³: ${DOMAIN_NAME}" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… å°‚é–€å®¶å½¹å‰²: ${EXPERT_ROLE}" >> $GITHUB_STEP_SUMMARY
          echo "- âœ… ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒãƒ£ãƒ³ã‚¯æ•°: ${CHUNK_COUNT}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

      - name: Locate and Copy Domain Input Schema
        id: inputs
        run: |
          PRIMARY_DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          SCHEMA_PATH="meta/domain-templates/$PRIMARY_DOMAIN/input-schema.yaml"
          mkdir -p artifacts/domain-input-schema
          if [ -f "$SCHEMA_PATH" ]; then
            cp "$SCHEMA_PATH" artifacts/domain-input-schema/input-schema.yaml
            echo "input_schema=artifacts/domain-input-schema/input-schema.yaml" >> $GITHUB_OUTPUT
            echo "âœ… Input schema found: $SCHEMA_PATH"
            echo "- âœ… å…¥åŠ›ã‚¹ã‚­ãƒ¼ãƒæ¤œå‡º: $PRIMARY_DOMAIN/input-schema.yaml" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Input schema not found for domain: $PRIMARY_DOMAIN"
            echo "input_schema=" >> $GITHUB_OUTPUT
            echo "- âŒ å…¥åŠ›ã‚¹ã‚­ãƒ¼ãƒæœªæ¤œå‡º" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Locate Domain Checklists
        id: checklists
        run: |
          PRIMARY_DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          LIST=$(ls -1 "meta/domain-templates/$PRIMARY_DOMAIN"/checklist-*-specific.md 2>/dev/null || true)
          mkdir -p artifacts
          if [ -n "$LIST" ]; then
            echo "$LIST" > artifacts/domain-checklists.txt
            COUNT=$(echo "$LIST" | wc -l | tr -d ' ')
            echo "checklist_count=$COUNT" >> $GITHUB_OUTPUT
            echo "- âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæ¤œå‡º ($COUNT ä»¶):" >> $GITHUB_STEP_SUMMARY
            echo "$LIST" | sed 's/^/- /' >> $GITHUB_STEP_SUMMARY
            echo "ğŸ“š Using domain checklists (detected $COUNT):"
            echo "$LIST" | sed 's/^/- /'
          else
            echo "checklist_count=0" >> $GITHUB_OUTPUT
            echo "- âŒ ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆæœªæ¤œå‡º" >> $GITHUB_STEP_SUMMARY
            echo "ğŸ“š Using domain checklists: (none found)"
          fi
          
      - name: Upload Template Data
        uses: actions/upload-artifact@v4
        with:
          name: domain-template-data
          path: artifacts/

  # ===========================================
  # PHASE 3: PROFESSIONAL TASK DECOMPOSITION
  # ===========================================
  
  professional-task-decomposition:
    name: "ğŸ§  Professional Task Decomposition"
    runs-on: ubuntu-latest
    needs: ['validate-and-detect', 'load-domain-templates']
    outputs:
      task_count: ${{ steps.decompose.outputs.task_count }}
      dependency_groups: ${{ steps.decompose.outputs.dependency_groups }}
      estimated_duration: ${{ steps.decompose.outputs.estimated_duration }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          
      - name: Install Dependencies
        run: |
          npm install -g @anthropic-ai/claude-code
          pip install pyyaml

      - name: Install jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          
      - name: Download and Merge Previous Artifacts
        run: |
          echo "ğŸ“¥ Downloading artifacts from previous jobs..."
          
          # Download artifacts selectively
          mkdir -p artifacts
          
          # Download issue-domain-data
          echo "Downloading issue-domain-data..."
          gh api "/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts" \
            --jq '.artifacts[] | select(.name == "issue-domain-data") | .id' | \
          while read -r artifact_id; do
            gh api "/repos/${{ github.repository }}/actions/artifacts/${artifact_id}/zip" \
              --header "Accept: application/vnd.github+json" \
              > artifacts/issue-domain-data.zip
            unzip -q -o artifacts/issue-domain-data.zip -d artifacts/issue-domain-data/
            rm artifacts/issue-domain-data.zip
          done
          
          # Download domain-template-data
          echo "Downloading domain-template-data..."
          gh api "/repos/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts" \
            --jq '.artifacts[] | select(.name == "domain-template-data") | .id' | \
          while read -r artifact_id; do
            gh api "/repos/${{ github.repository }}/actions/artifacts/${artifact_id}/zip" \
              --header "Accept: application/vnd.github+json" \
              > artifacts/domain-template-data.zip
            unzip -q -o artifacts/domain-template-data.zip -d artifacts/domain-template-data/
            rm artifacts/domain-template-data.zip
          done
          
          # Merge artifacts to flat structure
          echo "Merging artifacts..."
          find artifacts -type f -name "*.json" -o -name "*.txt" -o -name "*.yaml" | while read -r file; do
            filename=$(basename "$file")
            if [ ! -f "artifacts/$filename" ]; then
              cp "$file" "artifacts/$filename"
            fi
          done
          
          echo "âœ… Artifacts downloaded and merged"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Professional Task Decomposition with Domain Knowledge
        id: decompose
        run: |
          echo "ğŸ§  Starting professional task decomposition..."
          
          # Create decomposition prompt using file references
          PRIMARY_DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          EXPERT_ROLE="${{ needs.load-domain-templates.outputs.expert_role }}"
          
          cat > decomposition_prompt.txt << 'EOF'
          å°‚é–€çš„ãªã‚¿ã‚¹ã‚¯åˆ†è§£ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
          
          ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã‚¿ã‚¹ã‚¯åˆ†è§£ã‚’è¡Œã£ã¦ãã ã•ã„ï¼š
          1. ãƒ‰ãƒ¡ã‚¤ãƒ³å°‚é–€çŸ¥è­˜: artifacts/domain-template-data/domain_decomposition_data.json
          2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: artifacts/issue-domain-data/issue_content.txt
          
          ãƒ‰ãƒ¡ã‚¤ãƒ³å°‚é–€çŸ¥è­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ï¼š
          - expert_context: å°‚é–€å®¶ã®å®Œå…¨ãªçŸ¥è­˜
          - task_decomposition_context: ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æœ€é©åŒ–æƒ…å ±
          - constraints_and_requirements: ã™ã¹ã¦ã®åˆ¶ç´„äº‹é …
          - implementation_resources: åˆ©ç”¨å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹
          - complex_thinking_guide: æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®ã‚¬ã‚¤ãƒ‰
          
          ã‚¿ã‚¹ã‚¯åˆ†è§£ã®æ‰‹é †ï¼š
          1. domain_decomposition_data.jsonã®expert_contextã‚’å®Œå…¨ã«ç†è§£
          2. task_decomposition_contextã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‚ç…§
          3. constraints_and_requirementsã®ã™ã¹ã¦ã®åˆ¶ç´„ã‚’è€ƒæ…®
          4. complex_thinking_guideã«å¾“ã£ã¦è¤‡é›‘ãªæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ
          5. implementation_resourcesã‹ã‚‰æœ€é©ãªãƒªã‚½ãƒ¼ã‚¹ã‚’é¸æŠ
          
          å‡ºåŠ›ã‚’artifacts/professional_task_decomposition.jsonã«ä¿å­˜ã—ã¦ãã ã•ã„ã€‚
          
          å‡ºåŠ›å½¢å¼ï¼š
          {
            "professional_analysis": {
              "understanding": "ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å°‚é–€çš„ç†è§£ï¼ˆè©³ç´°ï¼‰",
              "considerations": ["è€ƒæ…®äº‹é …ã®ãƒªã‚¹ãƒˆ"],
              "thinking_process": "æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°ãªè¨˜éŒ²"
            },
            "tasks": [
              {
                "id": "task-1",
                "name": "ã‚¿ã‚¹ã‚¯å",
                "description": "è©³ç´°ãªèª¬æ˜",
                "reasoning": "ãªãœã“ã®ã‚¿ã‚¹ã‚¯ãŒå¿…è¦ã‹",
                "minimal_units": ["unit1", "unit2"],
                "dependencies": [],
                "estimated_duration": "5-10åˆ†",
                "professional_notes": "å°‚é–€çš„ãªæ³¨æ„ç‚¹",
                "quality_criteria": "å“è³ªåŸºæº–"
              }
            ],
            "workflow_optimization": {
              "parallel_groups": [],
              "critical_path": [],
              "optimization_rationale": "æœ€é©åŒ–ã®ç†ç”±"
            },
            "workflow_generation_parameters": {
              "calculated_scene_count": "constraints.yamlã®scene_calculationã«åŸºã¥ã„ã¦è¨ˆç®—ã—ãŸæ•°å€¤",
              "matrix_scene_list": "[1, 2, 3, ...]ã®å½¢å¼ã§è¨ˆç®—ã•ã‚ŒãŸã‚·ãƒ¼ãƒ³ãƒªã‚¹ãƒˆ",
              "max_parallel": "calculated_scene_countã¨åŒã˜å€¤",
              "assumed_scene_duration": "è¨ˆç®—ã§ä½¿ç”¨ã—ãŸ1ã‚·ãƒ¼ãƒ³ã‚ãŸã‚Šã®ç§’æ•°",
              "per_scene_job_structure": "CRITICAL: T2Iâ†’I2V must be in SAME job (prevents URL expiry)"
            },
            "total_estimated_duration": "30åˆ†",
            "domain_specific_constraints": []
          }
          EOF
          
          # Add expert role context
          echo "ã‚ãªãŸã¯${EXPERT_ROLE}ã§ã™ã€‚" > final_prompt.txt
          echo "" >> final_prompt.txt
          cat decomposition_prompt.txt >> final_prompt.txt
          
          # Execute Claude Code for task decomposition
          npx @anthropic-ai/claude-code \
            -p "$(cat final_prompt.txt)" \
            --allowedTools "Read,Write" \
            --permission-mode "acceptEdits" \
            > claude_output.log 2>&1
          
          # Check if execution was successful
          if [ $? -eq 0 ]; then
            echo "âœ… Claude Code execution completed"
            
            # Try multiple methods to find the generated file
            if [ -f "artifacts/professional_task_decomposition.json" ]; then
              echo "âœ… Found file at expected location"
            elif [ -f "professional_task_decomposition.json" ]; then
              echo "ğŸ“ Found file in current directory, moving to artifacts"
              mv professional_task_decomposition.json artifacts/
            else
              # Search for any JSON file that might contain the task decomposition
              echo "ğŸ” Searching for generated JSON files..."
              find . -name "*.json" -type f -newer final_prompt.txt -exec grep -l "professional_analysis" {} \; | while read -r file; do
                echo "ğŸ“ Found potential task decomposition at: $file"
                cp "$file" artifacts/professional_task_decomposition.json
                break
              done
            fi
            
            # Final check
            if [ ! -f "artifacts/professional_task_decomposition.json" ]; then
              echo "âŒ Could not find task decomposition file"
              echo "ğŸ“‹ Claude output:"
              cat claude_output.log
              exit 1
            fi
          else
            echo "âŒ Claude Code execution failed"
            cat claude_output.log
            exit 1
          fi
          
          # Extract results
          if [ -f "artifacts/professional_task_decomposition.json" ]; then
            TASK_COUNT=$(jq '.tasks | length' artifacts/professional_task_decomposition.json)
            DEPENDENCY_GROUPS=$(jq -c '.dependency_groups' artifacts/professional_task_decomposition.json)
            ESTIMATED_DURATION=$(jq -r '.total_estimated_duration' artifacts/professional_task_decomposition.json)
            
            echo "task_count=$TASK_COUNT" >> $GITHUB_OUTPUT
            echo "dependency_groups=$DEPENDENCY_GROUPS" >> $GITHUB_OUTPUT
            echo "estimated_duration=$ESTIMATED_DURATION" >> $GITHUB_OUTPUT
            
            echo "âœ… Decomposed into $TASK_COUNT tasks"
            echo "â±ï¸ Estimated duration: $ESTIMATED_DURATION"
            
            # Add Phase 3 Report
            echo "## ğŸ§  Phase 3: ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚¿ã‚¹ã‚¯åˆ†è§£" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… ã‚¿ã‚¹ã‚¯æ•°: ${TASK_COUNT}å€‹" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… æ¨å®šå®Ÿè¡Œæ™‚é–“: ${ESTIMATED_DURATION}" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… ä¾å­˜é–¢ä¿‚ã‚°ãƒ«ãƒ¼ãƒ—åˆ†æå®Œäº†" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Task decomposition failed"
            exit 1
          fi
          
      - name: Upload Task Decomposition
        uses: actions/upload-artifact@v4
        with:
          name: task-decomposition
          path: artifacts/professional_task_decomposition.json

  # ===========================================
  # PHASE 4: TASK ORDER OPTIMIZATION
  # ===========================================
  
  optimize-task-order:
    name: "ğŸ”„ Optimize Task Execution Order"
    runs-on: ubuntu-latest
    needs: ['professional-task-decomposition']
    outputs:
      optimized_order: ${{ steps.optimize.outputs.order }}
      mermaid_available: ${{ steps.optimize.outputs.mermaid_available }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Task Decomposition
        uses: actions/download-artifact@v4
        with:
          name: task-decomposition
          path: artifacts/
          
      - name: Analyze and Optimize Task Order
        id: optimize
        run: |
          echo "ğŸ”„ ã‚¿ã‚¹ã‚¯å®Ÿè¡Œé †åºã®æœ€é©åŒ–..."
          
          # Create artifacts directory
          mkdir -p artifacts
          
          # Run Claude Code SDK with specialized prompt file for reliable Mermaid generation
          npx @anthropic-ai/claude-code \
            --mcp-config ".claude/mcp-kamuicode.json" \
            -p "$(cat meta/prompts/task-order-optimization-with-mermaid.md)" \
            --allowedTools "Read,Write" \
            --permission-mode "acceptEdits"
          
          # çµæœã‚’ç¢ºèª
          if [ -f "artifacts/optimized_task_order.json" ]; then
            echo "order=true" >> $GITHUB_OUTPUT
            
            # Add Phase 4 Report with Task Order
            echo "## ğŸ”„ Phase 4: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œé †åºã®æœ€é©åŒ–" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… ä¾å­˜é–¢ä¿‚åˆ†æå®Œäº†" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… ä¸¦åˆ—å‡¦ç†ã‚°ãƒ«ãƒ¼ãƒ—ç‰¹å®š" >> $GITHUB_STEP_SUMMARY
            
            # å‹•çš„ãƒ†ã‚­ã‚¹ãƒˆå›³ã‚’ç”Ÿæˆï¼ˆæœ€é©åŒ–ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰è‡ªå‹•ç”Ÿæˆï¼‰
            if [ -f "artifacts/optimized_task_order.json" ]; then
              echo "- âœ… æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè¡Œé †åºç”Ÿæˆå®Œäº†" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "### ğŸ“Š å®Ÿè¡Œãƒ•ãƒ­ãƒ¼å›³ï¼ˆå‹•çš„ç”Ÿæˆï¼‰" >> $GITHUB_STEP_SUMMARY
              echo '```' >> $GITHUB_STEP_SUMMARY
              
              # JSONã‹ã‚‰å‹•çš„ã«ã‚¿ã‚¹ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç”Ÿæˆï¼ˆå˜ä¸€è¡Œã§å®Ÿè¡Œï¼‰
              if [ -f "artifacts/optimized_task_order.json" ]; then
                python3 -c "import json; data=json.load(open('artifacts/optimized_task_order.json')); print('ã‚¿ã‚¹ã‚¯å®Ÿè¡Œãƒ•ãƒ­ãƒ¼ï¼ˆå‹•çš„ç”Ÿæˆãƒ»ä¸¦åˆ—å‡¦ç†æœ€é©åŒ–æ¸ˆã¿ï¼‰:'); print(); [print(f\"âš¡ {phase.get('phase', f'Phase {i+1}')} [{phase.get('execution_type', 'sequential').upper()}]\") if phase.get('execution_type') == 'parallel' else print(f\"ğŸ“‹ {phase.get('phase', f'Phase {i+1}')} [{phase.get('execution_type', 'sequential').upper()}]\") for i, phase in enumerate(data.get('optimized_execution_order', []))]" >> $GITHUB_STEP_SUMMARY
                echo "â±ï¸ æœ€é©åŒ–ã«ã‚ˆã‚Šä¸¦åˆ—å‡¦ç†ã‚’æ´»ç”¨ã—ãŸåŠ¹ç‡çš„ãªå®Ÿè¡Œé †åºã‚’ç”Ÿæˆ" >> $GITHUB_STEP_SUMMARY
              else
                echo "ã‚¿ã‚¹ã‚¯ãƒ•ãƒ­ãƒ¼æƒ…å ±ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“" >> $GITHUB_STEP_SUMMARY
              fi
              
              echo '```' >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "mermaid_available=true" >> $GITHUB_OUTPUT
            else
              echo "- âŒ æœ€é©åŒ–é †åºã®ç”Ÿæˆã«å¤±æ•—" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "mermaid_available=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "order=false" >> $GITHUB_OUTPUT
          fi
          
      # Summary display moved to final report for better organization
          
      - name: Upload Optimized Order
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: optimized-task-order
          path: artifacts/
          if-no-files-found: warn

  # ===========================================
  # PHASE 5: CONSTRAINT-AWARE WORKFLOW GENERATION
  # ===========================================
  
  generate-professional-workflow:
    name: "âš¡ Generate Professional Workflow"
    runs-on: ubuntu-latest
    needs: ['validate-and-detect', 'load-domain-templates', 'professional-task-decomposition', 'optimize-task-order']
    outputs:
      workflow_path: ${{ steps.generate.outputs.workflow_path }}
      workflow_name: ${{ steps.generate.outputs.workflow_name }}
      project_dir: ${{ steps.generate.outputs.project_dir }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Environment
        run: |
          npm install -g @anthropic-ai/claude-code
          pip install pyyaml

      - name: Install jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
          
      - name: Generate Professional Workflow
        id: generate
        run: |
          echo "âš¡ Generating professional workflow..."
          
          # Prepare all necessary data
          ISSUE_NUMBER="${{ needs.validate-and-detect.outputs.issue_number }}"
          PRIMARY_DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          
          # Create generation directory with absolute path
          # Use GITHUB_WORKSPACE for consistency across jobs
          if [ -n "$GITHUB_WORKSPACE" ]; then
            BASE_DIR="$GITHUB_WORKSPACE"
          else
            BASE_DIR="$(pwd)"
          fi
          PROJECT_DIR="${BASE_DIR}/projects/issue-${ISSUE_NUMBER}-${TIMESTAMP}"
          mkdir -p "$PROJECT_DIR/generated-workflow"
          
          echo "ğŸ“ Project directory: $PROJECT_DIR"
          
          # Load template chunks progressively
          CHUNKS=$(jq -r '.chunks[].id' artifacts/domain-template-data/template_chunks.json)
          
          # Create comprehensive generation prompt (domain-agnostic with strict domain enforcement when provided)
          cat > generation_prompt.txt << 'EOF'
          You are a "Universal Meta-Workflow Generator". Read the user requirements (Issue) and domain templates to generate optimal GitHub Actions workflows.

          Input (must reference):
          1. Task decomposition: artifacts/task-decomposition/professional_task_decomposition.json
             Critical: workflow_generation_parameters section's calculated_scene_count and matrix_scene_list
          2. Optimized order (optional): artifacts/optimized-task-order/optimized_task_order.json
          3. Input schema (required): artifacts/domain-input-schema/input-schema.yaml
          4. Required inputs list (optional): artifacts/required_inputs.json / artifacts/required_input_keys.txt
          5. Domain summary/decomposition data (optional): artifacts/domain-template-data/domain_summary.json, artifacts/domain-template-data/domain_decomposition_data.json
          6. Domain checklist list (optional): artifacts/domain-template-data/domain-checklists.txt
          7. Common rules: docs/YAML_CONSTRUCTION_GUIDELINES.md, docs/MINIMAL_UNIT_DATA_DEPENDENCIES.md
          8. Claude Code data persistence: docs/CLAUDE_CODE_DATA_PERSISTENCE_GUIDE.md

          Generation Guide (Universal):
          STARTUP ERROR PREVENTION (CRITICAL):
          - MUST: Include BOTH workflow_dispatch AND push triggers
          - MUST: Push trigger must have: paths-ignore: ['.github/workflows/**']
          - MUST: All required inputs MUST have default values
          - MUST: First job must check event type and skip on push events
          - MUST: All jobs must have timeout-minutes defined
          
          Standard Requirements:
          - MUST: Don't use local path references with uses: (inline implementation)
          - MUST: Save all outputs/intermediate products under PROJECT_DIR_PLACEHOLDER
          - MUST: Use actions/upload-artifact / download-artifact for job sharing
          - MUST: workflow_dispatch.inputs must reflect input-schema.yaml and required inputs list (enumâ†’choice, default, description, supplement required keys)
          - MUST: Prohibit absolute paths or root-level outputs, always use PROJECT_DIR_PLACEHOLDER
          - MUST: Never use matrix variables in job outputs section (outputs cannot reference ${{ matrix.* }})
          - MUST: Matrix strategy can only be used within steps, not in outputs definition
          - MUST: For news video, calculate scenes as: ceil(duration_seconds / 5) (e.g., 60s = 12 scenes, 30s = 6 scenes)
          - MUST: Create ONE news anchor with fixed seed value, generate multiple lip-sync videos for all scenes
          - MUST: Include the following in environment variable section (env:):
            env:
              CLAUDE_CODE_CI_MODE: true
              CLAUDE_CODE_AUTO_APPROVE_MCP: true
              CLAUDE_CODE_OAUTH_TOKEN: SECRETS_PLACEHOLDER
            Note: SECRETS_PLACEHOLDER will be automatically replaced with proper GitHub Actions secrets syntax in post-processing
          - MUST: Follow these patterns when executing Claude Code:
            1. Execute generation/processing with MCP tools
            2. Save local files with Write tool (explicit path: ${PROJECT_DIR}/media/...)
            3. Verify save with ls -la using Bash tool
            * Details: See docs/CLAUDE_CODE_DATA_PERSISTENCE_GUIDE.md
          - MUST: For video-production, combine T2I and I2V in single job per scene:
            Example structure for each scene job:
            1. Generate image with T2I MCP tool
            2. Save image locally
            3. IMMEDIATELY convert to video with I2V MCP tool (within 3 minutes)
            4. Save video locally
            This prevents URL expiration between T2I and I2V
          - MUST: Execute file search with multiple patterns:
            1. Specific pattern: "*scene${NUM}*.png"
            2. Time-based: "*.png -mmin -2"
            3. Generic pattern: "*.png"
          - MUST: Download immediately when URL file detected
            * IMPORTANT: MCP tools return gs:// URLs from Google services
            * These gs:// URLs are PUBLIC and can be converted to HTTPS
            * Pattern: gs://bucket/path â†’ https://storage.googleapis.com/bucket/path
            * Always create download_url() helper function for proper handling
          - SHOULD: Split steps within 21000 characters

          Domain Knowledge Application (Mandatory):
          - If artifacts/domain-template-data/domain-checklists.txt exists, read each listed checklist and comply with "MUST" requirements. If compliance is not possible, perform alternative design during generation, and if still impossible, output with the premise of FAILED at validation stage.
          - If meta/domain-templates/<domain>/constraints.yaml exists:
            * MUST: Apply constraints.composition_rules / timing_constraints / orchestration / path constraints (only when relevant tasks exist)
            * SHOULD: Read each file listed in constraints.rule_references / checklist_references (rules/*.yaml, checklists/*.md) in order, prioritizing MUST for design reflection
          - When multiple domains are involved:
            * MUST: Apply integrated constraints / rules / checklists from each domain
            * MUST: Adopt safer side (stricter MUST) in conflicts, explicitly note compromises

          Design Principles (Examples):
          - When explicit serial requirements like imageâ†’video are shown in constraints/rules, execute "serial chain" within the same job for each target item/scene, and parallelize overall with matrix (max-parallel follows constraints).
          - Similar tasks (e.g., generating multiple slides) are optimized for parallel execution. However, serialize when data dependencies exist.
          - When domain is not specified or no constraints exist, use general serial/parallel configuration based on task decomposition/optimization order.
          
          Important: For video-production domain:
          - Always use workflow_generation_parameters.matrix_scene_list to set matrix.scene
          - IMPORTANT: max-parallel in GitHub Actions only accepts numeric literals (not variables)
            - Option 1: Remove max-parallel entirely (defaults to unlimited parallel jobs)
            - Option 2: Set a high fixed value like max-parallel: 256 as upper limit
            - Recommendation: Remove max-parallel to allow full parallelization based on scene count
          - Use calculated dynamic values, not fixed values (e.g., batch: [1,2,3])
          - Use consistent character for news anchor (fixed seed)
          - CRITICAL: Image generation (T2I) â†’ video conversion (I2V) MUST be in SAME JOB (serial execution)
            - This avoids Google Cloud Storage URL expiration (15 minutes)
            - Each scene job should: Generate image â†’ Convert to video immediately
            - NEVER split T2I and I2V into separate jobs
          - MUST: Include the following EXACT code patterns in each scene image generation step:
            a. Explicit save path specification to Claude Code:
               SAVE_PATH="\${PROJECT_DIR}/media/images/scene\${SCENE_NUM}.png"
               URL_PATH="\${PROJECT_DIR}/media/images/scene\${SCENE_NUM}-url.txt"
            b. Claude Code prompt must include:
               "1. Generate image with MCP tool
                2. Save to \${SAVE_PATH} using Write tool
                3. Save URL to \${URL_PATH} using Write tool
                4. Execute ls -la \${PROJECT_DIR}/media/images/ using Bash tool"
            c. After Claude Code execution, MUST include:
               # Immediate URL download (prevent expiration)
               [ -f "\$URL_PATH" ] && curl -L -o "\$SAVE_PATH" "\$(cat \$URL_PATH)"
            d. Multi-pattern file search (NEVER assume exact filename):
               IMAGE=\$(find "\$PROJECT_DIR" -name "*scene\${SCENE_NUM}*.png" 2>/dev/null | head -1)
               [ -z "\$IMAGE" ] && IMAGE=\$(find "\$PROJECT_DIR" -name "*.png" -mmin -2 2>/dev/null | head -1)
               [ -z "\$IMAGE" ] && IMAGE=\$(find "\$PROJECT_DIR" -name "*.png" 2>/dev/null | head -1)
            e. File validation before use:
               if [ -f "\$IMAGE" ] && [ \$(stat -c%s "\$IMAGE") -gt 10000 ]; then
                 echo "âœ… Valid image: \$IMAGE"
               else
                 echo "âŒ Invalid or missing image"
               fi
          - CRITICAL: Setup job MUST include:
            echo "workflow_start=\$(date -Iseconds)" >> \$GITHUB_OUTPUT
            # NOT github.run_started_at which may be empty
          - Recommended job structure:
            1. phase1: Information gathering/research
            2. phase2: Content composition/script creation
            3. phase3-scene-N-image: Each scene image generation (N parallel jobs)
               - News anchor uses fixed seed
               - Include explicit file save and URL processing
            4. phase4-scene-N-video: Each scene video conversion (N parallel jobs)
               - needs: corresponding phase3-scene-N-image
               - Execute within URL expiration (early start)
               - Support both URL and local path
            5. phase5: Audio generation (narration, BGM)
            6. phase6: Final editing/synthesis

          Output destination (MUST):
          - Generate workflow at PROJECT_DIR_PLACEHOLDER/generated-workflow/workflow.yml
          
          ğŸ”„ ERROR RECOVERY REQUIREMENTS (MANDATORY):
          For ANY parallel generation tasks (scenes, images, data), you MUST include:
          1. Main generation job with:
             - strategy.fail-fast: false
             - continue-on-error: true
             - outputs.failed_items to track failures
             - Status verification for each item
          2. Recovery job that:
             - Runs after main generation
             - Only processes failed items
             - Uses fromJson(needs.main.outputs.failed_items) for matrix
             - Includes 3 recovery attempts with different strategies
          3. Final assembly that:
             - Waits for both main and recovery jobs
             - Verifies all items are available
          
          Refer to docs/GENERIC_ERROR_RECOVERY_PATTERNS.md for implementation details.
          
          CRITICAL REQUIREMENTS:
          - Output ONLY valid GitHub Actions YAML
          - Do NOT include any function calls, tool invocations, or internal commands in the output
          - The workflow file must be complete and self-contained
          - Use unquoted on: field (GitHub Actions requires it unquoted)
          - Keep workflow names simple and without special characters
          - ALWAYS include recovery jobs for parallel generation tasks
          EOF
          
          # Replace project directory placeholder with actual path
          sed -i "s|PROJECT_DIR_PLACEHOLDER|$PROJECT_DIR|g" generation_prompt.txt
          
          # Add visibility in logs: which checklists will be referenced
          echo "ğŸ“š Using domain checklists:" >> $GITHUB_STEP_SUMMARY
          if [ -f artifacts/domain-template-data/domain-checklists.txt ]; then
            cat artifacts/domain-template-data/domain-checklists.txt | sed 's/^/- /' >> $GITHUB_STEP_SUMMARY
          elif [ -f artifacts/domain-checklists.txt ]; then
            cat artifacts/domain-checklists.txt | sed 's/^/- /' >> $GITHUB_STEP_SUMMARY
          else
            echo "- (none found)" >> $GITHUB_STEP_SUMMARY
          fi

          # Also list rule references if present in constraints
          if [ -f artifacts/domain-template-data/domain_summary.json ]; then
            echo "### Domain Rule References" >> $GITHUB_STEP_SUMMARY
            jq -r '.constraints.rule_references[]?.path' artifacts/domain-template-data/domain_summary.json 2>/dev/null | sed 's/^/- /' >> $GITHUB_STEP_SUMMARY || true
          fi

          # Note: do not inject domain-specific helper jobs inline. All domain rules are enforced via referenced checklists during validation/auto-fix.
          
          # Execute workflow generation with explicit output
          echo "ğŸ“ Generating workflow with Claude Code SDK..."
          echo "Target: $PROJECT_DIR/generated-workflow/workflow.yml"
          
          npx @anthropic-ai/claude-code \
            -p "$(cat generation_prompt.txt)" \
            --mcp-config ".claude/mcp-kamuicode.json" \
            --allowedTools "Write,Read,MultiEdit,Bash" \
            --permission-mode "acceptEdits" \
            --max-turns 50

          # Do not inlineãƒ‰ãƒ¡ã‚¤ãƒ³è¦ä»¶ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã¯è¡Œã‚ãšã€ç”Ÿæˆæ™‚ã«å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã‚€å½¢ã§åæ˜ ã•ã›ã¾ã™
          
          # Verify workflow was created
          WORKFLOW_PATH="$PROJECT_DIR/generated-workflow/workflow.yml"
          if [ -f "$WORKFLOW_PATH" ]; then
            echo "ğŸ“‹ Post-processing generated workflow..."
            
            # Clean up any function call artifacts that might have been included
            if grep -q "<function_calls>" "$WORKFLOW_PATH" 2>/dev/null; then
              echo "âš ï¸ Found function call artifacts, cleaning up..."
              # Remove everything from <function_calls> onwards
              sed -i '/<function_calls>/,$d' "$WORKFLOW_PATH"
            fi
            
            # Note: Do NOT quote the "on" field - GitHub Actions requires it unquoted
            # Make sure it's unquoted (in case Claude Code quoted it)
            sed -i 's/^"on":$/on:/' "$WORKFLOW_PATH"
            
            # Replace SECRETS_PLACEHOLDER or actual token values with the proper secrets reference
            if grep -q "SECRETS_PLACEHOLDER" "$WORKFLOW_PATH" 2>/dev/null; then
              echo "âš ï¸ Replacing SECRETS_PLACEHOLDER with actual secrets reference..."
              sed -i 's/SECRETS_PLACEHOLDER/\${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}/' "$WORKFLOW_PATH"
            fi
            
            # CRITICAL: Remove any actual OAUTH_TOKEN values that were directly inserted
            if grep -q "sk-ant-" "$WORKFLOW_PATH" 2>/dev/null; then
              echo "ğŸš¨ CRITICAL: Found exposed OAUTH_TOKEN value, replacing with secrets reference..."
              sed -i 's/sk-ant-[^[:space:]]*/\${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}/g' "$WORKFLOW_PATH"
            fi
            
            # Remove invalid matrix references in outputs section
            # GitHub Actions doesn't allow ${{ matrix.* }} in job outputs
            if grep -q 'outputs:.*matrix\.' "$WORKFLOW_PATH" 2>/dev/null; then
              echo "âš ï¸ Found invalid matrix references in outputs, removing..."
              # Remove lines with matrix references in outputs sections
              sed -i '/^\s*outputs:/,/^\s*steps:/{/matrix\./d}' "$WORKFLOW_PATH"
            fi
            
            # Fix GCS URL handling - proper solution
            if grep -q 'gs://' "$WORKFLOW_PATH" 2>/dev/null; then
              echo "âš ï¸ Found GCS URLs, converting to HTTPS URLs..."
              
              # Simple replacement: gs://bucket/path -> https://storage.googleapis.com/bucket/path
              sed -i 's|gs://\([^/]*\)/|https://storage.googleapis.com/\1/|g' "$WORKFLOW_PATH" 2>/dev/null || true
              
              echo "  âœ… Converted GCS URLs to HTTPS format"
            fi
            
            # Fix common YAML syntax issues with simple sed commands
            echo "âš ï¸ Fixing common YAML syntax issues..."
            
            # Remove max-parallel to allow unlimited parallelization
            sed -i '/^\s*max-parallel:/d' "$WORKFLOW_PATH" 2>/dev/null || true
            echo "  âœ… Removed max-parallel to allow dynamic parallelization"
            
            # Fix heredoc issues - escape $(date) in cat << EOF blocks
            sed -i 's/Generated: $(date)/Generated: \\$(date)/g' "$WORKFLOW_PATH" 2>/dev/null || true
            echo "  âœ… Fixed heredoc syntax issues"
            
            # Ensure workflow ends properly (no trailing content)
            # Add a newline at the end if missing
            if [ -n "$(tail -c 1 "$WORKFLOW_PATH")" ]; then
              echo "" >> "$WORKFLOW_PATH"
            fi
            
            WORKFLOW_NAME="professional-workflow-${PRIMARY_DOMAIN}-${TIMESTAMP}"
            
            echo "workflow_path=$WORKFLOW_PATH" >> $GITHUB_OUTPUT
            echo "workflow_name=$WORKFLOW_NAME" >> $GITHUB_OUTPUT
            echo "project_dir=$PROJECT_DIR" >> $GITHUB_OUTPUT
            
            echo "âœ… Workflow generated and cleaned: $WORKFLOW_NAME"
            
            # Add Phase 5 Report
            echo "## âš¡ Phase 5: ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å: ${WORKFLOW_NAME}" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… ãƒ‰ãƒ¡ã‚¤ãƒ³: ${PRIMARY_DOMAIN}" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… GitHub Actionså½¢å¼ã§ç”Ÿæˆå®Œäº†" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Workflow generation failed"
            exit 1
          fi
          
      - name: Upload Generated Workflow
        uses: actions/upload-artifact@v4
        with:
          name: generated-workflow
          path: |
            projects/issue-${{ needs.validate-and-detect.outputs.issue_number }}-*

  # ===========================================
  # PHASE 6: VALIDATION & DEPLOYMENT
  # ==========================================
  
  validate-workflow:
    name: "ğŸ” Validate Workflow"
    runs-on: ubuntu-latest
    needs: ['validate-and-detect','generate-professional-workflow']
    outputs:
      validation_passed: ${{ steps.validate.outputs.passed }}
      validation_report: ${{ steps.validate.outputs.report_path }}
      error_count: ${{ steps.validate.outputs.error_count }}
      warning_count: ${{ steps.validate.outputs.warning_count }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Download Generated Workflow
        uses: actions/download-artifact@v4
        with:
          name: generated-workflow
          path: projects/

      - name: Download Domain Template Data (for input schema)
        uses: actions/download-artifact@v4
        with:
          name: domain-template-data
          path: artifacts/

      - name: Validate Startup Requirements
        id: startup-validation
        run: |
          WORKFLOW_PATH="${{ needs.generate-professional-workflow.outputs.workflow_path }}"
          echo "ğŸ” Startup validation..."
          ERRORS=0
          
          # Check for push event handling
          if ! grep -q "paths-ignore:" "$WORKFLOW_PATH" 2>/dev/null; then
            if ! grep -q "push:" "$WORKFLOW_PATH" 2>/dev/null; then
              echo "âŒ ERROR: No push event handling - workflow will fail on git push"
              ERRORS=$((ERRORS + 1))
            fi
          fi
          
          # Check for default values on required inputs
          if grep -q "required: true" "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âš ï¸ Checking for defaults on required inputs..."
          fi
          
          # Check for environment variables
          if ! grep -q "^env:" "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âŒ ERROR: No environment variables defined at workflow level"
            ERRORS=$((ERRORS + 1))
          fi
          
          # Check for timeout on jobs
          if ! grep -q "timeout-minutes:" "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âš ï¸ WARNING: No timeout-minutes defined for jobs"
          fi
          
          echo "startup_errors=$ERRORS" >> $GITHUB_OUTPUT
          
      - name: Validate Data Flow Pattern
        run: |
          WORKFLOW_PATH="${{ needs.generate-professional-workflow.outputs.workflow_path }}"
          echo "ğŸ” ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ¤œè¨¼..."
          
          # Claude Codeå®Ÿè¡Œéƒ¨åˆ†ã®æ¤œè¨¼
          if ! grep -q "Write.*media/images" "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âš ï¸ WARNING: æ˜ç¤ºçš„ãªãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜æŒ‡ç¤ºãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
            echo "  æ¨å¥¨: Writeãƒ„ãƒ¼ãƒ«ã§æ˜ç¤ºçš„ã«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„"
          fi
          
          if ! grep -q "ls -la.*media" "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âš ï¸ WARNING: ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç¢ºèªã‚³ãƒãƒ³ãƒ‰ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
            echo "  æ¨å¥¨: ls -laã‚³ãƒãƒ³ãƒ‰ã§ä¿å­˜ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
          fi
          
          if ! grep -q "curl.*-[Lo].*url" "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âš ï¸ WARNING: URLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
            echo "  æ¨å¥¨: URLãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡ºæ™‚ã¯curlã§å³åº§ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
          fi
          
          # findã‚³ãƒãƒ³ãƒ‰ã®ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’ç¢ºèª
          FIND_COUNT=$(grep -c "find.*PROJECT_DIR" "$WORKFLOW_PATH" 2>/dev/null || echo 0)
          if [ "$FIND_COUNT" -lt 3 ]; then
            echo "âš ï¸ WARNING: ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå°‘ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼ˆç¾åœ¨: $FIND_COUNTå›ï¼‰"
            echo "  æ¨å¥¨: æœ€ä½3ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢ï¼ˆç‰¹å®šåã€æ™‚é–“ãƒ™ãƒ¼ã‚¹ã€æ±ç”¨ï¼‰"
          fi
          
          # Check for invalid matrix references in outputs
          if grep -q 'outputs:.*matrix\.' "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âŒ ERROR: Invalid matrix references found in outputs section"
            echo "  GitHub Actions does not allow \${{ matrix.* }} in job outputs"
            echo "  Found: $(grep 'outputs:.*matrix\.' "$WORKFLOW_PATH")"
          fi
          
          # Check for GCS URL handling
          if grep -q 'curl.*gs://' "$WORKFLOW_PATH" 2>/dev/null; then
            echo "âŒ ERROR: Direct curl of GCS URLs detected"
            echo "  curl does not support gs:// protocol"
            echo "  Use gsutil or convert to signed https:// URL"
            VALIDATION_PASSED=false
          fi
          
          # Check for URL validity checks that don't handle GCS
          if grep -q 'curl -IfsS.*URL' "$WORKFLOW_PATH" 2>/dev/null; then
            if ! grep -B2 -A2 'curl -IfsS' "$WORKFLOW_PATH" | grep -q 'if.*gs://'; then
              echo "âš ï¸ WARNING: URL validity checks may fail on GCS URLs"
              echo "  Consider adding GCS URL detection before curl checks"
            fi
          fi
          
          echo "âœ… ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼æ¤œè¨¼å®Œäº†"
          
      - name: Comprehensive Workflow Validation and Auto-Fix
        run: |
          WORKFLOW_PATH="${{ needs.generate-professional-workflow.outputs.workflow_path }}"
          
          echo "ğŸ” Running comprehensive validation..."
          
          # Use unified validator with auto-fix
          if python3 scripts/workflow-validator.py "$WORKFLOW_PATH" --auto-fix; then
            echo "âœ… Workflow validation passed"
          else
            echo "âš ï¸ Validation failed, checking report..."
            
            # Check if critical errors remain
            if [ -f "$(dirname "$WORKFLOW_PATH")/validation_report.json" ]; then
              CRITICAL_ERRORS=$(jq -r '.errors | length' "$(dirname "$WORKFLOW_PATH")/validation_report.json")
              if [ "$CRITICAL_ERRORS" -gt 0 ]; then
                echo "âŒ Critical errors remain after auto-fix"
                jq -r '.errors[]' "$(dirname "$WORKFLOW_PATH")/validation_report.json" | while read error; do
                  echo "  â€¢ $error"
                done
                exit 1
              fi
            fi
          fi
          
      - name: Inject required inputs into generated workflow from schema
        run: |
          WORKFLOW_PATH="${{ needs.generate-professional-workflow.outputs.workflow_path }}"
          # Schema file downloaded from 'domain-template-data' artifact into artifacts/
          SCHEMA_PATH="artifacts/domain-input-schema/input-schema.yaml"
          if [ -f "$SCHEMA_PATH" ] && [ -f "$WORKFLOW_PATH" ]; then
            python -c "import yaml,sys; wf_path='$WORKFLOW_PATH'; schema_path='$SCHEMA_PATH'; wf=yaml.safe_load(open(wf_path)) or {}; schema=yaml.safe_load(open(schema_path)) or {}; req=(schema.get('inputs') or {}).get('required') or {}; on=wf.setdefault('on',{}); wd=on.setdefault('workflow_dispatch',{}); inputs=wd.setdefault('inputs',{}); [ (inputs.setdefault(k,{}).update({'description':(v.get('description',k)), 'required':True}) or (inputs[k].update({'default':v['default']}) if 'default' in v else None) or (inputs[k].update({'type':'choice','options':v['enum']}) if 'enum' in v else inputs[k].pop('type', None)) ) for k,v in req.items() ]; open(wf_path,'w').write(yaml.safe_dump(wf, sort_keys=False))"
            echo "âœ… Injected required inputs into workflow_dispatch"
          else
            echo "::warning::Schema or workflow file not found; skip injection"
          fi

      - name: Install jq
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          
      - name: Validate Workflow
        id: validate
        run: |
          echo "âœ… Validating generated workflow..."
          
          # Use generated workflow path
          WORKFLOW_PATH="${{ needs.generate-professional-workflow.outputs.workflow_path }}"
          
          # ğŸ”’ SECURITY VALIDATION FIRST (CRITICAL)
          echo "ğŸ”’ Checking for security issues..."
          SECURITY_ISSUES=0
          
          # Check for exposed OAuth tokens
          if grep -q 'CLAUDE_CODE_OAUTH_TOKEN:\s*sk-ant-' "$WORKFLOW_PATH" 2>/dev/null; then
            echo "ğŸš¨ CRITICAL: OAuth token exposed! Should use secrets.CLAUDE_CODE_OAUTH_TOKEN"
            SECURITY_ISSUES=$((SECURITY_ISSUES + 1))
          fi
          
          # Check for HEREDOC with GitHub Actions variables (causes YAML parsing errors)
          if awk '/cat.*<<.*EOF/,/^EOF$/' "$WORKFLOW_PATH" 2>/dev/null | grep -qE '\$\{\{'; then
            echo "ğŸš¨ CRITICAL: HEREDOC with GitHub Actions variables detected! This causes YAML parsing errors"
            echo "  Lines with issues:"
            awk '/cat.*<<.*EOF/,/^EOF$/' "$WORKFLOW_PATH" | grep -n '\$\{\{' | head -5
            SECURITY_ISSUES=$((SECURITY_ISSUES + 1))
          fi
          
          # Check for any exposed tokens or API keys (not using secrets)
          if grep -E '(api[_-]key|token|bearer|secret)\s*:\s*["\047]?[A-Za-z0-9_-]{20,}' "$WORKFLOW_PATH" 2>/dev/null | grep -v 'secrets\.' >/dev/null; then
            echo "ğŸš¨ CRITICAL: Exposed API keys or tokens detected!"
            SECURITY_ISSUES=$((SECURITY_ISSUES + 1))
          fi
          
          # Check for hardcoded passwords
          if grep -Ei 'password\s*:\s*["\047]' "$WORKFLOW_PATH" 2>/dev/null | grep -v 'secrets\.' >/dev/null; then
            echo "ğŸš¨ CRITICAL: Hardcoded passwords detected!"
            SECURITY_ISSUES=$((SECURITY_ISSUES + 1))
          fi
          
          # If security issues found, fail immediately
          if [ "$SECURITY_ISSUES" -gt 0 ]; then
            echo "âŒ SECURITY VALIDATION FAILED - $SECURITY_ISSUES critical issues found"
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "security_failed=true" >> $GITHUB_OUTPUT
            echo "error_count=$SECURITY_ISSUES" >> $GITHUB_OUTPUT
            echo "warning_count=0" >> $GITHUB_OUTPUT
            
            # Create security failure report
            echo '{' > validation_result.json
            echo '  "overall_result": "FAILED",' >> validation_result.json
            echo '  "security_validation": "FAILED",' >> validation_result.json
            echo '  "critical_security_issues": true,' >> validation_result.json
            echo '  "failed_items": [' >> validation_result.json
            echo '    "Exposed OAuth token - must use secrets.CLAUDE_CODE_OAUTH_TOKEN",' >> validation_result.json
            echo '    "Hardcoded credentials detected - use GitHub Secrets"' >> validation_result.json
            echo '  ],' >> validation_result.json
            echo '  "details": {' >> validation_result.json
            echo '    "security_validation": {' >> validation_result.json
            echo '      "status": "FAILED",' >> validation_result.json
            echo '      "message": "Critical security vulnerabilities detected. Workflow must not contain exposed tokens or credentials."' >> validation_result.json
            echo '    }' >> validation_result.json
            echo '  }' >> validation_result.json
            echo '}' >> validation_result.json
            exit 0
          fi
          
          echo "âœ… Security validation passed"
          
          # YAML syntax validation
          python -c "import yaml; yaml.safe_load(open('$WORKFLOW_PATH'))"
          echo "âœ… YAML syntax valid"
          
          # GitHub Actions structure validation (robust against quoted keys)
          python -c "import yaml,sys; wf_path='$WORKFLOW_PATH'; wf=yaml.safe_load(open(wf_path)) or {}; sys.exit(0 if isinstance(wf, dict) and 'name' in wf and 'on' in wf and 'jobs' in wf else 1)"
          if [ $? -eq 0 ]; then
            echo "âœ… GitHub Actions structure valid"

            # Input schema compliance validation (file-referenced, no inline duplication)
            if [ -f "artifacts/domain-input-schema/input-schema.yaml" ]; then
              python -c "import yaml,sys; wf_path='$WORKFLOW_PATH'; schema_path='artifacts/domain-input-schema/input-schema.yaml'; wf=yaml.safe_load(open(wf_path)) or {}; schema=yaml.safe_load(open(schema_path)) or {}; wf_inputs=(((wf.get('on') or {}).get('workflow_dispatch') or {}).get('inputs') or {}); groups=(schema.get('inputs') or {}); required=(groups.get('required') or {}); missing=[k for k in required.keys() if k not in wf_inputs]; (print('Missing inputs in workflow:', missing) or sys.exit(1)) if missing else None"
              echo "âœ… Input schema alignment (required): OK"
            else
              echo "::warning::No input schema file found for validation"
            fi

            # Static structural checks (non-domain specific)
            echo "ğŸ” Static structural checks..."
            python3 -c "import os, re, sys, yaml; wf_path='$WORKFLOW_PATH'; text=open(wf_path,'r',encoding='utf-8').read(); data=yaml.safe_load(text) or {}; jobs=data.get('jobs') or {}; warnings=[]; [warnings.append(f\"Local action reference detected in job '{job_name}': uses={uses}\") if uses.startswith(('./','../')) else warnings.append(f\"Suspicious action reference (missing @) in job '{job_name}': uses={uses}\") if '@' not in uses and not uses.startswith('docker://') else None for job_name,job in (jobs or {}).items() for step in (job or {}).get('steps',[]) or [] if isinstance(step,dict) and 'uses' in step for uses in [str(step['uses']).strip()]]; warnings.append('No \${PROJECT_DIR}/ occurrences found') if '\${PROJECT_DIR}/' not in text else None; warnings.append('actions/upload-artifact not found') if not re.search(r'actions/upload-artifact@',text) else None; warnings.append('actions/download-artifact not found') if not re.search(r'actions/download-artifact@',text) else None; [print(f'::warning ::{w}') for w in warnings]; print(f'Static checks: {len(warnings)} warning(s)')"
            
            # Domain-aware validation (generic, non-specialized)
            echo "ğŸ” Executing domain-aware validation (generic)..."
            DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
            CONSTRAINTS_PATH="meta/domain-templates/$DOMAIN/constraints.yaml"
            {
              echo "åŸºæœ¬æ¤œè¨¼ã¨ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆï¼‰ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¶ç´„ã®æ¤œè¨¼ã‚’è¡Œã„ã€validation_result.json ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚";
              echo;
              echo "å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: $WORKFLOW_PATH";
              echo "åˆ¶ç´„ãƒ•ã‚¡ã‚¤ãƒ«: $CONSTRAINTS_PATH (å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿)";
              echo;
              echo "åŸºæœ¬æ¤œè¨¼:";
              echo "- uses: ã§ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹å‚ç…§ãŒç„¡ã„ã“ã¨";
              echo "- ç”Ÿæˆç‰©ã¯ projects/ é…ä¸‹ã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨";
              echo "- ã‚¸ãƒ§ãƒ–é–“å…±æœ‰ã« artifacts ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨";
              echo "- ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‘ã‚¹ãŒç„¡ã„ã“ã¨ï¼ˆå‹•çš„å‚ç…§ã‚’ä½¿ç”¨ï¼‰";
              echo "- ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§è¤‡æ•°è¡Œæ–‡å­—åˆ—ãŒæ­£ã—ãå‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã“ã¨";
              echo "- video-productionãƒ‰ãƒ¡ã‚¤ãƒ³ã®å ´åˆ:";
              echo "  * å„ã‚·ãƒ¼ãƒ³ç”»åƒç”ŸæˆãŒç‹¬ç«‹ã‚¸ãƒ§ãƒ–ï¼ˆphase3-scene-N-imageï¼‰ã¨ã—ã¦å­˜åœ¨";
              echo "  * å„ã‚·ãƒ¼ãƒ³å‹•ç”»å¤‰æ›ãŒç‹¬ç«‹ã‚¸ãƒ§ãƒ–ï¼ˆphase4-scene-N-videoï¼‰ã¨ã—ã¦å­˜åœ¨";
              echo "  * phase4ãŒphase3ã®å¯¾å¿œã™ã‚‹ã‚·ãƒ¼ãƒ³ã«ä¾å­˜ï¼ˆneedsï¼‰ã—ã¦ã„ã‚‹ã“ã¨";
              echo "  * ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼ç”¨ã®å›ºå®šseedå€¤ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨";
              echo "  * ã‚·ãƒ¼ãƒ³æ•°ãŒå‹•çš„ã«è¨ˆç®—ã•ã‚ŒãŸå€¤ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨";
              echo;
              echo "åˆ¶ç´„æ¤œè¨¼ï¼ˆä»»æ„ï¼‰:";
              echo "- constraints.composition_rules ãŒã‚ã‚Œã°ã€ãã®è¦ä»¶ï¼ˆpipeline/matrix/max_parallel/duration_allocationï¼‰ã«æ²¿ã£ã¦ã„ã‚‹ã“ã¨";
              echo "- constraints.rule_references ã¨ checklist_references ãŒç¤ºã™ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †ã«èª­ã¿ã€MUSTã‚’å„ªå…ˆã—ã¦å¦¥å½“æ€§ã‚’ç¢ºèªã™ã‚‹ã“ã¨";
              echo "- ç›´åˆ—/ä¸¦åˆ—/matrix/å‘½å/ä¸€è²«æ€§/æ™‚é–“é…åˆ†ãªã©ã®è¦å‰‡ãŒã‚ã‚Œã°é©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨";
              echo;
              echo '{"overall_result":"PASSED","failed_items":[],"details":{}}';
            } > validation_prompt.txt
            
            # Replace placeholder with actual path
            sed -i "s|WORKFLOW_PATH_PLACEHOLDER|$WORKFLOW_PATH|g" validation_prompt.txt
            
            npx @anthropic-ai/claude-code \
              --mcp-config ".claude/mcp-kamuicode.json" \
              --allowedTools "Read,Write" \
              --permission-mode "acceptEdits" \
              --max-turns 30 \
              -p "$(cat validation_prompt.txt)"
            
            # Ensure validation_result.json exists (create minimal PASSED if missing)
            if [ ! -f "validation_result.json" ]; then
              echo "âš ï¸ validation_result.json not created - creating minimal PASSED report"
              echo '{"overall_result":"PASSED","failed_items":[],"details":{}}' > validation_result.json
            fi

            # Domain rule structural checks (non-blocking warnings)
            echo "ğŸ” Domain rule structural checks..."
            python3 -c "import os, re, sys, yaml, json; wf_path='$WORKFLOW_PATH'; domain='${{ needs.validate-and-detect.outputs.primary_domain }}'; constraints_path=f'meta/domain-templates/{domain}/constraints.yaml'; constraints=yaml.safe_load(open(constraints_path, 'r', encoding='utf-8')) if os.path.exists(constraints_path) else {}; rule_refs=constraints.get('rule_references') or []; rules_by_name={}; [rules_by_name.update({p: yaml.safe_load(open(p, 'r', encoding='utf-8')) if os.path.exists(p) and p.endswith(('.yaml','.yml')) else None}) for ref in rule_refs if (p:=(ref or {}).get('path'))]; text=open(wf_path, 'r', encoding='utf-8').read(); data=yaml.safe_load(text) or {}; jobs=data.get('jobs') or {}; warns=[]; orc=next((v for k,v in rules_by_name.items() if k.endswith('/rules/orchestration.yaml') and isinstance(v, dict)), None); matrix_key=(((orc.get('matrix') or {}).get('key')) or 'scene') if orc else 'scene'; max_parallel_lim=((orc.get('matrix') or {}).get('max_parallel')) if orc else None; found=any(matrix_key in ((job or {}).get('strategy') or {}).get('matrix', {}) for job in (jobs or {}).values() if isinstance(((job or {}).get('strategy') or {}).get('matrix'), dict)); warns.append(f\"No job with strategy.matrix containing key '{matrix_key}' found\") if orc and not found else None; cons=next((v for k,v in rules_by_name.items() if k.endswith('/rules/consistency.yaml') and isinstance(v, dict)), None); img_pat=((cons.get('naming') or {}).get('image_pattern')) if cons else None; vid_pat=((cons.get('naming') or {}).get('video_pattern')) if cons else None; warns.append('Scene image naming pattern not referenced in workflow text (scene_)') if img_pat and 'scene_' not in text else None; warns.append('Scene video naming pattern not referenced in workflow text (scene_)') if vid_pat and 'scene_' not in text else None; [print(f'::warning ::{w}') for w in warns]; print(f'Domain rule checks: {len(warns)} warning(s)')"
            
            # Check validation result from JSON
            if [ -f "validation_result.json" ]; then
              VALIDATION_RESULT=$(jq -r '.overall_result' validation_result.json 2>/dev/null || echo "FAILED")
              CRITICAL_PASS=$(jq -r '.critical_pass_count' validation_result.json 2>/dev/null || echo "0/10")
              FAILED_ITEMS=$(jq -r '.failed_items[]' validation_result.json 2>/dev/null || echo "Unknown")
              
              echo "ğŸ“Š Validation Result: $VALIDATION_RESULT"
              echo "ğŸ“Š Critical Requirements: $CRITICAL_PASS"
              
              if [ "$VALIDATION_RESULT" = "PASSED" ]; then
                echo "âœ… STRICT validation PASSED - All critical requirements met"
                echo "passed=true" >> $GITHUB_OUTPUT
                
                # Add detailed validation report to summary
                echo "### ğŸ¯ Strict Validation Report" >> $GITHUB_STEP_SUMMARY
                echo "- **Result**: âœ… PASSED" >> $GITHUB_STEP_SUMMARY
                echo "- **Critical Requirements**: $CRITICAL_PASS" >> $GITHUB_STEP_SUMMARY
                echo "" >> $GITHUB_STEP_SUMMARY
              else
                echo "âŒ STRICT validation FAILED - Critical requirements not met"
                echo "âŒ Failed items: $FAILED_ITEMS"
                
                # Add failure report to summary
                echo "### ğŸš¨ Strict Validation Report" >> $GITHUB_STEP_SUMMARY
                echo "- **Result**: âŒ FAILED" >> $GITHUB_STEP_SUMMARY
                echo "- **Critical Requirements**: $CRITICAL_PASS" >> $GITHUB_STEP_SUMMARY
                echo "- **Failed Items**:" >> $GITHUB_STEP_SUMMARY
                echo "$FAILED_ITEMS" | while read item; do
                  echo "  - $item" >> $GITHUB_STEP_SUMMARY
                done
                echo "" >> $GITHUB_STEP_SUMMARY
                echo "- **Action**: Attempting auto-fix..." >> $GITHUB_STEP_SUMMARY

                echo "ğŸ”§ Attempting generic auto-fix based on domain constraints..."

                # Build rule file list from constraints (if any) for precise autofix
                RULE_FILES=$(jq -r '.constraints.rule_references[]?.path' artifacts/domain-template-data/domain_summary.json 2>/dev/null || true)
                CHECKLIST_FILES=$(jq -r '.constraints.checklist_references[]?.path' artifacts/domain-template-data/domain_summary.json 2>/dev/null || true)

                echo "ğŸ”§ AI auto-fix with explicit domain rule inputs..."
                npx @anthropic-ai/claude-code \
                  --mcp-config ".claude/mcp-kamuicode.json" \
                  --allowedTools "Read,Write" \
                  --permission-mode "acceptEdits" \
                  --max-turns 40 \
                  -p "æ¬¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æ¤œè¨¼å¤±æ•—ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

                  å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: $WORKFLOW_PATH
                  æ¤œè¨¼çµæœ: validation_result.json
                  ãƒ‰ãƒ¡ã‚¤ãƒ³: $DOMAIN
                  åˆ¶ç´„: $CONSTRAINTS_PATH
                  ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:\n$RULE_FILES
                  ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆä¸€è¦§:\n$CHECKLIST_FILES

                  å¿…é ˆä¿®æ­£ï¼ˆMUSTï¼‰:
                  1) ãƒ­ãƒ¼ã‚«ãƒ« uses ã®æ’é™¤ / ã™ã¹ã¦ã®å‡ºåŠ›ã‚’ ${PROJECT_DIR} é…ä¸‹ã¸ / artifacts ã«ã‚ˆã‚‹å…±æœ‰
                  2) constraints.composition_rules / rules/orchestration.yaml ã«å¾“ã„ã€è©²å½“ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆ:
                     - per-item/perscene ç›´åˆ—ãƒã‚§ãƒ¼ãƒ³ï¼ˆä¾‹: generate_image -> image_to_videoï¼‰ã‚’åŒä¸€ã‚¸ãƒ§ãƒ–å†…ã§å®Ÿè¡Œ
                     - strategy.matrix ã‚’å°å…¥ã—ã€matrix.keyï¼ˆrulesã®matrix.keyï¼‰ã§ä¸¦åˆ—åŒ–
                     - strategy.max-parallel ã‚’ rulesã® max_parallel ä»¥ä¸‹ã«è¨­å®š
                  3) rules/consistency.yaml ã®å‘½åè¦ç´„ï¼ˆimage_pattern / video_patternï¼‰ã¨è§£åƒåº¦/éŸ³å£°åŸºæº–ã‚’æº€ãŸã™ã‚ˆã†å‘½åãƒ»è¨­å®š
                  4) paths: ${PROJECT_DIR}/media/{images|videos|audio}/ ãŠã‚ˆã³ ${PROJECT_DIR}/metadata/ ã«æ•´ãˆã‚‹

                  æ¨å¥¨ä¿®æ­£ï¼ˆSHOULDï¼‰:
                  - checklist ã® MUST/SHOULD ã®ã†ã¡ã€å®Ÿè£…å¯èƒ½ãªé …ç›®ã¯é©ç”¨

                  å¤‰æ›´ã¯ $WORKFLOW_PATH ã«ä¸Šæ›¸ãä¿å­˜ã€‚fix_summary.txt ã«ä¿®æ­£ç‚¹ã®è¦ç´„ï¼ˆé©ç”¨ã—ãŸãƒ«ãƒ¼ãƒ«ã€å¤‰æ›´ç®‡æ‰€ï¼‰ã‚’è¨˜éŒ²ã€‚"

                # Safety cap for max-parallel if still exceeding rule limit
                echo "Applying safety cap for max-parallel..."
              
                if [ $? -eq 0 ]; then
                  echo "âœ… Auto-fix completed - Re-validating..."
                  
                  # Re-validate after fix
                npx @anthropic-ai/claude-code \
                  --mcp-config ".claude/mcp-kamuicode.json" \
                  --allowedTools "Read,Write" \
                  --permission-mode "acceptEdits" \
                  --max-turns 10 \
                  -p "ä¿®æ­£å¾Œã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å†æ¤œè¨¼ã—ã¦ãã ã•ã„ã€‚

                  å¯¾è±¡: $WORKFLOW_PATH
                  ãƒ‰ãƒ¡ã‚¤ãƒ³: $DOMAIN
                  åˆ¶ç´„: $CONSTRAINTS_PATHï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰
                  
                  1) åŸºæœ¬æ¤œè¨¼ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«usesç¦æ­¢ã€artifactsåˆ©ç”¨ã€æ§‹é€ å¦¥å½“æ€§ï¼‰
                  2) åˆ¶ç´„ãŒã‚ã‚‹å ´åˆã®ã¿ composition_rules ã®é©ç”¨ç¢ºèª
                  çµæœã‚’ revalidation_result.json ã«ä¿å­˜"
                  
                  if [ -f "revalidation_result.json" ]; then
                    REVALIDATION_RESULT=$(jq -r '.overall_result' revalidation_result.json 2>/dev/null || echo "FAILED")
                    if [ "$REVALIDATION_RESULT" = "PASSED" ]; then
                      echo "âœ… Re-validation PASSED - All issues fixed"
                      echo "passed=true" >> $GITHUB_OUTPUT
                    else
                      echo "âŒ Re-validation FAILED - Some issues remain"
                      echo "passed=false" >> $GITHUB_OUTPUT
                    fi
                  else
                    echo "âŒ Re-validation failed to complete"
                    echo "passed=false" >> $GITHUB_OUTPUT
                  fi
                else
                  echo "âŒ Auto-fix failed - manual intervention required"
                  echo "passed=false" >> $GITHUB_OUTPUT
                fi
              fi
            else
              echo "âŒ Validation result file not found"
              echo "passed=false" >> $GITHUB_OUTPUT
            fi
            
            # Add Phase 6 Report with detailed validation results
            echo "## ğŸ” Phase 6: è©³ç´°æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… YAMLæ§‹æ–‡æ¤œè¨¼: æ­£å¸¸" >> $GITHUB_STEP_SUMMARY
            echo "- âœ… GitHub Actionsæ§‹é€ æ¤œè¨¼: æ­£å¸¸" >> $GITHUB_STEP_SUMMARY
            echo "- ğŸ“Š ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–æ¤œè¨¼: å®Œäº†" >> $GITHUB_STEP_SUMMARY
            
            # Add detailed error/warning counts
            if [ -f "validation_result.json" ]; then
              ERROR_COUNT=$(jq -r '.errors | length' validation_result.json 2>/dev/null || echo "0")
              WARNING_COUNT=$(jq -r '.warnings | length' validation_result.json 2>/dev/null || echo "0")
              echo "- ğŸš¨ ã‚¨ãƒ©ãƒ¼æ•°: $ERROR_COUNT" >> $GITHUB_STEP_SUMMARY
              echo "- âš ï¸ è­¦å‘Šæ•°: $WARNING_COUNT" >> $GITHUB_STEP_SUMMARY
              
              # Output counts for next job
              echo "error_count=$ERROR_COUNT" >> $GITHUB_OUTPUT
              echo "warning_count=$WARNING_COUNT" >> $GITHUB_OUTPUT
              echo "report_path=validation_result.json" >> $GITHUB_OUTPUT
            fi
            echo "" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Invalid GitHub Actions structure"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Upload Validation Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-report
          path: |
            validation_result.json
            revalidation_result.json
          if-no-files-found: warn
          
      - name: Generate Detailed Validation Report
        id: detailed-report
        if: always()
        run: |
          echo "ğŸ“Š Generating detailed validation report..."
          
          WORKFLOW_PATH="${{ needs.generate-professional-workflow.outputs.workflow_path }}"
          WORKFLOW_NAME="${{ needs.generate-professional-workflow.outputs.workflow_name }}"
          DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          
          # Create comprehensive validation report
          npx @anthropic-ai/claude-code \
            --mcp-config ".claude/mcp-kamuicode.json" \
            --allowedTools "Read,Write" \
            --permission-mode "acceptEdits" \
            --max-turns 20 \
            -p "è©³ç´°ãªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
            
            å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: $WORKFLOW_PATH
            ãƒ‰ãƒ¡ã‚¤ãƒ³: $DOMAIN
            
            æ¤œè¨¼é …ç›®:
            1. æ±ç”¨çš„ãªå•é¡Œ:
               - HEREDOC + GitHub Actionså¤‰æ•°ã®æ··åœ¨
               - ãƒ­ãƒ¼ã‚«ãƒ« uses: å‚ç…§
               - ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‘ã‚¹
               - artifactã®ä¸é©åˆ‡ãªä½¿ç”¨
            
            2. ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ã®å•é¡Œ ($DOMAIN):
               - ã‚­ãƒ£ã‚¹ã‚¿ãƒ¼åˆ†é›¢ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…
               - 5ç§’ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯
               - èƒŒæ™¯ã«äººç‰©ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹
            
            3. æ§‹é€ çš„ãªå•é¡Œ:
               - ã‚¸ãƒ§ãƒ–ä¾å­˜é–¢ä¿‚ã®æ­£ã—ã•
               - matrixã®ä½¿ç”¨æ–¹æ³•
               - ãƒ‘ãƒ©ãƒ¬ãƒ«å®Ÿè¡Œã®æœ€é©æ€§
            
            ãƒ¬ãƒãƒ¼ãƒˆã‚’ detailed_validation_report.json ã«ä¿å­˜:
            {
              \"total_errors\": 0,
              \"total_warnings\": 0,
              \"critical_issues\": [],
              \"generic_issues\": [],
              \"domain_specific_issues\": [],
              \"structural_issues\": [],
              \"fix_priority\": [],
              \"auto_fixable\": [],
              \"manual_fix_required\": []
            }"
          
      - name: Post Validation Summary to Issue
        if: always()
        run: |
          # Use input parameter for workflow_dispatch, or job output for issue_comment
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            ISSUE_NUMBER="${{ inputs.issue_number }}"
          else
            ISSUE_NUMBER="${{ needs.validate-and-detect.outputs.issue_number }}"
          fi
          WORKFLOW_NAME="${{ needs.generate-professional-workflow.outputs.workflow_name }}"
          DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          VALIDATION_RESULT="${{ steps.validate.outputs.passed }}"
          
          if [ "$VALIDATION_RESULT" = "true" ]; then
            gh issue comment "$ISSUE_NUMBER" --body "## âœ… Workflow Generation & Validation Complete!
            
            **Workflow**: \`$WORKFLOW_NAME\`
            **Domain**: $DOMAIN
            **Validation**: âœ… PASSED
            
            ### ğŸ¯ All checks passed:
            - YAML syntax valid
            - GitHub Actions structure correct
            - Domain-specific requirements met
            
            ### ğŸ“¦ Download:
            Download the validated workflow from run artifacts.
            
            ---
            *Meta Workflow v12*"
          else
            ERROR_COUNT="${{ steps.validate.outputs.error_count }}"
            WARNING_COUNT="${{ steps.validate.outputs.warning_count }}"
            
            gh issue comment "$ISSUE_NUMBER" --body "## âš ï¸ Workflow Validation Failed
            
            **Workflow**: \`$WORKFLOW_NAME\`
            **Domain**: $DOMAIN
            **Validation**: âŒ FAILED
            
            ### ğŸ’¡ Issues Found:
            - **Errors**: $ERROR_COUNT
            - **Warnings**: $WARNING_COUNT
            
            ### ğŸ”„ Next Action:
            Triggering automatic regeneration with fixes...
            
            ---
            *Check the regeneration job for updated workflow*"
          fi
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # ===========================================
  # PHASE 7: ERROR FIX LOOP (IF NEEDED)
  # ===========================================
  
  regeneration-loop:
    name: "ğŸ”§ ã‚¨ãƒ©ãƒ¼ä¿®æ­£ãƒ«ãƒ¼ãƒ—"
    runs-on: ubuntu-latest
    needs: ['validate-workflow', 'validate-and-detect', 'load-domain-templates']
    if: |
      always() && 
      needs.validate-workflow.result == 'success' &&
      needs.validate-workflow.outputs.validation_passed == 'false'
    outputs:
      regeneration_attempt: ${{ steps.attempt.outputs.regeneration_attempt }}
      regeneration_success: ${{ steps.fix-workflow.outputs.success }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        
      - name: Setup Environment
        run: |
          npm install -g @anthropic-ai/claude-code
          pip install pyyaml
          
      - name: Download All Previous Artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
          
      - name: Record Fix Attempt
        id: attempt
        run: |
          echo "ğŸ”§ æ¤œè¨¼å¤±æ•—ã«ã‚ˆã‚‹ã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚’é–‹å§‹..."
          ATTEMPT_COUNT=1
          echo "regeneration_attempt=$ATTEMPT_COUNT" >> $GITHUB_OUTPUT
          
          echo "## ğŸ”§ Phase 7: ã‚¨ãƒ©ãƒ¼ä¿®æ­£ãƒ«ãƒ¼ãƒ—" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸš¨ åˆå›ç”Ÿæˆã®æ¤œè¨¼å¤±æ•—ã‚’æ¤œå‡º" >> $GITHUB_STEP_SUMMARY
          echo "- ğŸ”§ ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆä¿®æ­£ã‚’å®Ÿè¡Œä¸­..." >> $GITHUB_STEP_SUMMARY
          
      - name: Analyze Validation Failures
        id: analyze
        run: |
          echo "ğŸ” æ¤œè¨¼å¤±æ•—ã®è©³ç´°åˆ†æ..."
          DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          
          # Extract specific failure reasons from previous validation
          npx @anthropic-ai/claude-code \
            --mcp-config ".claude/mcp-kamuicode.json" \
            --allowedTools "Read,Write" \
            --permission-mode "acceptEdits" \
            --max-turns 20 \
            -p "æ¤œè¨¼å¤±æ•—ã®åŸå› ã‚’è©³ç´°åˆ†æã—ã¦ãã ã•ã„:
            
          å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«:
          1. artifacts/validation-report/detailed_validation_report.json (è©³ç´°æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ)
          2. artifacts/validation-report/validation_result.json (åŸºæœ¬æ¤œè¨¼çµæœ)
          3. projects/workflow-execution-logs/meta-workflow-construction-checklist.md (æ±ç”¨å¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³)
          4. meta/domain-templates/$DOMAIN/checklists/ (ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ)
          5. docs/CLAUDE_CODE_DATA_PERSISTENCE_GUIDE.md (ãƒ‡ãƒ¼ã‚¿æ°¸ç¶šåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³)
          6. docs/YAML_CONSTRUCTION_GUIDELINES.md (YAMLæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³)
          
          åˆ†æé …ç›®:
          - HEREDOCã¨GitHub Actionså¤‰æ•°ã®æ··åœ¨å•é¡Œ
          - ãƒ­ãƒ¼ã‚«ãƒ«uses:å‚ç…§ã®å•é¡Œ
          - ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰å•é¡Œ
          - ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆå…±æœ‰ã®å•é¡Œ
          - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®è¦ä»¶é•åï¼ˆã‚­ãƒ£ã‚¹ã‚¿ãƒ¼åˆ†é›¢ã€ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç­‰ï¼‰
          
          åˆ†æçµæœã‚’artifacts/failure_analysis.jsonã«ä¿å­˜ã—ã¦ãã ã•ã„:
          {
            \"failure_reasons\": [\"åŸå› 1\", \"åŸå› 2\"],
            \"critical_issues\": [\"é‡è¦ãªå•é¡Œ1\", \"é‡è¦ãªå•é¡Œ2\"],
            \"improvement_strategy\": \"æ”¹å–„æˆ¦ç•¥ã®è©³ç´°\",
            \"regeneration_focus\": [\"å†ç”Ÿæˆã§ç‰¹ã«æ³¨æ„ã™ã¹ãç‚¹1\", \"ç‚¹2\"],
            \"heredoc_issues\": [\"HEREDOCå•é¡Œã®ã‚ã‚‹è¡Œç•ªå·\"],
            \"local_uses_issues\": [\"ãƒ­ãƒ¼ã‚«ãƒ«useså‚ç…§ã®å ´æ‰€\"],
            \"domain_specific_issues\": [\"ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®å•é¡Œ\"]
          }"
          
          if [ -f "artifacts/failure_analysis.json" ]; then
            echo "âœ… å¤±æ•—åˆ†æå®Œäº†"
            echo "- âœ… å¤±æ•—åŸå› ã®ç‰¹å®šå®Œäº†" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ å¤±æ•—åˆ†æã«å¤±æ•— - Fallbackã‚’ä½œæˆ"
            # Fallback: use validation report to craft minimal analysis
            REPORT_DIR="artifacts/validation-report"
            REPORT_FILE=""
            if [ -f "$REPORT_DIR/validation_result.json" ]; then
              REPORT_FILE="$REPORT_DIR/validation_result.json"
            elif [ -f "validation_result.json" ]; then
              REPORT_FILE="validation_result.json"
            fi
            mkdir -p artifacts
            if [ -n "$REPORT_FILE" ]; then
              jq -n --argjson failed "$(jq -r '.failed_items // []' "$REPORT_FILE" 2>/dev/null || echo '[]')" '{failure_reasons:$failed, critical_issues:$failed, improvement_strategy:"Auto-fallback from validation_result.json", regeneration_focus:$failed}' > artifacts/failure_analysis.json || echo '{"failure_reasons":[],"critical_issues":[],"improvement_strategy":"fallback","regeneration_focus":[]}' > artifacts/failure_analysis.json
              echo "- âš ï¸ Fallback failure_analysis.json ã‚’ä½œæˆ" >> $GITHUB_STEP_SUMMARY
            else
              echo '{"failure_reasons":[],"critical_issues":[],"improvement_strategy":"no-report","regeneration_focus":[]}' > artifacts/failure_analysis.json
              echo "- âš ï¸ æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆæœªæ¤œå‡ºã®ãŸã‚ç©ºã®åˆ†æã‚’ä½œæˆ" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
      - name: Fix Workflow Errors (Pinpoint Fixes)
        id: fix-workflow
        run: |
          echo "ğŸ”§ ã‚¨ãƒ©ãƒ¼ç®‡æ‰€ã®ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆä¿®æ­£ã‚’å®Ÿè¡Œ..."
          DOMAIN="${{ needs.validate-and-detect.outputs.primary_domain }}"
          ISSUE_NUMBER="${{ needs.validate-and-detect.outputs.issue_number }}"
          PROJECT_DIR="${{ needs.generate-professional-workflow.outputs.project_dir }}"
          
          # Get the previously generated workflow path
          PREVIOUS_WORKFLOW=$(find artifacts/generated-workflow -name "*.yml" -type f | head -1)
          
          if [ -z "$PREVIOUS_WORKFLOW" ]; then
            echo "âŒ å‰å›ç”Ÿæˆã—ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            exit 1
          fi
          
          echo "ğŸ“ ä¿®æ­£å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: $PREVIOUS_WORKFLOW"
          
          # Create fix directory
          FIX_DIR="projects/issue-$ISSUE_NUMBER-fixes-$(date +%Y%m%d-%H%M%S)"
          mkdir -p "$FIX_DIR/logs"
          
          # Copy original workflow for backup
          cp "$PREVIOUS_WORKFLOW" "$FIX_DIR/original-workflow.yml"
          
          # Fix loop - up to 5 iterations for thorough fixes
          MAX_FIX_ATTEMPTS=5
          FIX_SUCCESS=false
          
          for fix_attempt in $(seq 1 $MAX_FIX_ATTEMPTS); do
            echo "ğŸ”„ ä¿®æ­£è©¦è¡Œ $fix_attempt/$MAX_FIX_ATTEMPTS"
            
            # Collect detailed error information before fix
            echo "ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’åé›†ä¸­..."
            ERROR_DETAILS=""
            
            # Check for HEREDOC with GitHub Actions variables (HIGHEST PRIORITY)
            if awk '/cat.*<<.*EOF/,/^EOF$/' "$PREVIOUS_WORKFLOW" 2>/dev/null | grep -qE '\$\{\{'; then
              HEREDOC_LINES=$(awk '/cat.*<<.*EOF/,/^EOF$/' "$PREVIOUS_WORKFLOW" | grep -n '\$\{\{' | head -5)
              ERROR_DETAILS="${ERROR_DETAILS}
            HEREDOCå†…ã®GitHub Actionså¤‰æ•°ï¼ˆYAMLæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®åŸå› ï¼‰:
            ${HEREDOC_LINES}"
              echo "ğŸš¨ HEREDOC with GitHub Actions variables detected"
            fi
            
            # Check for exposed tokens with line numbers
            if grep -n 'sk-ant-' "$PREVIOUS_WORKFLOW" 2>/dev/null; then
              TOKEN_LINES=$(grep -n 'sk-ant-' "$PREVIOUS_WORKFLOW" | head -3)
              ERROR_DETAILS="${ERROR_DETAILS}
            éœ²å‡ºã—ãŸOAuthãƒˆãƒ¼ã‚¯ãƒ³:
            ${TOKEN_LINES}"
            fi
            
            # Check for hardcoded paths with line numbers
            if grep -n -E '"/home/runner/work/[^"]*"' "$PREVIOUS_WORKFLOW" 2>/dev/null | grep -vF '${' | grep -q .; then
              PATH_LINES=$(grep -n -E '"/home/runner/work/[^"]*"' "$PREVIOUS_WORKFLOW" | grep -vF '${' | head -3)
              ERROR_DETAILS="${ERROR_DETAILS}
            ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ‘ã‚¹:
            ${PATH_LINES}"
            fi
            
            # Apply pinpoint fixes with detailed error information
            npx @anthropic-ai/claude-code \
              --mcp-config ".claude/mcp-kamuicode.json" \
              --allowedTools "Read,Write,Edit,MultiEdit" \
              --permission-mode "acceptEdits" \
              --max-turns 40 \
              -p "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ã‚¨ãƒ©ãƒ¼ã‚’ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆä¿®æ­£ã—ã¦ãã ã•ã„:
              
            å¯¾è±¡ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: $PREVIOUS_WORKFLOW
            
            æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ï¼ˆè¡Œç•ªå·ä»˜ãï¼‰:
            $ERROR_DETAILS
            
            ã€é‡è¦ã€‘ä¿®æ­£æ‰‹é †ï¼ˆå¿…ãšã“ã®é †ç•ªã§å®Ÿè¡Œï¼‰:
            1. Read ãƒ„ãƒ¼ãƒ«ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€: $PREVIOUS_WORKFLOW
            2. ä»¥ä¸‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã™ã¹ã¦æ¤œç´¢ã—ã¦ä¿®æ­£:
               - sk-ant-ã§å§‹ã¾ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ â†’ \${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
               - /home/runner/work/kamuicode_meta/kamuicode_meta/ â†’ \${{ github.workspace }}/
               - HEREDOCã¨GitHub Actionså¤‰æ•° â†’ echoã‚³ãƒãƒ³ãƒ‰ã«å¤‰æ›
               - uses: ./ â†’ ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å®Ÿè£…
               - outputs:.*matrix\\. â†’ æ­£ã—ã„æ§‹æ–‡ã«ä¿®æ­£
            3. MultiEdit ã¾ãŸã¯ Edit ãƒ„ãƒ¼ãƒ«ã§ä¿®æ­£ã‚’é©ç”¨
            4. Write ãƒ„ãƒ¼ãƒ«ã§ä¿®æ­£å†…å®¹ã‚’ä¿å­˜: $PREVIOUS_WORKFLOWï¼ˆå¿…é ˆï¼‰
            5. Write ãƒ„ãƒ¼ãƒ«ã§ã‚µãƒãƒªãƒ¼ä¿å­˜: $FIX_DIR/logs/fix_attempt_${fix_attempt}.txt
            
            é‡è¦ãªåˆ¶ç´„:
            - æ—¢å­˜ã®æ©Ÿèƒ½ã‚„ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šé™¤ã—ãªã„
            - ã‚¸ãƒ§ãƒ–ã®æ§‹é€ ã‚’ç¶­æŒã™ã‚‹
            - å¿…è¦ãªå‡¦ç†ã¯å…¨ã¦æ®‹ã™
            - å¿…ãšWriteãƒ„ãƒ¼ãƒ«ã§æ˜ç¤ºçš„ã«ä¿å­˜ã™ã‚‹
            
            å‚ç…§ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆå¿…ãšèª­ã‚“ã§ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ï¼‰:
            - docs/WORKFLOW_FIX_PATTERNS.md ï¼ˆä¿®æ­£ãƒ‘ã‚¿ãƒ¼ãƒ³é›† - æœ€å„ªå…ˆï¼‰
            - docs/YAML_CONSTRUCTION_GUIDELINES.md
            - docs/CLAUDE_CODE_DATA_PERSISTENCE_GUIDE.md"
            CLI_STATUS=$?
            
            # Log Claude Code execution status
            echo "ğŸ“ Claude Codeå®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: $CLI_STATUS"
            
            if [ $CLI_STATUS -eq 0 ]; then
              echo "âœ… Claude Codeå®Ÿè¡ŒæˆåŠŸ"
              # Don't break yet - verify fixes first
            else
              echo "âŒ Claude Codeå®Ÿè¡Œå¤±æ•—"
              continue
            fi
            
            # Re-validate after fix
            echo "ğŸ” ä¿®æ­£å¾Œã®å†æ¤œè¨¼..."
            python3 -c "import yaml; yaml.safe_load(open('$PREVIOUS_WORKFLOW'))" 2>/dev/null
            if [ $? -eq 0 ]; then
              echo "âœ… YAMLæ§‹æ–‡: æ­£å¸¸"
              
              # Check for common issues with more accurate patterns
              ERROR_COUNT=0
              
              # Check for HEREDOC with GitHub Actions variables (more precise check)
              # Look for HEREDOC blocks that contain dollar-brace within them
              if awk '/cat.*<<.*EOF/,/^EOF$/' "$PREVIOUS_WORKFLOW" | grep -qE '\$\{|github\.|needs\.|matrix\.'; then
                echo "âš ï¸ HEREDOCã¨GitHub Actionså¤‰æ•°ã®æ··åœ¨ãŒã¾ã å­˜åœ¨"
                ERROR_COUNT=$((ERROR_COUNT + 1))
              fi
              
              # Check for local uses references
              if grep -q 'uses:.*\./' "$PREVIOUS_WORKFLOW"; then
                echo "âš ï¸ ãƒ­ãƒ¼ã‚«ãƒ«useså‚ç…§ãŒã¾ã å­˜åœ¨"
                ERROR_COUNT=$((ERROR_COUNT + 1))
              fi
              
              # Check for hardcoded absolute paths (more precise pattern)
              # Look for paths that are not using GitHub Actions variables
              if grep -E '"/home/runner/work/[^"]*"' "$PREVIOUS_WORKFLOW" 2>/dev/null | grep -vF '${' | grep -q .; then
                echo "âš ï¸ ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸçµ¶å¯¾ãƒ‘ã‚¹ãŒã¾ã å­˜åœ¨"
                echo "  æ¤œå‡ºã•ã‚ŒãŸè¡Œ:"
                grep -n -E '"/home/runner/work/[^"]*"' "$PREVIOUS_WORKFLOW" | grep -vF '${' | head -3
                ERROR_COUNT=$((ERROR_COUNT + 1))
              fi
              
              if [ $ERROR_COUNT -eq 0 ]; then
                echo "âœ… ä¿®æ­£æˆåŠŸ - å…¨ã¦ã®ã‚¨ãƒ©ãƒ¼ãŒè§£æ±º"
                FIX_SUCCESS=true
                break
              else
                echo "âš ï¸ ã¾ã  $ERROR_COUNT å€‹ã®ã‚¨ãƒ©ãƒ¼ãŒæ®‹ã£ã¦ã„ã¾ã™"
                if [ $fix_attempt -lt $MAX_FIX_ATTEMPTS ]; then
                  echo "ğŸ”„ æ¬¡ã®ä¿®æ­£è©¦è¡Œã‚’æº–å‚™..."
                  sleep 5
                fi
              fi
            else
              echo "âŒ YAMLæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ãŒæ®‹ã£ã¦ã„ã¾ã™"
              if [ $fix_attempt -lt $MAX_FIX_ATTEMPTS ]; then
                echo "ğŸ”„ æ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã—ã¦å†è©¦è¡Œ..."
              fi
            fi
          done
          
          # Final verification and result
          if [ "$FIX_SUCCESS" = "true" ]; then
            # Double-check that critical errors are actually fixed
            FINAL_VERIFICATION_PASSED=true
            echo "ğŸ” æœ€çµ‚æ¤œè¨¼ã‚’å®Ÿè¡Œä¸­..."
            
            # Check for exposed OAuth tokens
            if grep -q 'sk-ant-' "$PREVIOUS_WORKFLOW" 2>/dev/null; then
              echo "âŒ æœ€çµ‚æ¤œè¨¼å¤±æ•—: OAuthãƒˆãƒ¼ã‚¯ãƒ³ãŒã¾ã éœ²å‡ºã—ã¦ã„ã¾ã™"
              FINAL_VERIFICATION_PASSED=false
            fi
            
            # Check for hardcoded paths
            if grep -E '"/home/runner/work/[^"]*"' "$PREVIOUS_WORKFLOW" 2>/dev/null | grep -vF '${' | grep -q .; then
              echo "âŒ æœ€çµ‚æ¤œè¨¼å¤±æ•—: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ãŒã¾ã å­˜åœ¨ã—ã¾ã™"
              grep -n -E '"/home/runner/work/[^"]*"' "$PREVIOUS_WORKFLOW" | grep -vF '${' | head -3
              FINAL_VERIFICATION_PASSED=false
            fi
            
            if [ "$FINAL_VERIFICATION_PASSED" = "true" ]; then
              echo "âœ… ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¿®æ­£å®Œäº†"
              echo "success=true" >> $GITHUB_OUTPUT
              
              # Copy fixed workflow to output location
              cp "$PREVIOUS_WORKFLOW" "$FIX_DIR/fixed-workflow.yml"
              
              echo "- âœ… ãƒ”ãƒ³ãƒã‚¤ãƒ³ãƒˆä¿®æ­£å®Œäº†" >> $GITHUB_STEP_SUMMARY
              echo "- âœ… å…¨ã‚¨ãƒ©ãƒ¼è§£æ±º" >> $GITHUB_STEP_SUMMARY
              echo "- ğŸ“ ä¿®æ­£æ¸ˆã¿ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: $FIX_DIR/fixed-workflow.yml" >> $GITHUB_STEP_SUMMARY
            else
              echo "âš ï¸ ä¿®æ­£ã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸãŒã€æ¤œè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ"
              echo "success=partial" >> $GITHUB_OUTPUT
              
              # Still copy the workflow for analysis
              cp "$PREVIOUS_WORKFLOW" "$FIX_DIR/fixed-workflow.yml"
              
              echo "- âš ï¸ ä¿®æ­£è©¦è¡Œã¯å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ" >> $GITHUB_STEP_SUMMARY
              echo "- âŒ æœ€çµ‚æ¤œè¨¼ã§ä¸€éƒ¨ã®ã‚¨ãƒ©ãƒ¼ãŒæ®‹å­˜" >> $GITHUB_STEP_SUMMARY
              echo "- ğŸ“ éƒ¨åˆ†ä¿®æ­£ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼: $FIX_DIR/fixed-workflow.yml" >> $GITHUB_STEP_SUMMARY
              echo "- ğŸ” æ‰‹å‹•ã§ã®è¿½åŠ ä¿®æ­£ãŒå¿…è¦ã§ã™" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "âŒ ä¿®æ­£ãŒå®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸ"
            echo "success=false" >> $GITHUB_OUTPUT
            echo "- âŒ ä¸€éƒ¨ã®ã‚¨ãƒ©ãƒ¼ãŒè§£æ±ºã§ãã¾ã›ã‚“ã§ã—ãŸ" >> $GITHUB_STEP_SUMMARY
            echo "- ğŸ“ æ‰‹å‹•ã§ã®ç¢ºèªãŒå¿…è¦ã§ã™" >> $GITHUB_STEP_SUMMARY
          fi
          
      - name: Upload Fixed Workflow
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: fixed-workflow
          path: |
            projects/issue-${{ needs.validate-and-detect.outputs.issue_number }}-fixes-*/fixed-workflow.yml
            projects/issue-${{ needs.validate-and-detect.outputs.issue_number }}-fixes-*/logs/
          if-no-files-found: warn

  # ===========================================
  # PHASE 8: FINAL REPORT DISPLAY
  # ===========================================
  
  display-final-report:
    name: "ğŸ“Š å®Ÿè¡Œå®Œäº†"
    runs-on: ubuntu-latest
    needs: ['validate-and-detect', 'load-domain-templates', 'professional-task-decomposition', 'optimize-task-order', 'generate-professional-workflow', 'validate-workflow', 'regeneration-loop']
    if: always()
    steps:
          
      - name: Add Completion Summary
        run: |
          echo "ğŸ“Š å®Ÿè¡Œå®Œäº†ã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ ä¸­..."
          
          # åŸºæœ¬æƒ…å ±
          ISSUE_NUMBER="${{ needs.validate-and-detect.outputs.issue_number }}"
          VALIDATION_STATUS="${{ needs.validate-workflow.outputs.validation_passed }}"
          
          # å®Ÿè¡Œå®Œäº†ã‚µãƒãƒªãƒ¼ã‚’è¿½åŠ 
          echo "## ğŸ‰ å®Ÿè¡Œå®Œäº†" >> $GITHUB_STEP_SUMMARY
          echo "- **å®Œäº†æ™‚åˆ»**: $(date '+%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')" >> $GITHUB_STEP_SUMMARY
          
          # å†ç”Ÿæˆçµæœã®ç¢ºèª
          REGENERATION_SUCCESS="${{ needs.regeneration-loop.outputs.regeneration_success }}"
          REGENERATION_ATTEMPTED="${{ needs.regeneration-loop.outputs.regeneration_attempt }}"
          
          if [ "$VALIDATION_STATUS" = "true" ]; then
            echo "- **å…¨ä½“å®Ÿè¡Œçµæœ**: âœ… æˆåŠŸï¼ˆåˆå›ç”Ÿæˆï¼‰" >> $GITHUB_STEP_SUMMARY
          elif [ "$REGENERATION_SUCCESS" = "true" ]; then
            echo "- **å…¨ä½“å®Ÿè¡Œçµæœ**: âœ… æˆåŠŸï¼ˆå†ç”Ÿæˆã«ã‚ˆã‚Šä¿®æ­£ï¼‰" >> $GITHUB_STEP_SUMMARY
            echo "- **å†ç”Ÿæˆå®Ÿè¡Œ**: âœ… å®Œäº†ï¼ˆæ¤œè¨¼å¤±æ•—ã‚’è‡ªå‹•ä¿®æ­£ï¼‰" >> $GITHUB_STEP_SUMMARY
          elif [ "$REGENERATION_ATTEMPTED" = "1" ]; then
            echo "- **å…¨ä½“å®Ÿè¡Œçµæœ**: âŒ å¤±æ•—ï¼ˆå†ç”Ÿæˆã§ã‚‚ä¿®æ­£ä¸å¯ï¼‰" >> $GITHUB_STEP_SUMMARY
            echo "- **å†ç”Ÿæˆå®Ÿè¡Œ**: âŒ å¤±æ•—ï¼ˆæ‰‹å‹•å¯¾å¿œãŒå¿…è¦ï¼‰" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **å…¨ä½“å®Ÿè¡Œçµæœ**: âš ï¸ ä¸€éƒ¨ã‚¨ãƒ©ãƒ¼ã‚ã‚Š" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æƒ…å ±
          cat >> $GITHUB_STEP_SUMMARY << EOF
          ## ğŸ“¥ æˆæœç‰©ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          
          ãƒ­ãƒ¼ã‚«ãƒ«ã§ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š
          
          \`\`\`bash
          # ã™ã¹ã¦ã®æˆæœç‰©ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          gh run download ${{ github.run_id }}
          
          # ç‰¹å®šã®æˆæœç‰©ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
          gh run download ${{ github.run_id }} -n generated-workflow
          EOF
          
          # å†ç”Ÿæˆã•ã‚ŒãŸå ´åˆã®è¿½åŠ æƒ…å ±
          if [ "$REGENERATION_SUCCESS" = "true" ]; then
            cat >> $GITHUB_STEP_SUMMARY << EOF
          
          # å†ç”Ÿæˆç‰ˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚‚ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ¨å¥¨ï¼‰
          gh run download ${{ github.run_id }} -n regenerated-workflow
          EOF
          fi
          
          cat >> $GITHUB_STEP_SUMMARY << EOF
          \`\`\`
          EOF
