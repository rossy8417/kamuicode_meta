#!/usr/bin/env python3
"""
Dynamic Workflow Assembler
é¸æŠã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰ã‹ã‚‰GitHub Actionsãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å‹•çš„ç”Ÿæˆ
"""

import json
import yaml
import os
from typing import Dict, List, Any, Optional
from pathlib import Path
# TaskNodeExtractorã‚¯ãƒ©ã‚¹ã‚’ç›´æ¥çµ„ã¿è¾¼ã¿
import re

class DynamicWorkflowAssembler:
    def __init__(self, node_database_path: str = ".meta/task-nodes.json"):
        self.node_database_path = node_database_path
        self.task_nodes = {}
        self.capabilities_index = {}
        self.load_node_database()
        
    def load_node_database(self):
        """ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
        if os.path.exists(self.node_database_path):
            with open(self.node_database_path, 'r', encoding='utf-8') as f:
                database = json.load(f)
                self.task_nodes = database.get('task_nodes', {})
                self.capabilities_index = database.get('capabilities_index', {})
        else:
            print(f"âš ï¸ Node database not found: {self.node_database_path}")
    
    def create_workflow_from_requirements(self, 
                                        requirements: List[str], 
                                        workflow_name: str = "dynamic-multimedia-workflow",
                                        description: str = "Dynamically generated multimedia workflow") -> Dict[str, Any]:
        """è¦æ±‚ã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å‹•çš„ç”Ÿæˆ"""
        
        # ãƒãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ç›´æ¥é¸æŠ
        selected_nodes = self.find_nodes_for_requirements(requirements)
        execution_stages = self.generate_dependency_order(selected_nodes)
        
        print(f"ğŸš€ Creating workflow: {workflow_name}")
        print(f"ğŸ“‹ Selected {len(selected_nodes)} task nodes")
        print(f"âš¡ {len(execution_stages)} execution stages")
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åŸºæœ¬æ§‹é€ 
        workflow = {
            'name': workflow_name,
            'on': {
                'workflow_dispatch': {
                    'inputs': {
                        'user_prompt': {
                            'description': 'User requirements for multimedia generation',
                            'required': True,
                            'type': 'string',
                            'default': ' | '.join(requirements)
                        }
                    }
                },
                'issues': {
                    'types': ['opened', 'edited']
                }
            },
            'permissions': {
                'contents': 'write',
                'actions': 'write',
                'issues': 'write',
                'pull-requests': 'write'
            },
            'env': {
                'WORKFLOW_TYPE': 'dynamic-multimedia',
                'GENERATED_AT': '$(date -u +"%Y-%m-%dT%H:%M:%SZ")',
                'REQUIREMENTS': ' | '.join(requirements)
            },
            'jobs': {}
        }
        
        # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’ã‚¸ãƒ§ãƒ–ã¨ã—ã¦ç”Ÿæˆ
        for stage_idx, stage_nodes in enumerate(execution_stages):
            stage_name = f"stage_{stage_idx + 1}"
            
            # ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªå ´åˆã¯è¤‡æ•°ã‚¸ãƒ§ãƒ–ã€ãã†ã§ãªã‘ã‚Œã°ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«
            if len(stage_nodes) > 1 and all(self.task_nodes[node_id].get('parallel', False) for node_id in stage_nodes):
                # ä¸¦åˆ—ã‚¸ãƒ§ãƒ–ç”Ÿæˆ
                for node_idx, node_id in enumerate(stage_nodes):
                    job_name = f"{stage_name}_parallel_{node_idx + 1}"
                    workflow['jobs'][job_name] = self.create_job_from_node(node_id, stage_idx)
            else:
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ã‚¸ãƒ§ãƒ–ç”Ÿæˆ
                workflow['jobs'][stage_name] = self.create_combined_job_from_nodes(stage_nodes, stage_idx)
        
        # ã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°çµ±åˆ
        workflow['jobs']['autofix_integration'] = self.create_autofix_job()
        workflow['jobs']['monitor_integration'] = self.create_monitor_job()
        
        return workflow
    
    def create_workflow_from_requirements_enhanced(self, 
                                                 requirements: List[str], 
                                                 enhanced_context: dict = None,
                                                 workflow_name: str = "dynamic-enhanced-workflow",
                                                 description: str = "Enhanced dynamically generated workflow") -> Dict[str, Any]:
        """å¼·åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å‹•çš„ç”Ÿæˆ"""
        
        print(f"ğŸš€ Creating enhanced workflow: {workflow_name}")
        if enhanced_context:
            clarity_score = enhanced_context.get('clarity_score', 7)
            print(f"ğŸ“Š Enhanced context: clarity={clarity_score}/10")
        
        # å¼·åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆã§ãƒãƒ¼ãƒ‰é¸æŠ
        selected_nodes = self.find_nodes_for_requirements(requirements, enhanced_context)
        execution_stages = self.generate_dependency_order(selected_nodes)
        
        print(f"ğŸ“‹ Selected {len(selected_nodes)} task nodes")
        print(f"âš¡ {len(execution_stages)} execution stages")
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼åŸºæœ¬æ§‹é€ ï¼ˆæ—¢å­˜ã¨åŒã˜ï¼‰
        workflow = {
            'name': workflow_name,
            'on': {
                'workflow_dispatch': {
                    'inputs': {
                        'user_prompt': {
                            'description': 'User requirements for multimedia generation',
                            'required': True,
                            'type': 'string',
                            'default': ' | '.join(requirements)
                        }
                    }
                },
                'issues': {
                    'types': ['opened', 'edited']
                }
            },
            'permissions': {
                'contents': 'write',
                'actions': 'write',
                'issues': 'write',
                'pull-requests': 'write'
            },
            'env': {
                'WORKFLOW_TYPE': 'dynamic-enhanced',
                'GENERATED_AT': '$(date -u +"%Y-%m-%dT%H:%M:%SZ")',
                'REQUIREMENTS': ' | '.join(requirements),
                'CLARITY_SCORE': str(enhanced_context.get('clarity_score', 7) if enhanced_context else 7)
            },
            'jobs': {}
        }
        
        # å„ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’ã‚¸ãƒ§ãƒ–ã¨ã—ã¦ç”Ÿæˆï¼ˆæ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ä½¿ç”¨ï¼‰
        for stage_idx, stage_nodes in enumerate(execution_stages):
            stage_name = f"stage_{stage_idx + 1}"
            
            if len(stage_nodes) > 1 and all(self.task_nodes[node_id].get('parallel', False) for node_id in stage_nodes):
                # ä¸¦åˆ—ã‚¸ãƒ§ãƒ–ç”Ÿæˆ
                for node_idx, node_id in enumerate(stage_nodes):
                    job_name = f"{stage_name}_parallel_{node_idx + 1}"
                    workflow['jobs'][job_name] = self.create_job_from_node(node_id, stage_idx)
            else:
                # ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ã‚¸ãƒ§ãƒ–ç”Ÿæˆ
                workflow['jobs'][stage_name] = self.create_combined_job_from_nodes(stage_nodes, stage_idx)
        
        # ã‚ªãƒ¼ãƒˆãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ»ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°çµ±åˆ
        workflow['jobs']['autofix_integration'] = self.create_autofix_job()
        workflow['jobs']['monitor_integration'] = self.create_monitor_job()
        
        return workflow
    
    def create_job_from_node(self, node_id: str, stage_idx: int) -> Dict[str, Any]:
        """å˜ä¸€ãƒãƒ¼ãƒ‰ã‹ã‚‰ã‚¸ãƒ§ãƒ–ã‚’ç”Ÿæˆ"""
        node = self.task_nodes[node_id]
        
        job = {
            'runs-on': 'ubuntu-latest',
            'timeout-minutes': node.get('duration_estimate', 10) + 5,  # ãƒãƒƒãƒ•ã‚¡è¿½åŠ 
            'env': {
                'TASK_NODE_ID': node_id,
                'TASK_NAME': node['name'],
                'STAGE': str(stage_idx + 1),
                'SOURCE_TEMPLATE': node['source_template']
            },
            'steps': [
                {
                    'name': 'Checkout repository',
                    'uses': 'actions/checkout@v4'
                },
                {
                    'name': 'Setup Claude Code Environment',
                    'run': '''
                        echo "ğŸ”§ Setting up Claude Code environment..."
                        mkdir -p .logs outputs artifacts
                        echo "TASK_START_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_ENV
                    '''
                }
            ]
        }
        
        # ãƒãƒ¼ãƒ‰ã®ã‚¸ãƒ§ãƒ–ã‚’å®Ÿéš›ã®ã‚¹ãƒ†ãƒƒãƒ—ã«å¤‰æ›
        for job_description in node.get('jobs', []):
            step = self.convert_job_to_step(job_description, node)
            job['steps'].append(step)
        
        # çµæœã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        job['steps'].append({
            'name': 'Upload task results',
            'uses': 'actions/upload-artifact@v4',
            'if': 'always()',
            'with': {
                'name': f"task-results-{node_id}",
                'path': 'outputs/',
                'retention-days': 7
            }
        })
        
        return job
    
    def create_combined_job_from_nodes(self, node_ids: List[str], stage_idx: int) -> Dict[str, Any]:
        """è¤‡æ•°ãƒãƒ¼ãƒ‰ã‚’çµ±åˆã—ãŸã‚¸ãƒ§ãƒ–ã‚’ç”Ÿæˆ"""
        primary_node = self.task_nodes[node_ids[0]]
        
        # å®Ÿè¡Œæ™‚é–“ã‚’åˆè¨ˆ
        total_duration = sum(self.task_nodes[node_id].get('duration_estimate', 5) for node_id in node_ids)
        
        job = {
            'runs-on': 'ubuntu-latest',
            'timeout-minutes': total_duration + 10,
            'env': {
                'STAGE': str(stage_idx + 1),
                'TASK_NODE_IDS': ','.join(node_ids),
                'TASK_COUNT': len(node_ids)
            },
            'steps': [
                {
                    'name': 'Checkout repository',
                    'uses': 'actions/checkout@v4'
                },
                {
                    'name': 'Setup Multi-Task Environment',
                    'run': f'''
                        echo "ğŸ”§ Setting up multi-task environment for stage {stage_idx + 1}..."
                        mkdir -p .logs outputs artifacts
                        echo "Processing {len(node_ids)} tasks in sequence..."
                        echo "STAGE_START_TIME=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_ENV
                    '''
                }
            ]
        }
        
        # å„ãƒãƒ¼ãƒ‰ã®ã‚¸ãƒ§ãƒ–ã‚’é †æ¬¡å®Ÿè¡Œ
        for node_id in node_ids:
            node = self.task_nodes[node_id]
            
            job['steps'].append({
                'name': f"Execute: {node['name']}",
                'run': f'''
                    echo "ğŸ“ Starting task: {node['name']}"
                    echo "CURRENT_TASK={node_id}" >> $GITHUB_ENV
                '''
            })
            
            for job_description in node.get('jobs', []):
                step = self.convert_job_to_step(job_description, node)
                job['steps'].append(step)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸çµæœã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        job['steps'].append({
            'name': 'Upload stage results',
            'uses': 'actions/upload-artifact@v4',
            'if': 'always()',
            'with': {
                'name': f"stage-{stage_idx + 1}-results",
                'path': 'outputs/',
                'retention-days': 7
            }
        })
        
        return job
    
    def convert_job_to_step(self, job_description: str, node: Dict[str, Any]) -> Dict[str, str]:
        """ã‚¸ãƒ§ãƒ–è¨˜è¿°ã‚’å®Ÿéš›ã®GitHub Actionsã‚¹ãƒ†ãƒƒãƒ—ã«å¤‰æ›"""
        
        # MCP ã‚µãƒ¼ãƒ“ã‚¹æ¤œå‡º
        mcp_services = node.get('mcp_services', [])
        capabilities = node.get('capabilities', [])
        
        step_name = job_description[:50] + "..." if len(job_description) > 50 else job_description
        
        # æ©Ÿèƒ½åˆ¥ã®ã‚³ãƒãƒ³ãƒ‰ç”Ÿæˆ
        if 'text_to_image' in capabilities:
            return {
                'name': f"T2I: {step_name}",
                'run': f'''
                    echo "ğŸ¨ Executing text-to-image generation..."
                    echo "Job: {job_description}"
                    
                    # MCP ã‚µãƒ¼ãƒ“ã‚¹ä½¿ç”¨
                    MCP_SERVICE="{mcp_services[0] if mcp_services else 't2i-fal-imagen4-ultra'}"
                    echo "Using MCP service: $MCP_SERVICE"
                    
                    # Claude Code + MCP ã§ç”»åƒç”Ÿæˆ
                    claude --continue "ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã—ã¦ãã ã•ã„: ${{{{ inputs.user_prompt || env.REQUIREMENTS }}}}" \\
                        --mcp "$MCP_SERVICE" \\
                        --output-format json > outputs/image_generation_result.json
                    
                    # ç”Ÿæˆçµæœã‹ã‚‰ç”»åƒãƒ‘ã‚¹ã‚’æŠ½å‡º
                    IMAGE_PATH=$(jq -r '.image_url // .file_path // "none"' outputs/image_generation_result.json 2>/dev/null)
                    
                    if [ "$IMAGE_PATH" != "none" ]; then
                        echo "âœ… Image generated: $IMAGE_PATH"
                        echo "$IMAGE_PATH" > outputs/generated_image_path.txt
                        echo "IMAGE_PATH=$IMAGE_PATH" >> $GITHUB_ENV
                    else
                        echo "âŒ Image generation failed"
                        exit 1
                    fi
                '''
            }
        
        elif 'image_to_video' in capabilities:
            return {
                'name': f"I2V: {step_name}",
                'run': f'''
                    echo "ğŸ¬ Executing image-to-video generation..."
                    echo "Job: {job_description}"
                    
                    # å‰æ®µéšã®ç”»åƒã‚’å–å¾—
                    if [ -f "outputs/generated_image_${{{{ github.run_number }}}}.png" ]; then
                        INPUT_IMAGE="outputs/generated_image_${{{{ github.run_number }}}}.png"
                    else
                        INPUT_IMAGE=$(find outputs -name "*.png" -o -name "*.jpg" | head -1)
                    fi
                    
                    if [ -n "$INPUT_IMAGE" ]; then
                        claude-code --mcp {mcp_services[0] if mcp_services else 'i2v-fal-hailuo-02-pro'} \\
                            --input "$INPUT_IMAGE" \\
                            --output "outputs/generated_video_${{{{ github.run_number }}}}.mp4"
                    fi
                '''
            }
        
        elif 'text_to_music' in capabilities:
            return {
                'name': f"T2M: {step_name}",
                'run': f'''
                    echo "ğŸµ Executing text-to-music generation..."
                    echo "Job: {job_description}"
                    
                    claude-code --mcp {mcp_services[0] if mcp_services else 't2m-google-lyria'} \\
                        --prompt "Generate BGM: ${{{{ inputs.user_prompt || env.REQUIREMENTS }}}}" \\
                        --output "outputs/generated_music_${{{{ github.run_number }}}}.mp3"
                '''
            }
        
        elif 'video_to_audio' in capabilities:
            return {
                'name': f"V2A: {step_name}",
                'run': f'''
                    echo "ğŸ”Š Executing video-to-audio extraction..."
                    echo "Job: {job_description}"
                    
                    # å‰æ®µéšã®ãƒ“ãƒ‡ã‚ªã‚’å–å¾—
                    INPUT_VIDEO=$(find outputs -name "*.mp4" | head -1)
                    
                    if [ -n "$INPUT_VIDEO" ]; then
                        claude-code --mcp {mcp_services[0] if mcp_services else 'v2a-fal-metavoice-v1'} \\
                            --input "$INPUT_VIDEO" \\
                            --output "outputs/extracted_audio_${{{{ github.run_number }}}}.wav"
                    fi
                '''
            }
        
        else:
            # æ±ç”¨ã‚¹ãƒ†ãƒƒãƒ—
            return {
                'name': f"Execute: {step_name}",
                'run': f'''
                    echo "âš¡ Executing general task..."
                    echo "Job: {job_description}"
                    echo "Capabilities: {','.join(capabilities)}"
                    
                    # æ±ç”¨å®Ÿè¡Œãƒ­ã‚¸ãƒƒã‚¯
                    mkdir -p "outputs/${{{{ env.CURRENT_TASK || 'general' }}}}"
                    echo "${{{{ github.run_number }}}}" > "outputs/${{{{ env.CURRENT_TASK || 'general' }}}}/execution_id.txt"
                '''
            }
    
    def create_autofix_job(self) -> Dict[str, Any]:
        """AutoFixçµ±åˆã‚¸ãƒ§ãƒ–"""
        return {
            'needs': [],  # å…¨ã‚¸ãƒ§ãƒ–å®Œäº†å¾Œã«å®Ÿè¡Œ
            'runs-on': 'ubuntu-latest',
            'if': 'failure()',
            'steps': [
                {
                    'name': 'Trigger AutoFix',
                    'run': '''
                        echo "ğŸ”§ Triggering AutoFix system..."
                        # AutoFix ã‚·ã‚¹ãƒ†ãƒ ã¸ã®é€šçŸ¥
                        echo "AUTOFIX_TRIGGERED=true" >> $GITHUB_ENV
                    '''
                }
            ]
        }
    
    def create_monitor_job(self) -> Dict[str, Any]:
        """Monitorçµ±åˆã‚¸ãƒ§ãƒ–"""
        return {
            'runs-on': 'ubuntu-latest',
            'if': 'always()',
            'steps': [
                {
                    'name': 'Report to Monitor',
                    'run': '''
                        echo "ğŸ“Š Reporting to Monitor system..."
                        echo "MONITOR_REPORTED=true" >> $GITHUB_ENV
                    '''
                }
            ]
        }
    
    def save_workflow(self, workflow: Dict[str, Any], output_path: str):
        """ç”Ÿæˆã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä¿å­˜"""
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(workflow, f, default_flow_style=False, allow_unicode=True, sort_keys=False)
        
        print(f"âœ… Dynamic workflow saved: {output_path}")
    
    def find_nodes_for_requirements(self, requirements: List[str], enhanced_context: dict = None) -> List[str]:
        """è¦æ±‚ã«åŸºã¥ã„ã¦é©åˆ‡ãªã‚¿ã‚¹ã‚¯ãƒãƒ¼ãƒ‰ã‚’é¸æŠï¼ˆå¼·åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆå¯¾å¿œï¼‰"""
        selected_nodes = []
        
        # å¼·åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è€ƒæ…®
        clarity_score = 7
        fallback_assumptions = []
        if enhanced_context:
            clarity_score = enhanced_context.get('clarity_score', 7)
            fallback_assumptions = enhanced_context.get('fallback_assumptions', [])
            print(f"ğŸ“Š Using enhanced context: clarity={clarity_score}/10, assumptions={len(fallback_assumptions)}")
        
        # è«–ç†ãƒ•ãƒ­ãƒ¼é‡è¦–ã®è¦æ±‚åˆ†æãƒãƒƒãƒ”ãƒ³ã‚°
        requirement_flow_mapping = [
            # Stage 1: ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ãƒ»ç”»åƒç”Ÿæˆ
            (['ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒ', 'ç”»åƒç”Ÿæˆ', 'text.*image', 'ãƒ†ã‚­ã‚¹ãƒˆ.*ç”»åƒ'], 'text_to_image', 1),
            
            # Stage 2: ç”»åƒã‹ã‚‰å‹•ç”»ç”Ÿæˆ  
            (['ç”»åƒã‹ã‚‰å‹•ç”»', 'å‹•ç”»ç”Ÿæˆ', 'image.*video', 'ç”»åƒ.*å‹•ç”»'], 'image_to_video', 2),
            
            # Stage 3: éŸ³æ¥½ãƒ»ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªç”Ÿæˆï¼ˆä¸¦è¡Œå¯èƒ½ï¼‰
            (['ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³æ¥½', 'éŸ³æ¥½ç”Ÿæˆ', 'BGM', 'text.*music', 'ãƒ†ã‚­ã‚¹ãƒˆ.*éŸ³æ¥½'], 'text_to_music', 3),
            
            # Stage 4: å‹•ç”»ã‹ã‚‰éŸ³å£°æŠ½å‡º
            (['å‹•ç”»ã‹ã‚‰éŸ³å£°', 'éŸ³å£°æŠ½å‡º', 'video.*audio', 'å‹•ç”».*éŸ³å£°', 'ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³', 'ã‚»ãƒªãƒ•'], 'video_to_audio', 4)
        ]
        
        # è¦æ±‚ã‚’è«–ç†ãƒ•ãƒ­ãƒ¼é †ã«åˆ†æ
        matched_stages = {}
        
        for requirement in requirements:
            req_lower = requirement.lower()
            
            for keywords, capability, stage in requirement_flow_mapping:
                for keyword in keywords:
                    if keyword.lower() in req_lower or (len(keyword.split('.*')) == 2 and 
                        all(part in req_lower for part in keyword.split('.*'))):
                        
                        if stage not in matched_stages:
                            matched_stages[stage] = []
                        
                        if capability in self.capabilities_index:
                            stage_nodes = self.capabilities_index[capability]
                            matched_stages[stage].extend(stage_nodes)
                            print(f"ğŸ¯ Stage {stage}: '{requirement}' â†’ {capability} â†’ {len(stage_nodes)} nodes")
                        break
        
        # è«–ç†ãƒ•ãƒ­ãƒ¼é †ï¼ˆstageé †ï¼‰ã§ãƒãƒ¼ãƒ‰ã‚’é¸æŠ
        for stage in sorted(matched_stages.keys()):
            stage_nodes = list(set(matched_stages[stage]))  # é‡è¤‡é™¤å»
            
            # æ˜ç¢ºåº¦ãŒä½ã„å ´åˆã¯å®‰å…¨ãªãƒãƒ¼ãƒ‰ã®ã¿é¸æŠ
            if clarity_score < 6:
                # è¤‡é›‘åº¦ã®ä½ã„ãƒãƒ¼ãƒ‰ã‚’å„ªå…ˆ
                stage_nodes = sorted(stage_nodes, key=lambda n: (
                    self.task_nodes[n]['complexity'],
                    self.task_nodes[n]['duration_estimate']
                ))[:3]  # æœ€å¤§3ãƒãƒ¼ãƒ‰ã«åˆ¶é™
                print(f"âš ï¸ Low clarity: limiting stage {stage} to {len(stage_nodes)} safe nodes")
            
            selected_nodes.extend(stage_nodes)
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»®å®šã«åŸºã¥ãè¿½åŠ ãƒãƒ¼ãƒ‰é¸æŠ
        if fallback_assumptions and len(selected_nodes) < 5:
            print("ğŸ”§ Applying fallback assumptions for node enhancement...")
            for assumption in fallback_assumptions:
                if 'æ¨™æº–å“è³ª' in assumption and 'text_to_image' in self.capabilities_index:
                    additional_nodes = self.capabilities_index['text_to_image'][:2]
                    selected_nodes.extend(additional_nodes)
                    print(f"ğŸ¯ Fallback: Added {len(additional_nodes)} standard quality nodes")
        
        # æœ€çµ‚çš„ãªé‡è¤‡é™¤å»ã¨å„ªå…ˆåº¦ã‚½ãƒ¼ãƒˆ
        unique_nodes = list(set(selected_nodes))
        sorted_nodes = sorted(unique_nodes, key=lambda n: (
            self.task_nodes[n]['stage'],
            self.task_nodes[n]['complexity'],
            self.task_nodes[n]['duration_estimate']
        ))
        
        print(f"âœ… Selected {len(sorted_nodes)} nodes across {len(matched_stages)} stages")
        return sorted_nodes
    
    def generate_dependency_order(self, selected_nodes: List[str]) -> List[List[str]]:
        """é¸æŠã•ã‚ŒãŸãƒãƒ¼ãƒ‰ã®ä¾å­˜é–¢ä¿‚ã«åŸºã¥ãå®Ÿè¡Œé †åºã‚’ç”Ÿæˆ"""
        stages = {}
        
        for node_id in selected_nodes:
            node = self.task_nodes[node_id]
            stage = node['stage']
            
            if stage not in stages:
                stages[stage] = []
            stages[stage].append(node_id)
        
        # ã‚¹ãƒ†ãƒ¼ã‚¸é †ã§ã‚½ãƒ¼ãƒˆ
        ordered_stages = [stages[stage] for stage in sorted(stages.keys())]
        
        return ordered_stages

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ï¼‰"""
    import os
    import json
    
    assembler = DynamicWorkflowAssembler()
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å¼·åŒ–ã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
    enhanced_context_file = os.getenv('ENHANCED_CONTEXT_FILE')
    workflow_type = os.getenv('WORKFLOW_TYPE', 'custom')
    
    enhanced_context = {}
    if enhanced_context_file and os.path.exists(enhanced_context_file):
        try:
            with open(enhanced_context_file, 'r', encoding='utf-8') as f:
                enhanced_context = json.load(f)
            print(f"âœ… Loaded enhanced context from {enhanced_context_file}")
            print(f"ğŸ“Š Clarity Score: {enhanced_context.get('clarity_score', 'N/A')}/10")
        except Exception as e:
            print(f"âš ï¸ Failed to load enhanced context: {e}")
    
    # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãè¦æ±‚è¨­å®š
    if workflow_type == "custom":
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¦æ±‚ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
        requirements = [
            "ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒç”Ÿæˆ",
            "ç”»åƒã‹ã‚‰å‹•ç”»ç”Ÿæˆ", 
            "ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³æ¥½ç”Ÿæˆ",
            "å‹•ç”»ã‹ã‚‰éŸ³å£°æŠ½å‡º"
        ]
    else:
        # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¿ã‚¤ãƒ—åˆ¥ã®è¦æ±‚ãƒãƒƒãƒ”ãƒ³ã‚°
        type_mapping = {
            "image-generation": ["ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒç”Ÿæˆ", "ç”»åƒå“è³ªå‘ä¸Š"],
            "video-generation": ["ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒç”Ÿæˆ", "ç”»åƒã‹ã‚‰å‹•ç”»ç”Ÿæˆ"],
            "audio-generation": ["ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³æ¥½ç”Ÿæˆ", "éŸ³å£°å“è³ªå‘ä¸Š"],
            "news-article": ["ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æ", "è¨˜äº‹ç”Ÿæˆ"],
            "news-video": ["ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æ", "ç”»åƒç”Ÿæˆ", "å‹•ç”»ç”Ÿæˆ"],
            "social-integration": ["ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ", "SNSæœ€é©åŒ–"]
        }
        requirements = type_mapping.get(workflow_type, ["åŸºæœ¬ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ"])
    
    print(f"ğŸš€ Creating dynamic {workflow_type} workflow...")
    print(f"ğŸ“‹ Requirements: {', '.join(requirements)}")
    
    # å¼·åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚¯ã‚¹ãƒˆã§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç”Ÿæˆ
    workflow = assembler.create_workflow_from_requirements_enhanced(
        requirements=requirements,
        enhanced_context=enhanced_context,
        workflow_name=f"dynamic-{workflow_type}-generation",
        description=f"Dynamically assembled {workflow_type} workflow with enhanced context"
    )
    
    # ã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã«ä¿å­˜
    output_path = f"generated/workflows/staging/dynamic-{workflow_type}-generation.yml"
    assembler.save_workflow(workflow, output_path)
    
    print(f"ğŸ¯ Generated workflow with {len(workflow['jobs'])} jobs")
    print(f"ğŸ“ Saved to: {output_path}")
    
    # ä½¿ç”¨ã—ãŸãƒãƒ¼ãƒ‰æ•°ã‚’å‡ºåŠ›ï¼ˆGitHub Actionsã§è§£æç”¨ï¼‰
    if 'jobs' in workflow:
        job_count = len([job for job_name, job in workflow['jobs'].items() 
                        if not job_name.startswith(('autofix', 'monitor'))])
        print(f"Selected {job_count} task nodes")

if __name__ == "__main__":
    main()