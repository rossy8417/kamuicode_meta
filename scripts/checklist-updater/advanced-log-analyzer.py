#!/usr/bin/env python3
"""
é«˜åº¦ãªãƒ­ã‚°åˆ†æã¨çŸ¥è¦‹è“„ç©ã‚·ã‚¹ãƒ†ãƒ 
Claude Code SDKã¨é€£æºã—ã¦ã€ãƒ­ã‚°ã‹ã‚‰å•é¡Œã¨è§£æ±ºç­–ã‚’æ§‹é€ åŒ–ã—ã¦æŠ½å‡º

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œï¼š
1. å®Ÿè¡Œãƒ­ã‚°ã‹ã‚‰å•é¡Œãƒ»è§£æ±ºãƒ»çµæœã‚’åˆ†æ
2. é‡è¤‡ã‚’æ’é™¤ã—ã¦çŸ¥è¦‹ã‚’çµ±åˆ
3. å®Ÿç”¨çš„ãªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
4. æ—¢å­˜ã®å˜ç´”è¿½åŠ ã‚’ä¿®æ­£
"""

import os
import sys
import re
import json
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from collections import defaultdict

class AdvancedLogAnalyzer:
    def __init__(self, base_dir: Path = Path(".")):
        self.base_dir = base_dir
        self.logs_dir = base_dir / "projects" / "workflow-execution-logs"
        self.knowledge_base = defaultdict(dict)
        
    def analyze_with_claude(self, log_content: str, log_file: str) -> Dict:
        """Claude Code SDKã‚’ä½¿ç”¨ã—ã¦ãƒ­ã‚°ã‚’é«˜åº¦ã«åˆ†æ"""
        
        # Claudeç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å•é¡Œã¨è§£æ±ºç­–ã‚’æ§‹é€ åŒ–ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š

ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_file}
---
{log_content[:3000]}  # æœ€åˆã®3000æ–‡å­—
---

ä»¥ä¸‹ã®å½¢å¼ã§JSONå‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{{
  "problems": [
    {{
      "issue": "å•é¡Œã®ç°¡æ½”ãªèª¬æ˜",
      "root_cause": "æ ¹æœ¬åŸå› ",
      "symptoms": ["ç—‡çŠ¶1", "ç—‡çŠ¶2"],
      "solutions_tried": ["è©¦ã—ãŸè§£æ±ºç­–1", "è©¦ã—ãŸè§£æ±ºç­–2"],
      "working_solution": "å®Ÿéš›ã«åŠ¹æœãŒã‚ã£ãŸè§£æ±ºç­–",
      "verification": "æ¤œè¨¼æ–¹æ³•ã¨Run ID",
      "category": "å•é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆMCP, GitHub Actions, etcï¼‰",
      "prevention": ["å†ç™ºé˜²æ­¢ç­–1", "å†ç™ºé˜²æ­¢ç­–2"]
    }}
  ]
}}

é‡è¦ï¼š
- åŒã˜å•é¡Œã®ç•°ãªã‚‹è¡¨ç¾ã¯çµ±åˆ
- å®Ÿéš›ã®è§£æ±ºãƒ—ãƒ­ã‚»ã‚¹ã‚’æŠ½å‡º
- æ¤œè¨¼æ¸ˆã¿ã®è§£æ±ºç­–ã‚’æ˜è¨˜
"""
        
        # Claude Code SDKã‚’å‘¼ã³å‡ºã—ï¼ˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ subprocess ã§Claude Code CLIã‚’å®Ÿè¡Œ
        try:
            # ã“ã®éƒ¨åˆ†ã¯å®Ÿéš›ã®Claude Code SDKå‘¼ã³å‡ºã—ã«ç½®ãæ›ãˆ
            result = self._simulate_claude_analysis(log_content, log_file)
            return result
        except Exception as e:
            print(f"Claude analysis failed: {e}")
            return self._fallback_analysis(log_content, log_file)
    
    def _simulate_claude_analysis(self, log_content: str, log_file: str) -> Dict:
        """Claudeåˆ†æã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå®Ÿéš›ã®å®Ÿè£…æ™‚ã¯å‰Šé™¤ï¼‰"""
        problems = []
        
        # æ—¢çŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰åˆ†æ
        if "SignatureDoesNotMatch" in log_content:
            problems.append({
                "issue": "Google Cloud Storageç½²åã‚¨ãƒ©ãƒ¼",
                "root_cause": "Service Accountèªè¨¼ã®æœŸé™åˆ‡ã‚Œã¾ãŸã¯æ¨©é™ä¸è¶³",
                "symptoms": ["SignatureDoesNotMatch", "Access denied", "ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—"],
                "solutions_tried": ["curlæ¡ä»¶åˆ¤å®šã®ç·©å’Œ", "ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·"],
                "working_solution": "Service Accountå†èªè¨¼ã¨Fallbackå‡¦ç†å®Ÿè£…",
                "verification": "Run 16844404207ã§å•é¡Œç¢ºèªã€ä¿®æ­£ç‰ˆã§æ¤œè¨¼äºˆå®š",
                "category": "GCSèªè¨¼",
                "prevention": ["å®šæœŸçš„ãªService Accountæ›´æ–°", "Fallbackå‡¦ç†ã®äº‹å‰æº–å‚™"]
            })
            
        if "Max turns" in log_content or "max turns" in log_content:
            problems.append({
                "issue": "Claude Code SDK Max turnsåˆ¶é™",
                "root_cause": "I2Vå‡¦ç†ã®éåŒæœŸæ€§ã«ã‚ˆã‚Š40ã‚¿ãƒ¼ãƒ³ã§ã¯ä¸è¶³",
                "symptoms": ["Max turns (40) exceeded", "I2Vå‡¦ç†æœªå®Œäº†"],
                "solutions_tried": ["--max-turns 40ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰"],
                "working_solution": "--max-turns 80ä»¥ä¸Šã«å¢—åŠ ",
                "verification": "Run 16843538810ã§æˆåŠŸç¢ºèª",
                "category": "Claude Code SDK",
                "prevention": ["I2Vå‡¦ç†ã¯åˆæœŸã‹ã‚‰80ã‚¿ãƒ¼ãƒ³è¨­å®š", "å‡¦ç†æ™‚é–“è¦‹ç©ã‚‚ã‚Šã®æ”¹å–„"]
            })
            
        if "placeholder" in log_content:
            problems.append({
                "issue": "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«æœªä¿å­˜ã«ã‚ˆã‚‹placeholder",
                "root_cause": "Google URLå–å¾—æˆåŠŸã‚‚curlãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æœªå®Ÿè¡Œ",
                "symptoms": ["placeholderè¨­å®š", "Videoç”Ÿæˆå¤±æ•—", "ä¾å­˜é–¢ä¿‚é€£é–å¤±æ•—"],
                "solutions_tried": ["URLå—ã‘æ¸¡ã—æ”¹å–„"],
                "working_solution": "curlãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã®ç¢ºå®Ÿãªå®Ÿè¡Œ",
                "verification": "ä¿®æ­£ç‰ˆãƒ†ã‚¹ãƒˆä¸­",
                "category": "ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†",
                "prevention": ["URLå–å¾—ç›´å¾Œã®å³åº§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", "ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªå¼·åŒ–"]
            })
            
        return {"problems": problems}
    
    def _fallback_analysis(self, log_content: str, log_file: str) -> Dict:
        """Fallbackåˆ†æï¼ˆClaudeåˆ©ç”¨ä¸å¯æ™‚ï¼‰"""
        problems = []
        
        # åŸºæœ¬çš„ãªã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
        error_patterns = {
            r'ERROR.*?(\n|$)': 'ã‚¨ãƒ©ãƒ¼',
            r'Failed.*?(\n|$)': 'å¤±æ•—',
            r'âŒ.*?(\n|$)': 'å•é¡Œç™ºç”Ÿ'
        }
        
        for pattern, category in error_patterns.items():
            matches = re.findall(pattern, log_content, re.MULTILINE)
            for match in matches[:3]:  # æœ€åˆã®3ã¤ã¾ã§
                problems.append({
                    "issue": match.strip(),
                    "category": category,
                    "root_cause": "è¦èª¿æŸ»",
                    "working_solution": "æ‰‹å‹•èª¿æŸ»å¿…è¦"
                })
                
        return {"problems": problems}
    
    def consolidate_knowledge(self, all_problems: List[Dict]) -> Dict:
        """å•é¡Œã‚’çµ±åˆã—ã¦çŸ¥è¦‹ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰"""
        consolidated = defaultdict(lambda: {
            "occurrences": [],
            "solutions": set(),
            "preventions": set(),
            "verifications": []
        })
        
        for problem in all_problems:
            # ã‚«ãƒ†ã‚´ãƒªã¨å•é¡Œã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            key = f"{problem.get('category', 'Unknown')}::{problem['issue']}"
            
            consolidated[key]["occurrences"].append(problem)
            if problem.get("working_solution"):
                consolidated[key]["solutions"].add(problem["working_solution"])
            if problem.get("prevention"):
                for p in problem["prevention"]:
                    consolidated[key]["preventions"].add(p)
            if problem.get("verification"):
                consolidated[key]["verifications"].append(problem["verification"])
                
        return consolidated
    
    def generate_structured_checklist(self, knowledge_base: Dict) -> str:
        """æ§‹é€ åŒ–ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        content = []
        content.append("# å®Ÿè¡Œãƒ­ã‚°ã‹ã‚‰å­¦ç¿’ã—ãŸå•é¡Œè§£æ±ºãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ")
        content.append(f"\n**æœ€çµ‚æ›´æ–°**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        content.append("\n## ğŸ“‹ å•é¡Œã‚«ãƒ†ã‚´ãƒªåˆ¥ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ\n")
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
        categories = defaultdict(list)
        for key, data in knowledge_base.items():
            category, issue = key.split("::", 1)
            categories[category].append((issue, data))
        
        for category, issues in sorted(categories.items()):
            content.append(f"\n### {category}\n")
            
            for issue, data in issues:
                content.append(f"#### âŒ å•é¡Œ: {issue}")
                
                # æœ€æ–°ã®ç™ºç”Ÿæƒ…å ±
                latest = data["occurrences"][-1] if data["occurrences"] else {}
                
                if latest.get("root_cause"):
                    content.append(f"**æ ¹æœ¬åŸå› **: {latest['root_cause']}")
                
                if latest.get("symptoms"):
                    content.append(f"**ç—‡çŠ¶**: {', '.join(latest['symptoms'])}")
                
                # è§£æ±ºç­–
                if data["solutions"]:
                    content.append("\n**âœ… æ¤œè¨¼æ¸ˆã¿è§£æ±ºç­–**:")
                    for solution in data["solutions"]:
                        content.append(f"- {solution}")
                
                # æ¤œè¨¼æƒ…å ±ï¼ˆé‡è¤‡ã‚’æ’é™¤ï¼‰
                if data["verifications"]:
                    unique_verifications = list(dict.fromkeys(data["verifications"]))  # é‡è¤‡æ’é™¤
                    content.append(f"\n**ğŸ“Š æ¤œè¨¼**: {', '.join(unique_verifications[:3])}")  # æœ€åˆã®3ã¤ã¾ã§
                
                # å†ç™ºé˜²æ­¢ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
                if data["preventions"]:
                    content.append("\n**ğŸ”§ å†ç™ºé˜²æ­¢ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**:")
                    for prevention in data["preventions"]:
                        content.append(f"- [ ] {prevention}")
                
                content.append("")  # ç©ºè¡Œ
        
        return "\n".join(content)
    
    def clean_existing_checklist(self):
        """æ—¢å­˜ã®å˜ç´”è¿½åŠ ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        checklist_path = self.logs_dir / "meta-workflow-construction-checklist.md"
        
        if not checklist_path.exists():
            return
            
        with open(checklist_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # AUTO-GENERATED-PATTERNSã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã¾ãŸã¯æ•´ç†
        pattern = r'<!-- AUTO-GENERATED-PATTERNS -->.*?(?=\n##|\n###|\Z)'
        content = re.sub(pattern, '', content, flags=re.DOTALL)
        
        # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‰Šé™¤
        lines = content.split('\n')
        seen = set()
        cleaned_lines = []
        
        for line in lines:
            # å˜ç´”ãªãƒ‘ã‚¿ãƒ¼ãƒ³è¡Œã‚’æ¤œå‡º
            if line.startswith('- Pattern:') or line.startswith('  Solution: Manual investigation'):
                continue  # ã‚¹ã‚­ãƒƒãƒ—
            cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def run_analysis(self, hours: int = 24):
        """ãƒ¡ã‚¤ãƒ³åˆ†æå®Ÿè¡Œ"""
        print(f"ğŸ” éå»{hours}æ™‚é–“ã®ãƒ­ã‚°ã‚’åˆ†æä¸­...")
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        cutoff_time = datetime.now() - timedelta(hours=hours)
        log_files = []
        
        for log_file in self.logs_dir.glob("*.md"):
            if log_file.stat().st_mtime > cutoff_time.timestamp():
                log_files.append(log_file)
        
        print(f"ğŸ“ {len(log_files)}å€‹ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†æ")
        
        # å„ãƒ­ã‚°ã‚’åˆ†æ
        all_problems = []
        for log_file in log_files:
            with open(log_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Claudeåˆ†æã¾ãŸã¯ Fallback
            analysis = self.analyze_with_claude(content, log_file.name)
            if analysis.get("problems"):
                all_problems.extend(analysis["problems"])
        
        # çŸ¥è¦‹ã‚’çµ±åˆ
        knowledge_base = self.consolidate_knowledge(all_problems)
        
        # ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆç”Ÿæˆ
        checklist_content = self.generate_structured_checklist(knowledge_base)
        
        # æ—¢å­˜ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleaned_content = self.clean_existing_checklist()
        
        # æ–°ã—ã„ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ä¿å­˜
        output_path = self.logs_dir / "knowledge-based-checklist.md"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(checklist_content)
        
        print(f"âœ… çŸ¥è¦‹ãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ: {output_path}")
        print(f"ğŸ“Š çµ±åˆã•ã‚ŒãŸå•é¡Œæ•°: {len(knowledge_base)}")
        
        return len(knowledge_base)

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='é«˜åº¦ãªãƒ­ã‚°åˆ†æã¨çŸ¥è¦‹è“„ç©')
    parser.add_argument('--hours', type=int, default=24, help='åˆ†æå¯¾è±¡æ™‚é–“ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ24æ™‚é–“ï¼‰')
    parser.add_argument('--base-dir', type=Path, default=Path('.'), help='ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    args = parser.parse_args()
    
    analyzer = AdvancedLogAnalyzer(base_dir=args.base_dir)
    result = analyzer.run_analysis(hours=args.hours)
    
    if result > 0:
        print(f"\nğŸ¯ {result}å€‹ã®å•é¡Œã‚«ãƒ†ã‚´ãƒªã‚’çµ±åˆãƒ»æ§‹é€ åŒ–ã—ã¾ã—ãŸ")
        print("ğŸ“ æ—¢å­˜ã®å˜ç´”è¿½åŠ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ¸ˆã¿")
    else:
        print("\nâœ… æ–°ã—ã„å•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")