#!/usr/bin/env python3
"""
Orchestrator Analyzer for Meta Workflow v10
Claude Code SDKã‚’ä½¿ç”¨ã—ã¦è¤‡åˆçš„ãªã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼åˆ†æžã‚’è¡Œã†
"""

import json
import os
import sys
import re
import yaml
from typing import List, Dict, Tuple, Optional
from datetime import datetime

class OrchestratorAnalyzer:
    def __init__(self):
        self.orchestrator_dir = "kamuicode-workflow/module-workflow"
        self.minimal_units_dir = "minimal-units"
        self.orchestrators = self.load_orchestrators()
        
    def load_orchestrators(self) -> Dict[str, Dict]:
        """ã™ã¹ã¦ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        orchestrators = {}
        pattern = os.path.join(self.orchestrator_dir, "orchestrator-*.yml")
        
        import glob
        for file_path in glob.glob(pattern):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = yaml.safe_load(f)
                    name = os.path.basename(file_path).replace('.yml', '')
                    orchestrators[name] = {
                        'path': file_path,
                        'content': content,
                        'jobs': self.extract_jobs(content)
                    }
            except Exception as e:
                print(f"Error loading {file_path}: {e}", file=sys.stderr)
                
        return orchestrators
    
    def extract_jobs(self, content: Dict) -> List[Dict]:
        """ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰ã‚¸ãƒ§ãƒ–æƒ…å ±ã‚’æŠ½å‡º"""
        jobs = []
        if 'jobs' in content:
            for job_name, job_config in content['jobs'].items():
                if isinstance(job_config, dict) and 'uses' in job_config:
                    jobs.append({
                        'name': job_name,
                        'uses': job_config['uses'],
                        'needs': job_config.get('needs', []),
                        'with': job_config.get('with', {})
                    })
        return jobs
    
    def analyze_user_request(self, request: str) -> Dict:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã‚’åˆ†æžã—ã¦é–¢é€£ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ç‰¹å®š"""
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = self.extract_keywords(request)
        
        # é–¢é€£ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®ç‰¹å®š
        relevant_orchestrators = []
        for name, orch_data in self.orchestrators.items():
            relevance_score = self.calculate_relevance(name, orch_data, keywords, request)
            if relevance_score > 0.3:
                relevant_orchestrators.append({
                    'name': name,
                    'score': relevance_score,
                    'data': orch_data
                })
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        relevant_orchestrators.sort(key=lambda x: x['score'], reverse=True)
        
        return {
            'keywords': keywords,
            'orchestrators': relevant_orchestrators,
            'analysis_timestamp': datetime.utcnow().isoformat() + 'Z'
        }
    
    def extract_keywords(self, request: str) -> List[str]:
        """è¦æ±‚æ–‡ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º"""
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆå®Ÿéš›ã¯Claude Code SDKã§é«˜åº¦ãªåˆ†æžã‚’è¡Œã†ï¼‰
        keywords = []
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—
        if any(word in request.lower() for word in ['å‹•ç”»', 'video', 'ãƒ“ãƒ‡ã‚ª']):
            keywords.append('video')
        if any(word in request.lower() for word in ['ç”»åƒ', 'image', 'å†™çœŸ']):
            keywords.append('image')
        if any(word in request.lower() for word in ['éŸ³å£°', 'audio', 'éŸ³æ¥½', 'bgm']):
            keywords.append('audio')
        if any(word in request.lower() for word in ['ãƒ‹ãƒ¥ãƒ¼ã‚¹', 'news']):
            keywords.append('news')
        if any(word in request.lower() for word in ['è¨˜äº‹', 'article', 'ãƒ–ãƒ­ã‚°']):
            keywords.append('article')
        if any(word in request.lower() for word in ['åºƒå‘Š', 'advertisement', 'ãƒãƒŠãƒ¼']):
            keywords.append('advertisement')
        if any(word in request.lower() for word in ['åˆ†æž', 'analysis', 'ãƒ‡ãƒ¼ã‚¿']):
            keywords.append('analysis')
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        if any(word in request.lower() for word in ['æ¤œç´¢', 'search', 'èª¿æŸ»']):
            keywords.append('search')
        if any(word in request.lower() for word in ['ç”Ÿæˆ', 'generate', 'ä½œæˆ']):
            keywords.append('generation')
        if any(word in request.lower() for word in ['ç·¨é›†', 'edit', 'çµåˆ']):
            keywords.append('editing')
        
        return list(set(keywords))
    
    def calculate_relevance(self, orch_name: str, orch_data: Dict, keywords: List[str], request: str) -> float:
        """ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.0
        
        # åå‰ã«ã‚ˆã‚‹ãƒžãƒƒãƒãƒ³ã‚°
        for keyword in keywords:
            if keyword in orch_name.lower():
                score += 0.3
        
        # ç‰¹å®šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒãƒ³ã‚°
        if 'news' in keywords and 'video' in keywords:
            if 'news-video' in orch_name:
                score += 0.5
            elif 'news-article' in orch_name:
                score += 0.3
        
        if 'banner' in keywords or 'advertisement' in keywords:
            if 'banner-advertisement' in orch_name:
                score += 0.6
        
        # ã‚¸ãƒ§ãƒ–å†…å®¹ã«ã‚ˆã‚‹è¿½åŠ ã‚¹ã‚³ã‚¢
        job_names = [job['name'] for job in orch_data['jobs']]
        for job_name in job_names:
            for keyword in keywords:
                if keyword in job_name.lower():
                    score += 0.1
        
        return min(score, 1.0)
    
    def merge_orchestrator_patterns(self, orchestrators: List[Dict], request: str) -> Dict:
        """è¤‡æ•°ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‹ã‚‰æœ€é©ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’æ§‹ç¯‰"""
        all_jobs = []
        job_map = {}
        
        # ã™ã¹ã¦ã®ã‚¸ãƒ§ãƒ–ã‚’åŽé›†
        for orch in orchestrators:
            for job in orch['data']['jobs']:
                job_key = job['uses']
                if job_key not in job_map:
                    job_map[job_key] = {
                        'job': job,
                        'source': orch['name'],
                        'relevance': orch['score']
                    }
                else:
                    # ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„ã‚‚ã®ã‚’é¸æŠž
                    if orch['score'] > job_map[job_key]['relevance']:
                        job_map[job_key] = {
                            'job': job,
                            'source': orch['name'],
                            'relevance': orch['score']
                        }
        
        # è«–ç†çš„ãªé †åºã§ä¸¦ã¹æ›¿ãˆ
        ordered_jobs = self.create_logical_order(job_map, request)
        
        return {
            'jobs': ordered_jobs,
            'sources': list(set([v['source'] for v in job_map.values()])),
            'job_count': len(ordered_jobs)
        }
    
    def create_logical_order(self, job_map: Dict, request: str) -> List[Dict]:
        """ã‚¸ãƒ§ãƒ–ã‚’è«–ç†çš„ãªé †åºã«ä¸¦ã¹æ›¿ãˆ"""
        # ã‚¸ãƒ§ãƒ–ã‚¿ã‚¤ãƒ—ã®åˆ†é¡ž
        categories = {
            'planning': [],
            'search': [],
            'analysis': [],
            'content_creation': [],
            'media_generation': [],
            'post_processing': [],
            'finalization': []
        }
        
        for job_key, job_data in job_map.items():
            job_name = job_data['job']['name']
            uses = job_data['job']['uses']
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ†ã‘
            if any(word in job_name.lower() for word in ['planning', 'plan', 'ä¼ç”»']):
                categories['planning'].append(job_data)
            elif any(word in job_name.lower() for word in ['search', 'web', 'æ¤œç´¢']):
                categories['search'].append(job_data)
            elif any(word in job_name.lower() for word in ['analysis', 'analyze', 'åˆ†æž']):
                categories['analysis'].append(job_data)
            elif any(word in job_name.lower() for word in ['article', 'content', 'text']):
                categories['content_creation'].append(job_data)
            elif any(word in uses.lower() for word in ['image', 'video', 'audio', 'banner']):
                categories['media_generation'].append(job_data)
            elif any(word in job_name.lower() for word in ['lipsync', 'edit', 'concat', 'overlay']):
                categories['post_processing'].append(job_data)
            else:
                categories['finalization'].append(job_data)
        
        # è«–ç†çš„ãªé †åºã§çµåˆ
        ordered_jobs = []
        for category in ['search', 'analysis', 'planning', 'content_creation', 
                        'media_generation', 'post_processing', 'finalization']:
            ordered_jobs.extend(categories[category])
        
        return ordered_jobs
    
    def generate_execution_plan(self, request: str) -> Dict:
        """å®Ÿè¡Œè¨ˆç”»ã‚’ç”Ÿæˆ"""
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ã‚’åˆ†æž
        analysis = self.analyze_user_request(request)
        
        if not analysis['orchestrators']:
            # ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯åŸºæœ¬ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
            return self.create_default_execution_plan(request, analysis['keywords'])
        
        # è¤‡æ•°ã®ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’çµ±åˆ
        merged_workflow = self.merge_orchestrator_patterns(
            analysis['orchestrators'], 
            request
        )
        
        # ä¾å­˜é–¢ä¿‚ã®è§£æžã¨ä¸¦åˆ—å®Ÿè¡Œã®æœ€é©åŒ–
        optimized_workflow = self.optimize_parallel_execution(merged_workflow)
        
        return {
            'request': request,
            'analysis': analysis,
            'workflow': optimized_workflow,
            'execution_pattern': self.determine_execution_pattern(optimized_workflow)
        }
    
    def create_default_execution_plan(self, request: str, keywords: List[str]) -> Dict:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å®Ÿè¡Œè¨ˆç”»ã‚’ä½œæˆ"""
        # å°‚é–€å®¶è¦–ç‚¹ã§ã®åŸºæœ¬çš„ãªå®Ÿè¡Œé †åº
        default_plan = {
            'jobs': [],
            'sources': ['expert_knowledge'],
            'execution_pattern': 'sequential'
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ã„ã¦åŸºæœ¬çš„ãªã‚¸ãƒ§ãƒ–ã‚’è¿½åŠ 
        if 'search' in keywords:
            default_plan['jobs'].append({
                'name': 'web_search',
                'category': 'search',
                'unit': 'minimal-units/planning/web-search.yml'
            })
        
        if 'analysis' in keywords:
            default_plan['jobs'].append({
                'name': 'data_analysis',
                'category': 'analysis',
                'unit': 'minimal-units/planning/data-analysis.yml'
            })
        
        # ... ä»–ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åŸºã¥ãã‚¸ãƒ§ãƒ–è¿½åŠ 
        
        return default_plan
    
    def optimize_parallel_execution(self, workflow: Dict) -> Dict:
        """ä¸¦åˆ—å®Ÿè¡Œã®æœ€é©åŒ–"""
        # ä¾å­˜é–¢ä¿‚ã‚’åˆ†æžã—ã¦ä¸¦åˆ—å®Ÿè¡Œå¯èƒ½ãªã‚¸ãƒ§ãƒ–ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç‰¹å®š
        # ï¼ˆå®Ÿè£…ã¯çœç•¥ - å®Ÿéš›ã«ã¯ã‚ˆã‚Šè¤‡é›‘ãªä¾å­˜é–¢ä¿‚è§£æžãŒå¿…è¦ï¼‰
        workflow['parallel_groups'] = []
        return workflow
    
    def determine_execution_pattern(self, workflow: Dict) -> str:
        """å®Ÿè¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ±ºå®š"""
        job_count = len(workflow.get('jobs', []))
        if job_count <= 3:
            return 'sequential'
        elif job_count <= 6:
            return 'mixed_parallel'
        else:
            return 'complex_parallel'


# ãƒ¡ã‚¤ãƒ³å‡¦ç†
if __name__ == "__main__":
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰æƒ…å ±ã‚’å–å¾—
    request = os.environ.get('USER_REQUEST', '')
    capabilities = os.environ.get('CAPABILITIES', '')
    
    if not request:
        print("Error: USER_REQUEST not provided", file=sys.stderr)
        sys.exit(1)
    
    # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã‚’åˆæœŸåŒ–
    analyzer = OrchestratorAnalyzer()
    
    # å®Ÿè¡Œè¨ˆç”»ã‚’ç”Ÿæˆ
    execution_plan = analyzer.generate_execution_plan(request)
    
    # çµæžœã‚’å‡ºåŠ›
    output_dir = "projects/current-session/metadata"
    os.makedirs(output_dir, exist_ok=True)
    
    with open(os.path.join(output_dir, "orchestrator_analysis.json"), 'w', encoding='utf-8') as f:
        json.dump(execution_plan, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Orchestrator analysis completed")
    print(f"ðŸ“ Results saved to: {output_dir}/orchestrator_analysis.json")